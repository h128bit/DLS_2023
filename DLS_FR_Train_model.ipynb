{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zHS65-6Q3NkU",
        "outputId": "bdd4803c-49b8-4237-ab77-3f12914c2b7e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "import os\n",
        "from tqdm import tqdm, tqdm_notebook"
      ],
      "metadata": {
        "id": "llrJz3E541wA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('/content/drive/MyDrive/DLS Face recognition/celebA_train_500/celebA_anno.txt', header=None, sep=' ')\n",
        "df.columns = ['Path', 'Class']"
      ],
      "metadata": {
        "id": "xGwatjT3ylmp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset\n",
        "from torchvision import transforms\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import torch\n",
        "from PIL import Image\n",
        "\n",
        "class CelebADataSet(Dataset):\n",
        "    def __init__(self, df=None, ptest=80, pval=5):\n",
        "        \"\"\"\n",
        "        Параметр ptest определяет какой процент данных будет отдан для тренировочной выборки\n",
        "        Параметр pval определяет какой процент данных будет отдан для валидационной выборки\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "\n",
        "        self._path_to_file_name = '/content/drive/MyDrive/DLS Face recognition/celebA_train_500/celebA_imgs'\n",
        "        self._path_to_annot = '/content/drive/MyDrive/DLS Face recognition/celebA_train_500/celebA_anno.txt'\n",
        "\n",
        "        # Записываем данные об изображениях в DataFrame\n",
        "        if df is None:\n",
        "            self._table = pd.read_csv(self._path_to_annot, header=None, sep=' ')\n",
        "        else:\n",
        "            self._table = df\n",
        "        self._table.columns = ['File_name', 'Class']\n",
        "\n",
        "        self._len = self._table.shape[0]\n",
        "\n",
        "        # Получаем индексы для разделения данных на train и test\n",
        "        # а ля train test split из sklearn\n",
        "        idx = np.random.choice(self._len, self._len, False)\n",
        "        p1 = self._len//100 * ptest\n",
        "        p2 = self._len//100 * (100 - pval)\n",
        "        self.train_idx, self.test_idx, self.val_idx = np.split(idx, [p1, p2])\n",
        "\n",
        "        self.label_encoder = LabelEncoder()\n",
        "        self.label_encoder.fit(self._table['Class'].values)\n",
        "\n",
        "    class _Data(Dataset):\n",
        "        \"\"\"\n",
        "        Класс, который будет подаваться в Dataloader\n",
        "        \"\"\"\n",
        "        def __init__(self, upper, idx, train=True):\n",
        "            super().__init__()\n",
        "            self._upper = upper\n",
        "            self.train = train\n",
        "\n",
        "            self._table = self._upper._table.iloc[idx]\n",
        "\n",
        "        def __len__(self):\n",
        "            return len(self._table)\n",
        "\n",
        "        def load_sample(self, file):\n",
        "            image = Image.open(file)\n",
        "            image.load()\n",
        "            return image\n",
        "\n",
        "        def __getitem__(self, index):\n",
        "            path_to_img = os.path.join(self._upper._path_to_file_name,\n",
        "                                       self._table['File_name'].iloc[index])\n",
        "            image = self.load_sample(path_to_img)\n",
        "\n",
        "            make_transforms = {True: transforms.Compose([transforms.Resize(224, antialias=True),\n",
        "                                                         transforms.RandomRotation((0, 5)),\n",
        "                                                         transforms.ColorJitter(hue=0.1, saturation=0.1),\n",
        "                                                         transforms.RandomAdjustSharpness(1, p=0.5),\n",
        "                                                         transforms.RandomAutocontrast(p=0.5),\n",
        "                                                         transforms.RandomEqualize(p=0.5),\n",
        "                                                         transforms.RandomHorizontalFlip(p=0.5),\n",
        "                                                         transforms.ToTensor(),\n",
        "                                                         transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])]),\n",
        "                               False: transforms.Compose([transforms.Resize(224, antialias=True),\n",
        "                                                          transforms.ToTensor(),\n",
        "                                                          transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])}\n",
        "\n",
        "            image = make_transforms[self.train](image)\n",
        "            label =  self._upper.label_encoder.transform([self._table['Class'].iloc[index]])[0]\n",
        "\n",
        "            return image, label\n",
        "\n",
        "    def get_train(self):\n",
        "        return self._Data(self, self.train_idx)\n",
        "    def get_test(self):\n",
        "        return self._Data(self, self.test_idx, False)\n",
        "    def get_val(self):\n",
        "        return self._Data(self, self.val_idx, False)\n",
        "    def get_train_and_test(self):\n",
        "        return self.get_train(), self.get_test()"
      ],
      "metadata": {
        "id": "jmvAgjiRE9ZD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def fit_epoch(model, train_loader, loss_fn, optimaizer, DEVICE):\n",
        "    loss_per_epoch = 0\n",
        "    accuracy_per_epoch = 0\n",
        "    processed = 0\n",
        "\n",
        "    num_batch = len(train_loader)\n",
        "\n",
        "    log_template = \"batch: {n:d}/{all:d} train loss: {t_loss:0.4f} train acc {t_acc:0.4f}\"\n",
        "\n",
        "    model.train()\n",
        "    for i, (images, labels) in enumerate(train_loader):\n",
        "        images = images.to(DEVICE)\n",
        "        labels = labels.to(DEVICE)\n",
        "\n",
        "        # forward and backward\n",
        "        optimaizer.zero_grad()\n",
        "\n",
        "        outputs = model(images)\n",
        "        loss = loss_fn(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimaizer.step()\n",
        "\n",
        "        #statistic\n",
        "        preds = torch.argmax(outputs, 1)\n",
        "        loss_per_epoch += loss.item() * images.size(0)\n",
        "        accuracy_per_epoch += torch.sum(preds == labels.data).item()\n",
        "\n",
        "        processed += images.size(0)\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "        print(log_template.format(n=i+1, all=num_batch, t_loss=loss_per_epoch/processed, t_acc=accuracy_per_epoch/processed))\n",
        "\n",
        "\n",
        "    loss_per_epoch = loss_per_epoch / processed\n",
        "    accuracy_per_epoch = accuracy_per_epoch / processed\n",
        "    torch.cuda.empty_cache()\n",
        "    return loss_per_epoch, accuracy_per_epoch"
      ],
      "metadata": {
        "id": "4_SHinSSU2IS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test_epoch(model, test_loader, loss_fn, DEVICE):\n",
        "    loss_per_epoch = 0\n",
        "    accuracy_per_epoch = 0\n",
        "    processed = 0\n",
        "\n",
        "    model.eval()\n",
        "    for images, labels in test_loader:\n",
        "        images = images.to(DEVICE)\n",
        "        labels = labels.to(DEVICE)\n",
        "        logits = []\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = model(images)\n",
        "            loss = loss_fn(outputs, labels)\n",
        "            logits.append(outputs)\n",
        "\n",
        "            #statistic\n",
        "            processed += images.shape[0]\n",
        "            loss_per_epoch += loss.item() * images.size(0)\n",
        "\n",
        "            pred = torch.nn.functional.softmax(torch.cat(logits), dim=-1).to(DEVICE)\n",
        "            accuracy_per_epoch += torch.sum(torch.argmax(pred, 1) == labels.data).item()\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "    loss_per_epoch = loss_per_epoch / processed\n",
        "    accuracy_per_epoch = accuracy_per_epoch / processed\n",
        "    torch.cuda.empty_cache()\n",
        "    return loss_per_epoch, accuracy_per_epoch"
      ],
      "metadata": {
        "id": "x_3unHT5ZTMx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(model, train_loader, test_loader, num_epoch, loss_fn, optimaizer, scheduler, DEVICE):\n",
        "    history = {'train loss':[], 'test loss': [],\n",
        "               'train accuracy': [], 'test accuracy': []}\n",
        "\n",
        "    log_template = \"\\nEpoch {ep:d} train loss: {t_loss:0.4f} test loss {v_loss:0.4f} train acc {t_acc:0.4f} test acc {v_acc:0.4f}\"\n",
        "\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "    for epoch in range(num_epoch):\n",
        "        print(f'Epoch {epoch+1}/{num_epoch}')\n",
        "\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "        train_loss, train_accuracy = fit_epoch(model, train_loader, loss_fn, optimaizer, DEVICE)\n",
        "        test_loss, test_accuracy = test_epoch(model, test_loader, loss_fn, DEVICE)\n",
        "\n",
        "        history['train loss'].append(train_loss)\n",
        "        history['test loss'].append(test_loss)\n",
        "        history['train accuracy'].append(train_accuracy)\n",
        "        history['test accuracy'].append(test_accuracy)\n",
        "\n",
        "        scheduler.step()\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "        print(log_template.format(ep=epoch, t_loss=train_loss, v_loss=test_loss,\n",
        "                                  t_acc=train_accuracy, v_acc=test_accuracy))\n",
        "\n",
        "    torch.cuda.empty_cache()\n",
        "    return history"
      ],
      "metadata": {
        "id": "3GrgTxd7bFI-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision.models import mobilenet_v3_large, MobileNet_V3_Large_Weights\n",
        "\n",
        "# Загружаю модель пердобученную на ImageNet\n",
        "model = mobilenet_v3_large(weights=MobileNet_V3_Large_Weights.IMAGENET1K_V2)"
      ],
      "metadata": {
        "id": "rfc61wYL4Zxk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "celebA = CelebADataSet(ptest=85)\n",
        "train, test = celebA.get_train_and_test()\n",
        "\n",
        "batch_size = 256\n",
        "num_workers = 2\n",
        "\n",
        "train_data = DataLoader(train, batch_size=batch_size, num_workers=num_workers)\n",
        "test_data = DataLoader(test, batch_size=batch_size, num_workers=num_workers)"
      ],
      "metadata": {
        "id": "yGVmxrApOSvo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Меняю количество выходов в последннем слое сети на количество классов в обучающей выборке\n",
        "# Дообучать модель буду по всем весам\n",
        "N_CLASSES = len(celebA.label_encoder.classes_)\n",
        "\n",
        "in_features = model.classifier[-1].in_features\n",
        "model.classifier[-1] = torch.nn.Linear(in_features, N_CLASSES)"
      ],
      "metadata": {
        "id": "opux8NBjUQnH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = torch.optim.AdamW(model.parameters())\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 3, 0.5)\n",
        "loss_fn = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = model.to(DEVICE)"
      ],
      "metadata": {
        "id": "W0GGsH7Yft9B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cuda.empty_cache()\n",
        "\n",
        "num_epoch = 20\n",
        "history = train_model(model, train_data, test_data, num_epoch, loss_fn, optimizer, scheduler, DEVICE)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mjxj4MEwx0Kx",
        "outputId": "60b22e8e-926d-4388-ef14-d9191c0b8ebc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "batch: 1/40 train loss: 6.2163 train acc 0.0000\n",
            "batch: 2/40 train loss: 6.2101 train acc 0.0000\n",
            "batch: 3/40 train loss: 6.2084 train acc 0.0052\n",
            "batch: 4/40 train loss: 6.2001 train acc 0.0059\n",
            "batch: 5/40 train loss: 6.1988 train acc 0.0063\n",
            "batch: 6/40 train loss: 6.1991 train acc 0.0065\n",
            "batch: 7/40 train loss: 6.1921 train acc 0.0078\n",
            "batch: 8/40 train loss: 6.1867 train acc 0.0078\n",
            "batch: 9/40 train loss: 6.1779 train acc 0.0078\n",
            "batch: 10/40 train loss: 6.1667 train acc 0.0094\n",
            "batch: 11/40 train loss: 6.1608 train acc 0.0092\n",
            "batch: 12/40 train loss: 6.1489 train acc 0.0094\n",
            "batch: 13/40 train loss: 6.1404 train acc 0.0105\n",
            "batch: 14/40 train loss: 6.1190 train acc 0.0128\n",
            "batch: 15/40 train loss: 6.0944 train acc 0.0143\n",
            "batch: 16/40 train loss: 6.0757 train acc 0.0149\n",
            "batch: 17/40 train loss: 6.0554 train acc 0.0149\n",
            "batch: 18/40 train loss: 6.0352 train acc 0.0169\n",
            "batch: 19/40 train loss: 6.0112 train acc 0.0177\n",
            "batch: 20/40 train loss: 5.9859 train acc 0.0199\n",
            "batch: 21/40 train loss: 5.9591 train acc 0.0216\n",
            "batch: 22/40 train loss: 5.9303 train acc 0.0234\n",
            "batch: 23/40 train loss: 5.9067 train acc 0.0246\n",
            "batch: 24/40 train loss: 5.8802 train acc 0.0259\n",
            "batch: 25/40 train loss: 5.8421 train acc 0.0270\n",
            "batch: 26/40 train loss: 5.8119 train acc 0.0281\n",
            "batch: 27/40 train loss: 5.7781 train acc 0.0289\n",
            "batch: 28/40 train loss: 5.7444 train acc 0.0310\n",
            "batch: 29/40 train loss: 5.7150 train acc 0.0327\n",
            "batch: 30/40 train loss: 5.6803 train acc 0.0342\n",
            "batch: 31/40 train loss: 5.6435 train acc 0.0360\n",
            "batch: 32/40 train loss: 5.6060 train acc 0.0385\n",
            "batch: 33/40 train loss: 5.5693 train acc 0.0404\n",
            "batch: 34/40 train loss: 5.5326 train acc 0.0431\n",
            "batch: 35/40 train loss: 5.5005 train acc 0.0450\n",
            "batch: 36/40 train loss: 5.4663 train acc 0.0470\n",
            "batch: 37/40 train loss: 5.4309 train acc 0.0496\n",
            "batch: 38/40 train loss: 5.3980 train acc 0.0517\n",
            "batch: 39/40 train loss: 5.3684 train acc 0.0545\n",
            "batch: 40/40 train loss: 5.3390 train acc 0.0569\n",
            "\n",
            "Epoch 0 train loss: 5.3390 test loss 5.4916 train acc 0.0569 test acc 0.0475\n",
            "Epoch 2/20\n",
            "batch: 1/40 train loss: 3.6687 train acc 0.2383\n",
            "batch: 2/40 train loss: 3.6619 train acc 0.2520\n",
            "batch: 3/40 train loss: 3.6327 train acc 0.2435\n",
            "batch: 4/40 train loss: 3.6402 train acc 0.2402\n",
            "batch: 5/40 train loss: 3.6069 train acc 0.2414\n",
            "batch: 6/40 train loss: 3.5936 train acc 0.2467\n",
            "batch: 7/40 train loss: 3.5740 train acc 0.2506\n",
            "batch: 8/40 train loss: 3.5521 train acc 0.2563\n",
            "batch: 9/40 train loss: 3.5345 train acc 0.2587\n",
            "batch: 10/40 train loss: 3.4916 train acc 0.2648\n",
            "batch: 11/40 train loss: 3.4732 train acc 0.2638\n",
            "batch: 12/40 train loss: 3.4599 train acc 0.2646\n",
            "batch: 13/40 train loss: 3.4403 train acc 0.2677\n",
            "batch: 14/40 train loss: 3.4078 train acc 0.2732\n",
            "batch: 15/40 train loss: 3.3796 train acc 0.2766\n",
            "batch: 16/40 train loss: 3.3709 train acc 0.2793\n",
            "batch: 17/40 train loss: 3.3412 train acc 0.2831\n",
            "batch: 18/40 train loss: 3.3132 train acc 0.2888\n",
            "batch: 19/40 train loss: 3.2838 train acc 0.2938\n",
            "batch: 20/40 train loss: 3.2673 train acc 0.2951\n",
            "batch: 21/40 train loss: 3.2370 train acc 0.2982\n",
            "batch: 22/40 train loss: 3.2157 train acc 0.3013\n",
            "batch: 23/40 train loss: 3.2006 train acc 0.3042\n",
            "batch: 24/40 train loss: 3.1855 train acc 0.3071\n",
            "batch: 25/40 train loss: 3.1529 train acc 0.3131\n",
            "batch: 26/40 train loss: 3.1297 train acc 0.3166\n",
            "batch: 27/40 train loss: 3.1082 train acc 0.3197\n",
            "batch: 28/40 train loss: 3.0853 train acc 0.3241\n",
            "batch: 29/40 train loss: 3.0587 train acc 0.3269\n",
            "batch: 30/40 train loss: 3.0314 train acc 0.3319\n",
            "batch: 31/40 train loss: 3.0111 train acc 0.3352\n",
            "batch: 32/40 train loss: 2.9937 train acc 0.3378\n",
            "batch: 33/40 train loss: 2.9720 train acc 0.3409\n",
            "batch: 34/40 train loss: 2.9473 train acc 0.3457\n",
            "batch: 35/40 train loss: 2.9270 train acc 0.3491\n",
            "batch: 36/40 train loss: 2.9069 train acc 0.3522\n",
            "batch: 37/40 train loss: 2.8868 train acc 0.3556\n",
            "batch: 38/40 train loss: 2.8648 train acc 0.3593\n",
            "batch: 39/40 train loss: 2.8484 train acc 0.3618\n",
            "batch: 40/40 train loss: 2.8316 train acc 0.3656\n",
            "\n",
            "Epoch 1 train loss: 2.8316 test loss 3.6148 train acc 0.3656 test acc 0.2400\n",
            "Epoch 3/20\n",
            "batch: 1/40 train loss: 1.7773 train acc 0.5703\n",
            "batch: 2/40 train loss: 1.8660 train acc 0.5352\n",
            "batch: 3/40 train loss: 1.8393 train acc 0.5378\n",
            "batch: 4/40 train loss: 1.7986 train acc 0.5645\n",
            "batch: 5/40 train loss: 1.7771 train acc 0.5672\n",
            "batch: 6/40 train loss: 1.7691 train acc 0.5697\n",
            "batch: 7/40 train loss: 1.7502 train acc 0.5781\n",
            "batch: 8/40 train loss: 1.7275 train acc 0.5835\n",
            "batch: 9/40 train loss: 1.7231 train acc 0.5838\n",
            "batch: 10/40 train loss: 1.7193 train acc 0.5840\n",
            "batch: 11/40 train loss: 1.7136 train acc 0.5827\n",
            "batch: 12/40 train loss: 1.7146 train acc 0.5820\n",
            "batch: 13/40 train loss: 1.7089 train acc 0.5829\n",
            "batch: 14/40 train loss: 1.6928 train acc 0.5876\n",
            "batch: 15/40 train loss: 1.6797 train acc 0.5906\n",
            "batch: 16/40 train loss: 1.6819 train acc 0.5889\n",
            "batch: 17/40 train loss: 1.6592 train acc 0.5944\n",
            "batch: 18/40 train loss: 1.6419 train acc 0.5990\n",
            "batch: 19/40 train loss: 1.6251 train acc 0.6032\n",
            "batch: 20/40 train loss: 1.6189 train acc 0.6043\n",
            "batch: 21/40 train loss: 1.6030 train acc 0.6079\n",
            "batch: 22/40 train loss: 1.5891 train acc 0.6101\n",
            "batch: 23/40 train loss: 1.5808 train acc 0.6114\n",
            "batch: 24/40 train loss: 1.5781 train acc 0.6100\n",
            "batch: 25/40 train loss: 1.5633 train acc 0.6134\n",
            "batch: 26/40 train loss: 1.5540 train acc 0.6152\n",
            "batch: 27/40 train loss: 1.5439 train acc 0.6170\n",
            "batch: 28/40 train loss: 1.5309 train acc 0.6193\n",
            "batch: 29/40 train loss: 1.5186 train acc 0.6214\n",
            "batch: 30/40 train loss: 1.5078 train acc 0.6229\n",
            "batch: 31/40 train loss: 1.4987 train acc 0.6245\n",
            "batch: 32/40 train loss: 1.4860 train acc 0.6276\n",
            "batch: 33/40 train loss: 1.4748 train acc 0.6294\n",
            "batch: 34/40 train loss: 1.4667 train acc 0.6313\n",
            "batch: 35/40 train loss: 1.4547 train acc 0.6339\n",
            "batch: 36/40 train loss: 1.4460 train acc 0.6364\n",
            "batch: 37/40 train loss: 1.4377 train acc 0.6381\n",
            "batch: 38/40 train loss: 1.4253 train acc 0.6412\n",
            "batch: 39/40 train loss: 1.4188 train acc 0.6433\n",
            "batch: 40/40 train loss: 1.4124 train acc 0.6451\n",
            "\n",
            "Epoch 2 train loss: 1.4124 test loss 3.3049 train acc 0.6451 test acc 0.3142\n",
            "Epoch 4/20\n",
            "batch: 1/40 train loss: 0.9108 train acc 0.7734\n",
            "batch: 2/40 train loss: 0.9703 train acc 0.7656\n",
            "batch: 3/40 train loss: 0.9426 train acc 0.7669\n",
            "batch: 4/40 train loss: 0.9369 train acc 0.7715\n",
            "batch: 5/40 train loss: 0.9086 train acc 0.7766\n",
            "batch: 6/40 train loss: 0.8950 train acc 0.7793\n",
            "batch: 7/40 train loss: 0.8810 train acc 0.7829\n",
            "batch: 8/40 train loss: 0.8610 train acc 0.7900\n",
            "batch: 9/40 train loss: 0.8468 train acc 0.7895\n",
            "batch: 10/40 train loss: 0.8253 train acc 0.7953\n",
            "batch: 11/40 train loss: 0.8077 train acc 0.7969\n",
            "batch: 12/40 train loss: 0.8063 train acc 0.7972\n",
            "batch: 13/40 train loss: 0.8069 train acc 0.7978\n",
            "batch: 14/40 train loss: 0.7975 train acc 0.7994\n",
            "batch: 15/40 train loss: 0.7872 train acc 0.8034\n",
            "batch: 16/40 train loss: 0.7875 train acc 0.8032\n",
            "batch: 17/40 train loss: 0.7716 train acc 0.8072\n",
            "batch: 18/40 train loss: 0.7567 train acc 0.8112\n",
            "batch: 19/40 train loss: 0.7471 train acc 0.8148\n",
            "batch: 20/40 train loss: 0.7362 train acc 0.8170\n",
            "batch: 21/40 train loss: 0.7253 train acc 0.8201\n",
            "batch: 22/40 train loss: 0.7190 train acc 0.8214\n",
            "batch: 23/40 train loss: 0.7142 train acc 0.8232\n",
            "batch: 24/40 train loss: 0.7117 train acc 0.8239\n",
            "batch: 25/40 train loss: 0.6999 train acc 0.8269\n",
            "batch: 26/40 train loss: 0.6905 train acc 0.8295\n",
            "batch: 27/40 train loss: 0.6824 train acc 0.8320\n",
            "batch: 28/40 train loss: 0.6745 train acc 0.8344\n",
            "batch: 29/40 train loss: 0.6637 train acc 0.8369\n",
            "batch: 30/40 train loss: 0.6565 train acc 0.8392\n",
            "batch: 31/40 train loss: 0.6458 train acc 0.8416\n",
            "batch: 32/40 train loss: 0.6400 train acc 0.8427\n",
            "batch: 33/40 train loss: 0.6336 train acc 0.8442\n",
            "batch: 34/40 train loss: 0.6278 train acc 0.8449\n",
            "batch: 35/40 train loss: 0.6230 train acc 0.8462\n",
            "batch: 36/40 train loss: 0.6170 train acc 0.8474\n",
            "batch: 37/40 train loss: 0.6151 train acc 0.8476\n",
            "batch: 38/40 train loss: 0.6092 train acc 0.8487\n",
            "batch: 39/40 train loss: 0.6065 train acc 0.8500\n",
            "batch: 40/40 train loss: 0.6027 train acc 0.8507\n",
            "\n",
            "Epoch 3 train loss: 0.6027 test loss 2.0844 train acc 0.8507 test acc 0.5383\n",
            "Epoch 5/20\n",
            "batch: 1/40 train loss: 0.3916 train acc 0.9102\n",
            "batch: 2/40 train loss: 0.4040 train acc 0.9102\n",
            "batch: 3/40 train loss: 0.3945 train acc 0.9036\n",
            "batch: 4/40 train loss: 0.3932 train acc 0.9023\n",
            "batch: 5/40 train loss: 0.3795 train acc 0.9062\n",
            "batch: 6/40 train loss: 0.3785 train acc 0.9049\n",
            "batch: 7/40 train loss: 0.3849 train acc 0.9029\n",
            "batch: 8/40 train loss: 0.3848 train acc 0.9038\n",
            "batch: 9/40 train loss: 0.3848 train acc 0.9028\n",
            "batch: 10/40 train loss: 0.3815 train acc 0.9031\n",
            "batch: 11/40 train loss: 0.3753 train acc 0.9045\n",
            "batch: 12/40 train loss: 0.3781 train acc 0.9046\n",
            "batch: 13/40 train loss: 0.3797 train acc 0.9038\n",
            "batch: 14/40 train loss: 0.3751 train acc 0.9054\n",
            "batch: 15/40 train loss: 0.3672 train acc 0.9078\n",
            "batch: 16/40 train loss: 0.3662 train acc 0.9084\n",
            "batch: 17/40 train loss: 0.3609 train acc 0.9099\n",
            "batch: 18/40 train loss: 0.3551 train acc 0.9110\n",
            "batch: 19/40 train loss: 0.3517 train acc 0.9130\n",
            "batch: 20/40 train loss: 0.3493 train acc 0.9137\n",
            "batch: 21/40 train loss: 0.3465 train acc 0.9144\n",
            "batch: 22/40 train loss: 0.3438 train acc 0.9153\n",
            "batch: 23/40 train loss: 0.3438 train acc 0.9158\n",
            "batch: 24/40 train loss: 0.3424 train acc 0.9157\n",
            "batch: 25/40 train loss: 0.3378 train acc 0.9169\n",
            "batch: 26/40 train loss: 0.3362 train acc 0.9171\n",
            "batch: 27/40 train loss: 0.3321 train acc 0.9183\n",
            "batch: 28/40 train loss: 0.3302 train acc 0.9195\n",
            "batch: 29/40 train loss: 0.3260 train acc 0.9200\n",
            "batch: 30/40 train loss: 0.3254 train acc 0.9202\n",
            "batch: 31/40 train loss: 0.3212 train acc 0.9214\n",
            "batch: 32/40 train loss: 0.3199 train acc 0.9214\n",
            "batch: 33/40 train loss: 0.3184 train acc 0.9216\n",
            "batch: 34/40 train loss: 0.3170 train acc 0.9221\n",
            "batch: 35/40 train loss: 0.3157 train acc 0.9222\n",
            "batch: 36/40 train loss: 0.3125 train acc 0.9229\n",
            "batch: 37/40 train loss: 0.3120 train acc 0.9228\n",
            "batch: 38/40 train loss: 0.3113 train acc 0.9230\n",
            "batch: 39/40 train loss: 0.3129 train acc 0.9226\n",
            "batch: 40/40 train loss: 0.3120 train acc 0.9226\n",
            "\n",
            "Epoch 4 train loss: 0.3120 test loss 2.1540 train acc 0.9226 test acc 0.5450\n",
            "Epoch 6/20\n",
            "batch: 1/40 train loss: 0.2595 train acc 0.9375\n",
            "batch: 2/40 train loss: 0.2559 train acc 0.9375\n",
            "batch: 3/40 train loss: 0.2348 train acc 0.9427\n",
            "batch: 4/40 train loss: 0.2285 train acc 0.9473\n",
            "batch: 5/40 train loss: 0.2272 train acc 0.9453\n",
            "batch: 6/40 train loss: 0.2236 train acc 0.9479\n",
            "batch: 7/40 train loss: 0.2284 train acc 0.9475\n",
            "batch: 8/40 train loss: 0.2292 train acc 0.9468\n",
            "batch: 9/40 train loss: 0.2279 train acc 0.9462\n",
            "batch: 10/40 train loss: 0.2262 train acc 0.9473\n",
            "batch: 11/40 train loss: 0.2252 train acc 0.9474\n",
            "batch: 12/40 train loss: 0.2274 train acc 0.9469\n",
            "batch: 13/40 train loss: 0.2264 train acc 0.9483\n",
            "batch: 14/40 train loss: 0.2287 train acc 0.9481\n",
            "batch: 15/40 train loss: 0.2241 train acc 0.9490\n",
            "batch: 16/40 train loss: 0.2242 train acc 0.9485\n",
            "batch: 17/40 train loss: 0.2211 train acc 0.9488\n",
            "batch: 18/40 train loss: 0.2169 train acc 0.9497\n",
            "batch: 19/40 train loss: 0.2184 train acc 0.9484\n",
            "batch: 20/40 train loss: 0.2141 train acc 0.9496\n",
            "batch: 21/40 train loss: 0.2120 train acc 0.9503\n",
            "batch: 22/40 train loss: 0.2110 train acc 0.9510\n",
            "batch: 23/40 train loss: 0.2111 train acc 0.9513\n",
            "batch: 24/40 train loss: 0.2090 train acc 0.9517\n",
            "batch: 25/40 train loss: 0.2068 train acc 0.9520\n",
            "batch: 26/40 train loss: 0.2055 train acc 0.9524\n",
            "batch: 27/40 train loss: 0.2041 train acc 0.9528\n",
            "batch: 28/40 train loss: 0.2038 train acc 0.9528\n",
            "batch: 29/40 train loss: 0.2028 train acc 0.9530\n",
            "batch: 30/40 train loss: 0.2022 train acc 0.9533\n",
            "batch: 31/40 train loss: 0.1995 train acc 0.9540\n",
            "batch: 32/40 train loss: 0.1978 train acc 0.9543\n",
            "batch: 33/40 train loss: 0.1965 train acc 0.9547\n",
            "batch: 34/40 train loss: 0.1950 train acc 0.9552\n",
            "batch: 35/40 train loss: 0.1937 train acc 0.9554\n",
            "batch: 36/40 train loss: 0.1943 train acc 0.9552\n",
            "batch: 37/40 train loss: 0.1931 train acc 0.9554\n",
            "batch: 38/40 train loss: 0.1914 train acc 0.9562\n",
            "batch: 39/40 train loss: 0.1912 train acc 0.9561\n",
            "batch: 40/40 train loss: 0.1898 train acc 0.9568\n",
            "\n",
            "Epoch 5 train loss: 0.1898 test loss 1.8813 train acc 0.9568 test acc 0.5942\n",
            "Epoch 7/20\n",
            "batch: 1/40 train loss: 0.1575 train acc 0.9531\n",
            "batch: 2/40 train loss: 0.1534 train acc 0.9590\n",
            "batch: 3/40 train loss: 0.1517 train acc 0.9648\n",
            "batch: 4/40 train loss: 0.1494 train acc 0.9629\n",
            "batch: 5/40 train loss: 0.1550 train acc 0.9594\n",
            "batch: 6/40 train loss: 0.1477 train acc 0.9629\n",
            "batch: 7/40 train loss: 0.1475 train acc 0.9626\n",
            "batch: 8/40 train loss: 0.1437 train acc 0.9639\n",
            "batch: 9/40 train loss: 0.1424 train acc 0.9648\n",
            "batch: 10/40 train loss: 0.1391 train acc 0.9664\n",
            "batch: 11/40 train loss: 0.1378 train acc 0.9666\n",
            "batch: 12/40 train loss: 0.1370 train acc 0.9674\n",
            "batch: 13/40 train loss: 0.1380 train acc 0.9678\n",
            "batch: 14/40 train loss: 0.1409 train acc 0.9660\n",
            "batch: 15/40 train loss: 0.1397 train acc 0.9664\n",
            "batch: 16/40 train loss: 0.1395 train acc 0.9661\n",
            "batch: 17/40 train loss: 0.1376 train acc 0.9667\n",
            "batch: 18/40 train loss: 0.1347 train acc 0.9683\n",
            "batch: 19/40 train loss: 0.1330 train acc 0.9681\n",
            "batch: 20/40 train loss: 0.1296 train acc 0.9695\n",
            "batch: 21/40 train loss: 0.1279 train acc 0.9699\n",
            "batch: 22/40 train loss: 0.1258 train acc 0.9702\n",
            "batch: 23/40 train loss: 0.1256 train acc 0.9704\n",
            "batch: 24/40 train loss: 0.1248 train acc 0.9707\n",
            "batch: 25/40 train loss: 0.1223 train acc 0.9716\n",
            "batch: 26/40 train loss: 0.1212 train acc 0.9719\n",
            "batch: 27/40 train loss: 0.1203 train acc 0.9722\n",
            "batch: 28/40 train loss: 0.1190 train acc 0.9727\n",
            "batch: 29/40 train loss: 0.1170 train acc 0.9733\n",
            "batch: 30/40 train loss: 0.1164 train acc 0.9732\n",
            "batch: 31/40 train loss: 0.1167 train acc 0.9729\n",
            "batch: 32/40 train loss: 0.1157 train acc 0.9734\n",
            "batch: 33/40 train loss: 0.1143 train acc 0.9737\n",
            "batch: 34/40 train loss: 0.1127 train acc 0.9745\n",
            "batch: 35/40 train loss: 0.1121 train acc 0.9748\n",
            "batch: 36/40 train loss: 0.1112 train acc 0.9750\n",
            "batch: 37/40 train loss: 0.1108 train acc 0.9752\n",
            "batch: 38/40 train loss: 0.1100 train acc 0.9754\n",
            "batch: 39/40 train loss: 0.1100 train acc 0.9756\n",
            "batch: 40/40 train loss: 0.1090 train acc 0.9759\n",
            "\n",
            "Epoch 6 train loss: 0.1090 test loss 1.5186 train acc 0.9759 test acc 0.6775\n",
            "Epoch 8/20\n",
            "batch: 1/40 train loss: 0.0702 train acc 0.9961\n",
            "batch: 2/40 train loss: 0.0654 train acc 0.9961\n",
            "batch: 3/40 train loss: 0.0581 train acc 0.9974\n",
            "batch: 4/40 train loss: 0.0636 train acc 0.9922\n",
            "batch: 5/40 train loss: 0.0655 train acc 0.9898\n",
            "batch: 6/40 train loss: 0.0650 train acc 0.9909\n",
            "batch: 7/40 train loss: 0.0644 train acc 0.9916\n",
            "batch: 8/40 train loss: 0.0637 train acc 0.9917\n",
            "batch: 9/40 train loss: 0.0633 train acc 0.9918\n",
            "batch: 10/40 train loss: 0.0651 train acc 0.9910\n",
            "batch: 11/40 train loss: 0.0634 train acc 0.9911\n",
            "batch: 12/40 train loss: 0.0639 train acc 0.9912\n",
            "batch: 13/40 train loss: 0.0631 train acc 0.9916\n",
            "batch: 14/40 train loss: 0.0629 train acc 0.9911\n",
            "batch: 15/40 train loss: 0.0625 train acc 0.9911\n",
            "batch: 16/40 train loss: 0.0628 train acc 0.9907\n",
            "batch: 17/40 train loss: 0.0619 train acc 0.9908\n",
            "batch: 18/40 train loss: 0.0606 train acc 0.9913\n",
            "batch: 19/40 train loss: 0.0606 train acc 0.9912\n",
            "batch: 20/40 train loss: 0.0596 train acc 0.9916\n",
            "batch: 21/40 train loss: 0.0584 train acc 0.9920\n",
            "batch: 22/40 train loss: 0.0580 train acc 0.9920\n",
            "batch: 23/40 train loss: 0.0577 train acc 0.9920\n",
            "batch: 24/40 train loss: 0.0578 train acc 0.9920\n",
            "batch: 25/40 train loss: 0.0573 train acc 0.9920\n",
            "batch: 26/40 train loss: 0.0569 train acc 0.9920\n",
            "batch: 27/40 train loss: 0.0568 train acc 0.9922\n",
            "batch: 28/40 train loss: 0.0561 train acc 0.9925\n",
            "batch: 29/40 train loss: 0.0561 train acc 0.9922\n",
            "batch: 30/40 train loss: 0.0555 train acc 0.9924\n",
            "batch: 31/40 train loss: 0.0550 train acc 0.9926\n",
            "batch: 32/40 train loss: 0.0545 train acc 0.9928\n",
            "batch: 33/40 train loss: 0.0540 train acc 0.9929\n",
            "batch: 34/40 train loss: 0.0534 train acc 0.9931\n",
            "batch: 35/40 train loss: 0.0534 train acc 0.9931\n",
            "batch: 36/40 train loss: 0.0531 train acc 0.9932\n",
            "batch: 37/40 train loss: 0.0529 train acc 0.9932\n",
            "batch: 38/40 train loss: 0.0524 train acc 0.9934\n",
            "batch: 39/40 train loss: 0.0525 train acc 0.9934\n",
            "batch: 40/40 train loss: 0.0519 train acc 0.9935\n",
            "\n",
            "Epoch 7 train loss: 0.0519 test loss 1.4623 train acc 0.9935 test acc 0.6883\n",
            "Epoch 9/20\n",
            "batch: 1/40 train loss: 0.0459 train acc 0.9961\n",
            "batch: 2/40 train loss: 0.0438 train acc 0.9941\n",
            "batch: 3/40 train loss: 0.0425 train acc 0.9961\n",
            "batch: 4/40 train loss: 0.0409 train acc 0.9961\n",
            "batch: 5/40 train loss: 0.0394 train acc 0.9969\n",
            "batch: 6/40 train loss: 0.0398 train acc 0.9967\n",
            "batch: 7/40 train loss: 0.0411 train acc 0.9950\n",
            "batch: 8/40 train loss: 0.0402 train acc 0.9951\n",
            "batch: 9/40 train loss: 0.0395 train acc 0.9952\n",
            "batch: 10/40 train loss: 0.0419 train acc 0.9934\n",
            "batch: 11/40 train loss: 0.0424 train acc 0.9929\n",
            "batch: 12/40 train loss: 0.0422 train acc 0.9935\n",
            "batch: 13/40 train loss: 0.0431 train acc 0.9931\n",
            "batch: 14/40 train loss: 0.0420 train acc 0.9933\n",
            "batch: 15/40 train loss: 0.0419 train acc 0.9935\n",
            "batch: 16/40 train loss: 0.0419 train acc 0.9934\n",
            "batch: 17/40 train loss: 0.0415 train acc 0.9936\n",
            "batch: 18/40 train loss: 0.0412 train acc 0.9935\n",
            "batch: 19/40 train loss: 0.0406 train acc 0.9938\n",
            "batch: 20/40 train loss: 0.0405 train acc 0.9939\n",
            "batch: 21/40 train loss: 0.0404 train acc 0.9940\n",
            "batch: 22/40 train loss: 0.0403 train acc 0.9941\n",
            "batch: 23/40 train loss: 0.0409 train acc 0.9942\n",
            "batch: 24/40 train loss: 0.0409 train acc 0.9945\n",
            "batch: 25/40 train loss: 0.0406 train acc 0.9944\n",
            "batch: 26/40 train loss: 0.0402 train acc 0.9944\n",
            "batch: 27/40 train loss: 0.0401 train acc 0.9946\n",
            "batch: 28/40 train loss: 0.0397 train acc 0.9947\n",
            "batch: 29/40 train loss: 0.0402 train acc 0.9942\n",
            "batch: 30/40 train loss: 0.0400 train acc 0.9944\n",
            "batch: 31/40 train loss: 0.0396 train acc 0.9945\n",
            "batch: 32/40 train loss: 0.0391 train acc 0.9946\n",
            "batch: 33/40 train loss: 0.0387 train acc 0.9948\n",
            "batch: 34/40 train loss: 0.0387 train acc 0.9947\n",
            "batch: 35/40 train loss: 0.0385 train acc 0.9948\n",
            "batch: 36/40 train loss: 0.0385 train acc 0.9948\n",
            "batch: 37/40 train loss: 0.0382 train acc 0.9949\n",
            "batch: 38/40 train loss: 0.0384 train acc 0.9948\n",
            "batch: 39/40 train loss: 0.0385 train acc 0.9949\n",
            "batch: 40/40 train loss: 0.0387 train acc 0.9946\n",
            "\n",
            "Epoch 8 train loss: 0.0387 test loss 1.4631 train acc 0.9946 test acc 0.6900\n",
            "Epoch 10/20\n",
            "batch: 1/40 train loss: 0.0512 train acc 0.9883\n",
            "batch: 2/40 train loss: 0.0425 train acc 0.9922\n",
            "batch: 3/40 train loss: 0.0392 train acc 0.9935\n",
            "batch: 4/40 train loss: 0.0372 train acc 0.9941\n",
            "batch: 5/40 train loss: 0.0349 train acc 0.9953\n",
            "batch: 6/40 train loss: 0.0371 train acc 0.9948\n",
            "batch: 7/40 train loss: 0.0350 train acc 0.9955\n",
            "batch: 8/40 train loss: 0.0342 train acc 0.9951\n",
            "batch: 9/40 train loss: 0.0328 train acc 0.9957\n",
            "batch: 10/40 train loss: 0.0334 train acc 0.9957\n",
            "batch: 11/40 train loss: 0.0334 train acc 0.9954\n",
            "batch: 12/40 train loss: 0.0340 train acc 0.9951\n",
            "batch: 13/40 train loss: 0.0341 train acc 0.9955\n",
            "batch: 14/40 train loss: 0.0350 train acc 0.9955\n",
            "batch: 15/40 train loss: 0.0352 train acc 0.9958\n",
            "batch: 16/40 train loss: 0.0356 train acc 0.9958\n",
            "batch: 17/40 train loss: 0.0352 train acc 0.9961\n",
            "batch: 18/40 train loss: 0.0344 train acc 0.9963\n",
            "batch: 19/40 train loss: 0.0340 train acc 0.9963\n",
            "batch: 20/40 train loss: 0.0333 train acc 0.9965\n",
            "batch: 21/40 train loss: 0.0326 train acc 0.9967\n",
            "batch: 22/40 train loss: 0.0326 train acc 0.9964\n",
            "batch: 23/40 train loss: 0.0323 train acc 0.9966\n",
            "batch: 24/40 train loss: 0.0326 train acc 0.9963\n",
            "batch: 25/40 train loss: 0.0325 train acc 0.9964\n",
            "batch: 26/40 train loss: 0.0321 train acc 0.9965\n",
            "batch: 27/40 train loss: 0.0323 train acc 0.9965\n",
            "batch: 28/40 train loss: 0.0317 train acc 0.9967\n",
            "batch: 29/40 train loss: 0.0315 train acc 0.9966\n",
            "batch: 30/40 train loss: 0.0314 train acc 0.9967\n",
            "batch: 31/40 train loss: 0.0310 train acc 0.9968\n",
            "batch: 32/40 train loss: 0.0307 train acc 0.9969\n",
            "batch: 33/40 train loss: 0.0305 train acc 0.9970\n",
            "batch: 34/40 train loss: 0.0305 train acc 0.9970\n",
            "batch: 35/40 train loss: 0.0305 train acc 0.9970\n",
            "batch: 36/40 train loss: 0.0304 train acc 0.9971\n",
            "batch: 37/40 train loss: 0.0305 train acc 0.9970\n",
            "batch: 38/40 train loss: 0.0304 train acc 0.9970\n",
            "batch: 39/40 train loss: 0.0303 train acc 0.9971\n",
            "batch: 40/40 train loss: 0.0301 train acc 0.9972\n",
            "\n",
            "Epoch 9 train loss: 0.0301 test loss 1.4314 train acc 0.9972 test acc 0.7050\n",
            "Epoch 11/20\n",
            "batch: 1/40 train loss: 0.0265 train acc 1.0000\n",
            "batch: 2/40 train loss: 0.0254 train acc 1.0000\n",
            "batch: 3/40 train loss: 0.0243 train acc 1.0000\n",
            "batch: 4/40 train loss: 0.0242 train acc 1.0000\n",
            "batch: 5/40 train loss: 0.0234 train acc 1.0000\n",
            "batch: 6/40 train loss: 0.0246 train acc 0.9993\n",
            "batch: 7/40 train loss: 0.0250 train acc 0.9989\n",
            "batch: 8/40 train loss: 0.0257 train acc 0.9980\n",
            "batch: 9/40 train loss: 0.0249 train acc 0.9983\n",
            "batch: 10/40 train loss: 0.0250 train acc 0.9980\n",
            "batch: 11/40 train loss: 0.0252 train acc 0.9975\n",
            "batch: 12/40 train loss: 0.0250 train acc 0.9977\n",
            "batch: 13/40 train loss: 0.0262 train acc 0.9973\n",
            "batch: 14/40 train loss: 0.0260 train acc 0.9975\n",
            "batch: 15/40 train loss: 0.0268 train acc 0.9971\n",
            "batch: 16/40 train loss: 0.0270 train acc 0.9968\n",
            "batch: 17/40 train loss: 0.0266 train acc 0.9970\n",
            "batch: 18/40 train loss: 0.0268 train acc 0.9970\n",
            "batch: 19/40 train loss: 0.0269 train acc 0.9969\n",
            "batch: 20/40 train loss: 0.0265 train acc 0.9971\n",
            "batch: 21/40 train loss: 0.0262 train acc 0.9972\n",
            "batch: 22/40 train loss: 0.0260 train acc 0.9973\n",
            "batch: 23/40 train loss: 0.0261 train acc 0.9973\n",
            "batch: 24/40 train loss: 0.0260 train acc 0.9974\n",
            "batch: 25/40 train loss: 0.0256 train acc 0.9975\n",
            "batch: 26/40 train loss: 0.0254 train acc 0.9976\n",
            "batch: 27/40 train loss: 0.0256 train acc 0.9973\n",
            "batch: 28/40 train loss: 0.0253 train acc 0.9973\n",
            "batch: 29/40 train loss: 0.0252 train acc 0.9973\n",
            "batch: 30/40 train loss: 0.0250 train acc 0.9974\n",
            "batch: 31/40 train loss: 0.0252 train acc 0.9974\n",
            "batch: 32/40 train loss: 0.0251 train acc 0.9973\n",
            "batch: 33/40 train loss: 0.0250 train acc 0.9974\n",
            "batch: 34/40 train loss: 0.0248 train acc 0.9975\n",
            "batch: 35/40 train loss: 0.0247 train acc 0.9975\n",
            "batch: 36/40 train loss: 0.0249 train acc 0.9976\n",
            "batch: 37/40 train loss: 0.0247 train acc 0.9977\n",
            "batch: 38/40 train loss: 0.0246 train acc 0.9977\n",
            "batch: 39/40 train loss: 0.0246 train acc 0.9977\n",
            "batch: 40/40 train loss: 0.0243 train acc 0.9977\n",
            "\n",
            "Epoch 10 train loss: 0.0243 test loss 1.4391 train acc 0.9977 test acc 0.7083\n",
            "Epoch 12/20\n",
            "batch: 1/40 train loss: 0.0231 train acc 1.0000\n",
            "batch: 2/40 train loss: 0.0231 train acc 1.0000\n",
            "batch: 3/40 train loss: 0.0201 train acc 1.0000\n",
            "batch: 4/40 train loss: 0.0195 train acc 1.0000\n",
            "batch: 5/40 train loss: 0.0191 train acc 1.0000\n",
            "batch: 6/40 train loss: 0.0208 train acc 0.9993\n",
            "batch: 7/40 train loss: 0.0208 train acc 0.9994\n",
            "batch: 8/40 train loss: 0.0201 train acc 0.9995\n",
            "batch: 9/40 train loss: 0.0214 train acc 0.9991\n",
            "batch: 10/40 train loss: 0.0216 train acc 0.9992\n",
            "batch: 11/40 train loss: 0.0215 train acc 0.9989\n",
            "batch: 12/40 train loss: 0.0213 train acc 0.9990\n",
            "batch: 13/40 train loss: 0.0218 train acc 0.9988\n",
            "batch: 14/40 train loss: 0.0217 train acc 0.9986\n",
            "batch: 15/40 train loss: 0.0218 train acc 0.9984\n",
            "batch: 16/40 train loss: 0.0224 train acc 0.9983\n",
            "batch: 17/40 train loss: 0.0223 train acc 0.9982\n",
            "batch: 18/40 train loss: 0.0221 train acc 0.9983\n",
            "batch: 19/40 train loss: 0.0219 train acc 0.9984\n",
            "batch: 20/40 train loss: 0.0224 train acc 0.9980\n",
            "batch: 21/40 train loss: 0.0222 train acc 0.9980\n",
            "batch: 22/40 train loss: 0.0221 train acc 0.9980\n",
            "batch: 23/40 train loss: 0.0223 train acc 0.9981\n",
            "batch: 24/40 train loss: 0.0224 train acc 0.9980\n",
            "batch: 25/40 train loss: 0.0222 train acc 0.9981\n",
            "batch: 26/40 train loss: 0.0219 train acc 0.9982\n",
            "batch: 27/40 train loss: 0.0219 train acc 0.9981\n",
            "batch: 28/40 train loss: 0.0216 train acc 0.9982\n",
            "batch: 29/40 train loss: 0.0218 train acc 0.9980\n",
            "batch: 30/40 train loss: 0.0219 train acc 0.9978\n",
            "batch: 31/40 train loss: 0.0216 train acc 0.9979\n",
            "batch: 32/40 train loss: 0.0214 train acc 0.9979\n",
            "batch: 33/40 train loss: 0.0214 train acc 0.9980\n",
            "batch: 34/40 train loss: 0.0213 train acc 0.9980\n",
            "batch: 35/40 train loss: 0.0213 train acc 0.9981\n",
            "batch: 36/40 train loss: 0.0214 train acc 0.9980\n",
            "batch: 37/40 train loss: 0.0215 train acc 0.9980\n",
            "batch: 38/40 train loss: 0.0214 train acc 0.9980\n",
            "batch: 39/40 train loss: 0.0218 train acc 0.9979\n",
            "batch: 40/40 train loss: 0.0219 train acc 0.9977\n",
            "\n",
            "Epoch 11 train loss: 0.0219 test loss 1.4381 train acc 0.9977 test acc 0.7042\n",
            "Epoch 13/20\n",
            "batch: 1/40 train loss: 0.0222 train acc 1.0000\n",
            "batch: 2/40 train loss: 0.0193 train acc 1.0000\n",
            "batch: 3/40 train loss: 0.0184 train acc 0.9987\n",
            "batch: 4/40 train loss: 0.0174 train acc 0.9990\n",
            "batch: 5/40 train loss: 0.0182 train acc 0.9984\n",
            "batch: 6/40 train loss: 0.0184 train acc 0.9987\n",
            "batch: 7/40 train loss: 0.0178 train acc 0.9989\n",
            "batch: 8/40 train loss: 0.0188 train acc 0.9985\n",
            "batch: 9/40 train loss: 0.0187 train acc 0.9987\n",
            "batch: 10/40 train loss: 0.0192 train acc 0.9988\n",
            "batch: 11/40 train loss: 0.0204 train acc 0.9986\n",
            "batch: 12/40 train loss: 0.0209 train acc 0.9987\n",
            "batch: 13/40 train loss: 0.0207 train acc 0.9988\n",
            "batch: 14/40 train loss: 0.0203 train acc 0.9989\n",
            "batch: 15/40 train loss: 0.0205 train acc 0.9987\n",
            "batch: 16/40 train loss: 0.0209 train acc 0.9988\n",
            "batch: 17/40 train loss: 0.0206 train acc 0.9989\n",
            "batch: 18/40 train loss: 0.0206 train acc 0.9987\n",
            "batch: 19/40 train loss: 0.0206 train acc 0.9988\n",
            "batch: 20/40 train loss: 0.0205 train acc 0.9988\n",
            "batch: 21/40 train loss: 0.0202 train acc 0.9989\n",
            "batch: 22/40 train loss: 0.0204 train acc 0.9989\n",
            "batch: 23/40 train loss: 0.0208 train acc 0.9990\n",
            "batch: 24/40 train loss: 0.0211 train acc 0.9989\n",
            "batch: 25/40 train loss: 0.0209 train acc 0.9989\n",
            "batch: 26/40 train loss: 0.0209 train acc 0.9988\n",
            "batch: 27/40 train loss: 0.0208 train acc 0.9988\n",
            "batch: 28/40 train loss: 0.0208 train acc 0.9989\n",
            "batch: 29/40 train loss: 0.0206 train acc 0.9988\n",
            "batch: 30/40 train loss: 0.0203 train acc 0.9988\n",
            "batch: 31/40 train loss: 0.0202 train acc 0.9987\n",
            "batch: 32/40 train loss: 0.0201 train acc 0.9988\n",
            "batch: 33/40 train loss: 0.0204 train acc 0.9985\n",
            "batch: 34/40 train loss: 0.0204 train acc 0.9985\n",
            "batch: 35/40 train loss: 0.0205 train acc 0.9985\n",
            "batch: 36/40 train loss: 0.0203 train acc 0.9986\n",
            "batch: 37/40 train loss: 0.0204 train acc 0.9986\n",
            "batch: 38/40 train loss: 0.0205 train acc 0.9984\n",
            "batch: 39/40 train loss: 0.0205 train acc 0.9984\n",
            "batch: 40/40 train loss: 0.0203 train acc 0.9984\n",
            "\n",
            "Epoch 12 train loss: 0.0203 test loss 1.4306 train acc 0.9984 test acc 0.7083\n",
            "Epoch 14/20\n",
            "batch: 1/40 train loss: 0.0167 train acc 1.0000\n",
            "batch: 2/40 train loss: 0.0187 train acc 1.0000\n",
            "batch: 3/40 train loss: 0.0191 train acc 1.0000\n",
            "batch: 4/40 train loss: 0.0179 train acc 1.0000\n",
            "batch: 5/40 train loss: 0.0175 train acc 1.0000\n",
            "batch: 6/40 train loss: 0.0186 train acc 0.9993\n",
            "batch: 7/40 train loss: 0.0184 train acc 0.9989\n",
            "batch: 8/40 train loss: 0.0182 train acc 0.9990\n",
            "batch: 9/40 train loss: 0.0185 train acc 0.9991\n",
            "batch: 10/40 train loss: 0.0185 train acc 0.9992\n",
            "batch: 11/40 train loss: 0.0193 train acc 0.9986\n",
            "batch: 12/40 train loss: 0.0191 train acc 0.9987\n",
            "batch: 13/40 train loss: 0.0188 train acc 0.9988\n",
            "batch: 14/40 train loss: 0.0187 train acc 0.9986\n",
            "batch: 15/40 train loss: 0.0185 train acc 0.9987\n",
            "batch: 16/40 train loss: 0.0186 train acc 0.9985\n",
            "batch: 17/40 train loss: 0.0182 train acc 0.9986\n",
            "batch: 18/40 train loss: 0.0179 train acc 0.9987\n",
            "batch: 19/40 train loss: 0.0177 train acc 0.9988\n",
            "batch: 20/40 train loss: 0.0176 train acc 0.9988\n",
            "batch: 21/40 train loss: 0.0174 train acc 0.9989\n",
            "batch: 22/40 train loss: 0.0175 train acc 0.9989\n",
            "batch: 23/40 train loss: 0.0175 train acc 0.9990\n",
            "batch: 24/40 train loss: 0.0174 train acc 0.9990\n",
            "batch: 25/40 train loss: 0.0173 train acc 0.9991\n",
            "batch: 26/40 train loss: 0.0173 train acc 0.9991\n",
            "batch: 27/40 train loss: 0.0173 train acc 0.9990\n",
            "batch: 28/40 train loss: 0.0170 train acc 0.9990\n",
            "batch: 29/40 train loss: 0.0170 train acc 0.9991\n",
            "batch: 30/40 train loss: 0.0171 train acc 0.9991\n",
            "batch: 31/40 train loss: 0.0173 train acc 0.9990\n",
            "batch: 32/40 train loss: 0.0172 train acc 0.9990\n",
            "batch: 33/40 train loss: 0.0173 train acc 0.9989\n",
            "batch: 34/40 train loss: 0.0173 train acc 0.9990\n",
            "batch: 35/40 train loss: 0.0173 train acc 0.9990\n",
            "batch: 36/40 train loss: 0.0176 train acc 0.9989\n",
            "batch: 37/40 train loss: 0.0177 train acc 0.9989\n",
            "batch: 38/40 train loss: 0.0178 train acc 0.9990\n",
            "batch: 39/40 train loss: 0.0178 train acc 0.9990\n",
            "batch: 40/40 train loss: 0.0176 train acc 0.9990\n",
            "\n",
            "Epoch 13 train loss: 0.0176 test loss 1.4208 train acc 0.9990 test acc 0.7092\n",
            "Epoch 15/20\n",
            "batch: 1/40 train loss: 0.0265 train acc 0.9961\n",
            "batch: 2/40 train loss: 0.0216 train acc 0.9980\n",
            "batch: 3/40 train loss: 0.0199 train acc 0.9987\n",
            "batch: 4/40 train loss: 0.0204 train acc 0.9980\n",
            "batch: 5/40 train loss: 0.0201 train acc 0.9977\n",
            "batch: 6/40 train loss: 0.0198 train acc 0.9967\n",
            "batch: 7/40 train loss: 0.0186 train acc 0.9972\n",
            "batch: 8/40 train loss: 0.0180 train acc 0.9976\n",
            "batch: 9/40 train loss: 0.0176 train acc 0.9978\n",
            "batch: 10/40 train loss: 0.0181 train acc 0.9973\n",
            "batch: 11/40 train loss: 0.0181 train acc 0.9972\n",
            "batch: 12/40 train loss: 0.0182 train acc 0.9971\n",
            "batch: 13/40 train loss: 0.0181 train acc 0.9973\n",
            "batch: 14/40 train loss: 0.0177 train acc 0.9975\n",
            "batch: 15/40 train loss: 0.0178 train acc 0.9974\n",
            "batch: 16/40 train loss: 0.0175 train acc 0.9976\n",
            "batch: 17/40 train loss: 0.0172 train acc 0.9977\n",
            "batch: 18/40 train loss: 0.0172 train acc 0.9978\n",
            "batch: 19/40 train loss: 0.0173 train acc 0.9979\n",
            "batch: 20/40 train loss: 0.0171 train acc 0.9980\n",
            "batch: 21/40 train loss: 0.0170 train acc 0.9981\n",
            "batch: 22/40 train loss: 0.0173 train acc 0.9980\n",
            "batch: 23/40 train loss: 0.0174 train acc 0.9980\n",
            "batch: 24/40 train loss: 0.0172 train acc 0.9980\n",
            "batch: 25/40 train loss: 0.0170 train acc 0.9981\n",
            "batch: 26/40 train loss: 0.0171 train acc 0.9982\n",
            "batch: 27/40 train loss: 0.0169 train acc 0.9983\n",
            "batch: 28/40 train loss: 0.0170 train acc 0.9983\n",
            "batch: 29/40 train loss: 0.0170 train acc 0.9982\n",
            "batch: 30/40 train loss: 0.0169 train acc 0.9983\n",
            "batch: 31/40 train loss: 0.0168 train acc 0.9984\n",
            "batch: 32/40 train loss: 0.0168 train acc 0.9984\n",
            "batch: 33/40 train loss: 0.0167 train acc 0.9985\n",
            "batch: 34/40 train loss: 0.0170 train acc 0.9984\n",
            "batch: 35/40 train loss: 0.0170 train acc 0.9984\n",
            "batch: 36/40 train loss: 0.0170 train acc 0.9984\n",
            "batch: 37/40 train loss: 0.0171 train acc 0.9984\n",
            "batch: 38/40 train loss: 0.0169 train acc 0.9985\n",
            "batch: 39/40 train loss: 0.0171 train acc 0.9984\n",
            "batch: 40/40 train loss: 0.0170 train acc 0.9984\n",
            "\n",
            "Epoch 14 train loss: 0.0170 test loss 1.4214 train acc 0.9984 test acc 0.7067\n",
            "Epoch 16/20\n",
            "batch: 1/40 train loss: 0.0138 train acc 1.0000\n",
            "batch: 2/40 train loss: 0.0171 train acc 1.0000\n",
            "batch: 3/40 train loss: 0.0156 train acc 1.0000\n",
            "batch: 4/40 train loss: 0.0150 train acc 1.0000\n",
            "batch: 5/40 train loss: 0.0156 train acc 1.0000\n",
            "batch: 6/40 train loss: 0.0154 train acc 1.0000\n",
            "batch: 7/40 train loss: 0.0151 train acc 1.0000\n",
            "batch: 8/40 train loss: 0.0151 train acc 1.0000\n",
            "batch: 9/40 train loss: 0.0151 train acc 1.0000\n",
            "batch: 10/40 train loss: 0.0153 train acc 0.9996\n",
            "batch: 11/40 train loss: 0.0150 train acc 0.9996\n",
            "batch: 12/40 train loss: 0.0149 train acc 0.9997\n",
            "batch: 13/40 train loss: 0.0148 train acc 0.9997\n",
            "batch: 14/40 train loss: 0.0148 train acc 0.9997\n",
            "batch: 15/40 train loss: 0.0148 train acc 0.9997\n",
            "batch: 16/40 train loss: 0.0151 train acc 0.9995\n",
            "batch: 17/40 train loss: 0.0151 train acc 0.9995\n",
            "batch: 18/40 train loss: 0.0151 train acc 0.9996\n",
            "batch: 19/40 train loss: 0.0150 train acc 0.9996\n",
            "batch: 20/40 train loss: 0.0147 train acc 0.9996\n",
            "batch: 21/40 train loss: 0.0145 train acc 0.9996\n",
            "batch: 22/40 train loss: 0.0144 train acc 0.9996\n",
            "batch: 23/40 train loss: 0.0145 train acc 0.9997\n",
            "batch: 24/40 train loss: 0.0144 train acc 0.9997\n",
            "batch: 25/40 train loss: 0.0145 train acc 0.9997\n",
            "batch: 26/40 train loss: 0.0145 train acc 0.9997\n",
            "batch: 27/40 train loss: 0.0144 train acc 0.9996\n",
            "batch: 28/40 train loss: 0.0144 train acc 0.9996\n",
            "batch: 29/40 train loss: 0.0145 train acc 0.9993\n",
            "batch: 30/40 train loss: 0.0144 train acc 0.9993\n",
            "batch: 31/40 train loss: 0.0144 train acc 0.9994\n",
            "batch: 32/40 train loss: 0.0144 train acc 0.9994\n",
            "batch: 33/40 train loss: 0.0143 train acc 0.9994\n",
            "batch: 34/40 train loss: 0.0145 train acc 0.9994\n",
            "batch: 35/40 train loss: 0.0146 train acc 0.9993\n",
            "batch: 36/40 train loss: 0.0146 train acc 0.9993\n",
            "batch: 37/40 train loss: 0.0147 train acc 0.9994\n",
            "batch: 38/40 train loss: 0.0146 train acc 0.9994\n",
            "batch: 39/40 train loss: 0.0147 train acc 0.9994\n",
            "batch: 40/40 train loss: 0.0147 train acc 0.9994\n",
            "\n",
            "Epoch 15 train loss: 0.0147 test loss 1.4178 train acc 0.9994 test acc 0.7058\n",
            "Epoch 17/20\n",
            "batch: 1/40 train loss: 0.0150 train acc 1.0000\n",
            "batch: 2/40 train loss: 0.0152 train acc 1.0000\n",
            "batch: 3/40 train loss: 0.0150 train acc 1.0000\n",
            "batch: 4/40 train loss: 0.0145 train acc 0.9990\n",
            "batch: 5/40 train loss: 0.0145 train acc 0.9992\n",
            "batch: 6/40 train loss: 0.0144 train acc 0.9993\n",
            "batch: 7/40 train loss: 0.0154 train acc 0.9994\n",
            "batch: 8/40 train loss: 0.0153 train acc 0.9995\n",
            "batch: 9/40 train loss: 0.0150 train acc 0.9996\n",
            "batch: 10/40 train loss: 0.0151 train acc 0.9996\n",
            "batch: 11/40 train loss: 0.0149 train acc 0.9996\n",
            "batch: 12/40 train loss: 0.0148 train acc 0.9997\n",
            "batch: 13/40 train loss: 0.0155 train acc 0.9994\n",
            "batch: 14/40 train loss: 0.0152 train acc 0.9994\n",
            "batch: 15/40 train loss: 0.0154 train acc 0.9995\n",
            "batch: 16/40 train loss: 0.0154 train acc 0.9995\n",
            "batch: 17/40 train loss: 0.0153 train acc 0.9995\n",
            "batch: 18/40 train loss: 0.0150 train acc 0.9996\n",
            "batch: 19/40 train loss: 0.0150 train acc 0.9996\n",
            "batch: 20/40 train loss: 0.0149 train acc 0.9996\n",
            "batch: 21/40 train loss: 0.0147 train acc 0.9996\n",
            "batch: 22/40 train loss: 0.0146 train acc 0.9996\n",
            "batch: 23/40 train loss: 0.0154 train acc 0.9992\n",
            "batch: 24/40 train loss: 0.0154 train acc 0.9992\n",
            "batch: 25/40 train loss: 0.0152 train acc 0.9992\n",
            "batch: 26/40 train loss: 0.0152 train acc 0.9992\n",
            "batch: 27/40 train loss: 0.0153 train acc 0.9991\n",
            "batch: 28/40 train loss: 0.0152 train acc 0.9992\n",
            "batch: 29/40 train loss: 0.0152 train acc 0.9991\n",
            "batch: 30/40 train loss: 0.0152 train acc 0.9991\n",
            "batch: 31/40 train loss: 0.0151 train acc 0.9991\n",
            "batch: 32/40 train loss: 0.0154 train acc 0.9990\n",
            "batch: 33/40 train loss: 0.0154 train acc 0.9989\n",
            "batch: 34/40 train loss: 0.0155 train acc 0.9989\n",
            "batch: 35/40 train loss: 0.0154 train acc 0.9989\n",
            "batch: 36/40 train loss: 0.0154 train acc 0.9989\n",
            "batch: 37/40 train loss: 0.0155 train acc 0.9988\n",
            "batch: 38/40 train loss: 0.0155 train acc 0.9989\n",
            "batch: 39/40 train loss: 0.0155 train acc 0.9989\n",
            "batch: 40/40 train loss: 0.0154 train acc 0.9989\n",
            "\n",
            "Epoch 16 train loss: 0.0154 test loss 1.4189 train acc 0.9989 test acc 0.7067\n",
            "Epoch 18/20\n",
            "batch: 1/40 train loss: 0.0197 train acc 0.9961\n",
            "batch: 2/40 train loss: 0.0171 train acc 0.9980\n",
            "batch: 3/40 train loss: 0.0156 train acc 0.9987\n",
            "batch: 4/40 train loss: 0.0156 train acc 0.9990\n",
            "batch: 5/40 train loss: 0.0147 train acc 0.9992\n",
            "batch: 6/40 train loss: 0.0158 train acc 0.9993\n",
            "batch: 7/40 train loss: 0.0155 train acc 0.9994\n",
            "batch: 8/40 train loss: 0.0153 train acc 0.9995\n",
            "batch: 9/40 train loss: 0.0152 train acc 0.9996\n",
            "batch: 10/40 train loss: 0.0153 train acc 0.9996\n",
            "batch: 11/40 train loss: 0.0155 train acc 0.9993\n",
            "batch: 12/40 train loss: 0.0152 train acc 0.9993\n",
            "batch: 13/40 train loss: 0.0157 train acc 0.9991\n",
            "batch: 14/40 train loss: 0.0155 train acc 0.9992\n",
            "batch: 15/40 train loss: 0.0155 train acc 0.9992\n",
            "batch: 16/40 train loss: 0.0155 train acc 0.9993\n",
            "batch: 17/40 train loss: 0.0160 train acc 0.9991\n",
            "batch: 18/40 train loss: 0.0159 train acc 0.9991\n",
            "batch: 19/40 train loss: 0.0157 train acc 0.9992\n",
            "batch: 20/40 train loss: 0.0158 train acc 0.9990\n",
            "batch: 21/40 train loss: 0.0154 train acc 0.9991\n",
            "batch: 22/40 train loss: 0.0154 train acc 0.9991\n",
            "batch: 23/40 train loss: 0.0153 train acc 0.9992\n",
            "batch: 24/40 train loss: 0.0154 train acc 0.9992\n",
            "batch: 25/40 train loss: 0.0154 train acc 0.9992\n",
            "batch: 26/40 train loss: 0.0157 train acc 0.9991\n",
            "batch: 27/40 train loss: 0.0156 train acc 0.9991\n",
            "batch: 28/40 train loss: 0.0156 train acc 0.9990\n",
            "batch: 29/40 train loss: 0.0156 train acc 0.9989\n",
            "batch: 30/40 train loss: 0.0155 train acc 0.9990\n",
            "batch: 31/40 train loss: 0.0154 train acc 0.9990\n",
            "batch: 32/40 train loss: 0.0154 train acc 0.9990\n",
            "batch: 33/40 train loss: 0.0153 train acc 0.9991\n",
            "batch: 34/40 train loss: 0.0154 train acc 0.9991\n",
            "batch: 35/40 train loss: 0.0155 train acc 0.9990\n",
            "batch: 36/40 train loss: 0.0154 train acc 0.9990\n",
            "batch: 37/40 train loss: 0.0153 train acc 0.9990\n",
            "batch: 38/40 train loss: 0.0152 train acc 0.9991\n",
            "batch: 39/40 train loss: 0.0153 train acc 0.9990\n",
            "batch: 40/40 train loss: 0.0152 train acc 0.9990\n",
            "\n",
            "Epoch 17 train loss: 0.0152 test loss 1.4221 train acc 0.9990 test acc 0.7075\n",
            "Epoch 19/20\n",
            "batch: 1/40 train loss: 0.0210 train acc 1.0000\n",
            "batch: 2/40 train loss: 0.0163 train acc 1.0000\n",
            "batch: 3/40 train loss: 0.0170 train acc 0.9987\n",
            "batch: 4/40 train loss: 0.0166 train acc 0.9990\n",
            "batch: 5/40 train loss: 0.0164 train acc 0.9992\n",
            "batch: 6/40 train loss: 0.0161 train acc 0.9993\n",
            "batch: 7/40 train loss: 0.0164 train acc 0.9994\n",
            "batch: 8/40 train loss: 0.0159 train acc 0.9995\n",
            "batch: 9/40 train loss: 0.0159 train acc 0.9996\n",
            "batch: 10/40 train loss: 0.0158 train acc 0.9996\n",
            "batch: 11/40 train loss: 0.0164 train acc 0.9993\n",
            "batch: 12/40 train loss: 0.0164 train acc 0.9993\n",
            "batch: 13/40 train loss: 0.0162 train acc 0.9994\n",
            "batch: 14/40 train loss: 0.0159 train acc 0.9994\n",
            "batch: 15/40 train loss: 0.0159 train acc 0.9995\n",
            "batch: 16/40 train loss: 0.0158 train acc 0.9995\n",
            "batch: 17/40 train loss: 0.0154 train acc 0.9995\n",
            "batch: 18/40 train loss: 0.0151 train acc 0.9996\n",
            "batch: 19/40 train loss: 0.0149 train acc 0.9996\n",
            "batch: 20/40 train loss: 0.0151 train acc 0.9996\n",
            "batch: 21/40 train loss: 0.0148 train acc 0.9996\n",
            "batch: 22/40 train loss: 0.0147 train acc 0.9996\n",
            "batch: 23/40 train loss: 0.0147 train acc 0.9997\n",
            "batch: 24/40 train loss: 0.0147 train acc 0.9997\n",
            "batch: 25/40 train loss: 0.0145 train acc 0.9997\n",
            "batch: 26/40 train loss: 0.0146 train acc 0.9997\n",
            "batch: 27/40 train loss: 0.0146 train acc 0.9996\n",
            "batch: 28/40 train loss: 0.0144 train acc 0.9996\n",
            "batch: 29/40 train loss: 0.0144 train acc 0.9996\n",
            "batch: 30/40 train loss: 0.0146 train acc 0.9995\n",
            "batch: 31/40 train loss: 0.0145 train acc 0.9995\n",
            "batch: 32/40 train loss: 0.0145 train acc 0.9995\n",
            "batch: 33/40 train loss: 0.0144 train acc 0.9995\n",
            "batch: 34/40 train loss: 0.0145 train acc 0.9994\n",
            "batch: 35/40 train loss: 0.0146 train acc 0.9994\n",
            "batch: 36/40 train loss: 0.0147 train acc 0.9995\n",
            "batch: 37/40 train loss: 0.0147 train acc 0.9995\n",
            "batch: 38/40 train loss: 0.0146 train acc 0.9995\n",
            "batch: 39/40 train loss: 0.0146 train acc 0.9995\n",
            "batch: 40/40 train loss: 0.0145 train acc 0.9995\n",
            "\n",
            "Epoch 18 train loss: 0.0145 test loss 1.4227 train acc 0.9995 test acc 0.7100\n",
            "Epoch 20/20\n",
            "batch: 1/40 train loss: 0.0148 train acc 1.0000\n",
            "batch: 2/40 train loss: 0.0149 train acc 1.0000\n",
            "batch: 3/40 train loss: 0.0140 train acc 1.0000\n",
            "batch: 4/40 train loss: 0.0129 train acc 1.0000\n",
            "batch: 5/40 train loss: 0.0127 train acc 1.0000\n",
            "batch: 6/40 train loss: 0.0137 train acc 0.9993\n",
            "batch: 7/40 train loss: 0.0134 train acc 0.9994\n",
            "batch: 8/40 train loss: 0.0136 train acc 0.9995\n",
            "batch: 9/40 train loss: 0.0134 train acc 0.9996\n",
            "batch: 10/40 train loss: 0.0131 train acc 0.9996\n",
            "batch: 11/40 train loss: 0.0135 train acc 0.9993\n",
            "batch: 12/40 train loss: 0.0144 train acc 0.9990\n",
            "batch: 13/40 train loss: 0.0144 train acc 0.9991\n",
            "batch: 14/40 train loss: 0.0141 train acc 0.9992\n",
            "batch: 15/40 train loss: 0.0141 train acc 0.9992\n",
            "batch: 16/40 train loss: 0.0141 train acc 0.9993\n",
            "batch: 17/40 train loss: 0.0139 train acc 0.9993\n",
            "batch: 18/40 train loss: 0.0138 train acc 0.9993\n",
            "batch: 19/40 train loss: 0.0139 train acc 0.9994\n",
            "batch: 20/40 train loss: 0.0137 train acc 0.9994\n",
            "batch: 21/40 train loss: 0.0136 train acc 0.9994\n",
            "batch: 22/40 train loss: 0.0137 train acc 0.9995\n",
            "batch: 23/40 train loss: 0.0138 train acc 0.9995\n",
            "batch: 24/40 train loss: 0.0139 train acc 0.9995\n",
            "batch: 25/40 train loss: 0.0139 train acc 0.9995\n",
            "batch: 26/40 train loss: 0.0139 train acc 0.9995\n",
            "batch: 27/40 train loss: 0.0140 train acc 0.9996\n",
            "batch: 28/40 train loss: 0.0139 train acc 0.9996\n",
            "batch: 29/40 train loss: 0.0137 train acc 0.9996\n",
            "batch: 30/40 train loss: 0.0137 train acc 0.9996\n",
            "batch: 31/40 train loss: 0.0139 train acc 0.9995\n",
            "batch: 32/40 train loss: 0.0138 train acc 0.9995\n",
            "batch: 33/40 train loss: 0.0137 train acc 0.9995\n",
            "batch: 34/40 train loss: 0.0139 train acc 0.9994\n",
            "batch: 35/40 train loss: 0.0141 train acc 0.9993\n",
            "batch: 36/40 train loss: 0.0141 train acc 0.9993\n",
            "batch: 37/40 train loss: 0.0140 train acc 0.9994\n",
            "batch: 38/40 train loss: 0.0140 train acc 0.9993\n",
            "batch: 39/40 train loss: 0.0140 train acc 0.9993\n",
            "batch: 40/40 train loss: 0.0140 train acc 0.9993\n",
            "\n",
            "Epoch 19 train loss: 0.0140 test loss 1.4259 train acc 0.9993 test acc 0.7100\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Посмотрим на поведение модели в время обучения и на финальную точность модели после дообучения"
      ],
      "metadata": {
        "id": "V85dddSpeBiW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "axs = plt.subplots(nrows=1, ncols=2, figsize=(20, 5))[1].flatten()\n",
        "\n",
        "x = [i for i in range(num_epoch)]\n",
        "\n",
        "y_train_l = history['train loss']\n",
        "y_test_l = history['test loss']\n",
        "y_train_acc = history['train accuracy']\n",
        "y_test_acc = history['test accuracy']\n",
        "\n",
        "axs[0].plot(x, y_train_l, color='r', label='train loss')\n",
        "axs[0].plot(x, y_test_l, color='y', label='test loss')\n",
        "axs[0].title.set_text('loss')\n",
        "axs[0].legend()\n",
        "\n",
        "axs[1].plot(x, y_train_acc, color='r', label='train accuracy')\n",
        "axs[1].plot(x, y_test_acc, color='y', label='test accuracy')\n",
        "axs[1].title.set_text('accuracy')\n",
        "axs[1].legend()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 486
        },
        "id": "2hbFg24ilcfV",
        "outputId": "d0a01346-8ab7-4be2-b6f8-fac59bfefa59"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7bd24931f3d0>"
            ]
          },
          "metadata": {},
          "execution_count": 15
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 2000x500 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "val_data = DataLoader(celebA.get_val(), batch_size=batch_size)\n",
        "\n",
        "val_loss = 0\n",
        "val_acc = 0\n",
        "\n",
        "logits = []\n",
        "labels_ = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for images, labels in val_data:\n",
        "        images = images.to(DEVICE)\n",
        "        labels = labels.to(DEVICE)\n",
        "\n",
        "        outputs = model(images).cpu()\n",
        "        logits.append(outputs)\n",
        "        labels_.extend([*labels])\n",
        "\n",
        "probs = torch.nn.functional.softmax(torch.cat(logits), dim=-1).numpy()"
      ],
      "metadata": {
        "id": "97YEGs1NhYke"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prob_ = torch.argmax(torch.Tensor(probs), 1).numpy()\n",
        "labels_ = torch.Tensor(labels_).numpy()\n",
        "\n",
        "val_accuracy = np.sum(labels_ == prob_)/len(labels_)\n",
        "val_accuracy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hjYL1Gokn7Dk",
        "outputId": "abe9d424-15b1-4e19-a1d9-e2f83f4777df"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.7152209492635024"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "В итоге, исходя из графика accuracy можно заметить, что модель имеет признаки переобучения, так как на обучающей выборке модель достигла точности в 99.93% на тестовой в 71%, на валидационной выборке модель достигает точности в 71.5%. При использовании весов mobilenet_DLS_FR_weightes_v2.pth модель достигает точности в 73%"
      ],
      "metadata": {
        "id": "lHWoyrlBqDyc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Сохраняем веса модели\n",
        "path_to_save_model_state = '/content/drive/MyDrive/DLS Face recognition/models weights/mobilenet_DLS_FR_weightes_v3.pth'\n",
        "torch.save(model.state_dict(), path_to_save_model_state)"
      ],
      "metadata": {
        "id": "U-zqBMMYlu9G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Identification Rate Metric\n",
        "\n",
        "Здесь расчет Identification Rate будет производится не при помощи вызова функций, а в виде линейного кода."
      ],
      "metadata": {
        "id": "fgeQk4t9ykqd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Загружаем веса дообученной модели и убираем классификационный слой,\n",
        "# оставляя только предклассификационныцй линейный слой\n",
        "from torchvision.models import mobilenet_v3_large, MobileNet_V3_Large_Weights\n",
        "\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "N_CLASSES = 460\n",
        "\n",
        "model_rec = mobilenet_v3_large()\n",
        "in_features = model_rec.classifier[-1].in_features\n",
        "model_rec.classifier[-1] = torch.nn.Linear(in_features, N_CLASSES)\n",
        "\n",
        "model_rec.load_state_dict(torch.load('/content/drive/MyDrive/DLS Face recognition/models weights/mobilenet_DLS_FR_weightes_v2.pth',\n",
        "                                     map_location=torch.device(DEVICE)))\n",
        "\n",
        "model_rec.classifier = model_rec.classifier[0]\n",
        "model_rec.classifier"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tzQ8Vc32wS2T",
        "outputId": "d0e0cac0-9cf4-4967-ea1f-e9f39054ea3c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Linear(in_features=960, out_features=1280, bias=True)"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "idf_metric_annot_df = pd.read_csv('/content/drive/MyDrive/DLS Face recognition/celebA_ir/celebA_anno_query.csv')\n",
        "idf_metric_annot_df.columns = ['Path', 'Class']"
      ],
      "metadata": {
        "id": "br2TuKLqwiYa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Функция подготавливающая изображение перед подачей его в сеть\n",
        "def make_transforms(image):\n",
        "    transformations = transforms.Compose([transforms.Resize(224, antialias=True),\n",
        "                                          transforms.ToTensor(),\n",
        "                                          transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])\n",
        "\n",
        "    image = transformations(image)\n",
        "    image = image[None,:,:,:]\n",
        "    return image"
      ],
      "metadata": {
        "id": "Sz0CieEb6CjK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Посчитаем ембендинги и косинусальное расстояние для query части"
      ],
      "metadata": {
        "id": "RbMZHvw10l5b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "query_embending_df = pd.DataFrame()\n",
        "\n",
        "# записываем эмбендинги из query части в отдельную таблицу\n",
        "model_rec.eval()\n",
        "path_img = '/content/drive/MyDrive/DLS Face recognition/celebA_ir/celebA_query'\n",
        "for i in range(idf_metric_annot_df.shape[0]):\n",
        "    path = idf_metric_annot_df['Path'].iloc[i]\n",
        "    img = make_transforms(\n",
        "        Image.open(os.path.join(path_img, path)))\n",
        "    emb = model_rec(img)\n",
        "\n",
        "    label = idf_metric_annot_df['Class'].iloc[i]\n",
        "    t = {'Path': path, 'Class': label, 'Embending': emb.tolist()}\n",
        "\n",
        "    df = pd.DataFrame(t)\n",
        "    query_embending_df = pd.concat([query_embending_df, df], ignore_index=True)"
      ],
      "metadata": {
        "id": "ePJvkzREyy-n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics.pairwise import cosine_distances\n",
        "\n",
        "# в similar face и diff face записываю рассчитанное расстояние для одинаковых и разных лиц\n",
        "cosine_distances_dict = {'similar face': [], 'diff face': []}\n",
        "\n",
        "for i in range(query_embending_df.shape[0]):\n",
        "    face1 = np.array(query_embending_df['Embending'].iloc[i]).reshape(1, -1)\n",
        "    label1 = query_embending_df['Class'].iloc[i]\n",
        "\n",
        "    for j in range(i + 1, query_embending_df.shape[0]):\n",
        "        face2 = np.array(query_embending_df['Embending'].iloc[j]).reshape(1, -1)\n",
        "        label2 = query_embending_df['Class'].iloc[j]\n",
        "\n",
        "        if label1 == label2:\n",
        "            cosine_distances_dict['similar face'].append(cosine_distances(face1, face2)[0][0])\n",
        "        else:\n",
        "            cosine_distances_dict['diff face'].append(cosine_distances(face1, face2)[0][0])"
      ],
      "metadata": {
        "id": "jla_rulM_Yyg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Посчитаем косинусальное расстояние между query и distractors частями"
      ],
      "metadata": {
        "id": "zi1-sheD30sb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cosine_distances_distractors = []\n",
        "\n",
        "path_to_img = '/content/drive/MyDrive/DLS Face recognition/celebA_ir/celebA_distractors'\n",
        "\n",
        "for img_name in os.listdir(path_to_img):\n",
        "    # фиксируем лицо из distractors части\n",
        "    img = make_transforms(\n",
        "        Image.open(os.path.join(path_to_img, img_name)))\n",
        "    face1 = np.array(model_rec(img).tolist())\n",
        "\n",
        "    for i in range(query_embending_df.shape[0]):\n",
        "        face2 = np.array(query_embending_df['Embending'].iloc[i]).reshape(1, -1)\n",
        "        cosine_distances_distractors.append(cosine_distances(face1, face2)[0][0])"
      ],
      "metadata": {
        "id": "MNKqW-aV4bok"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Считаем количество positive пар для которых косинусльное растояние меньше некоторого порога при различных FPR значениях"
      ],
      "metadata": {
        "id": "1lLWk533KQke"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "FPR = [0.5, 0.2, 0.1, 0.05]\n",
        "rate_list = []\n",
        "\n",
        "for fpr in FPR:\n",
        "    N = int((len(cosine_distances_distractors) + len(cosine_distances_dict['diff face'])) * fpr)\n",
        "\n",
        "    threashold = sorted(cosine_distances_distractors + cosine_distances_dict['diff face'])[N]\n",
        "\n",
        "    positive_pair = np.array(cosine_distances_dict['similar face'])\n",
        "    rate_list.append(positive_pair[positive_pair < threashold].size/positive_pair.size)"
      ],
      "metadata": {
        "id": "zUA_6IKNJpZG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for fpr, rate in zip(FPR, rate_list):\n",
        "    print(f'FPR = {fpr}, rate: {np.around(rate, 4)}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HiYMqD3qKeg-",
        "outputId": "160fa1e6-8640-48c6-a51d-723350447417"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "FPR = 0.5, rate: 0.8039\n",
            "FPR = 0.2, rate: 0.5454\n",
            "FPR = 0.1, rate: 0.3995\n",
            "FPR = 0.05, rate: 0.2947\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "В итоге можно заметить, что для точного распознования лиц модель MobileNet v3 предобученная на ImageNet обученная при помощи CE не подходит, так как при малых значениях FPR (0.05, 0.1) модель достигает невысоких значений -- 29% и 40%."
      ],
      "metadata": {
        "id": "gQav7UIOK-92"
      }
    }
  ]
}