{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "68oWm6c6RYeN",
        "outputId": "14ac9ddc-f234-4137-cbeb-1f699d15754e"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "jvNDayilW1Jk"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset\n",
        "from torchvision import transforms\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import torch\n",
        "from PIL import Image\n",
        "\n",
        "class CelebADataSet(Dataset):\n",
        "    def __init__(self, df=None, ptrain=80, pval=5,\n",
        "                 path_to_im='/content/drive/MyDrive/DLS Face recognition/celebA_train_500/celebA_imgs',\n",
        "                 path_to_annot='/content/drive/MyDrive/DLS Face recognition/celebA_train_500/celebA_anno.txt'):\n",
        "        \"\"\"\n",
        "        Параметр ptrain определяет какой процент данных будет отдан для тренировочной выборки\n",
        "        Параметр pval определяет какой процент данных будет отдан для валидационной выборки\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "\n",
        "        self._path_to_file_name = path_to_im\n",
        "        self._path_to_annot = path_to_annot\n",
        "\n",
        "        # Записываем данные об изображениях в DataFrame\n",
        "        if df is None:\n",
        "            self._table = pd.read_csv(self._path_to_annot, header=None, sep=' ')\n",
        "        else:\n",
        "            self._table = df\n",
        "        self._table.columns = ['File_name', 'Class']\n",
        "\n",
        "        self._len = self._table.shape[0]\n",
        "\n",
        "        # Получаем индексы для разделения данных на train и test\n",
        "        # а ля train test split из sklearn\n",
        "        idx = np.random.choice(self._len, self._len, False)\n",
        "        p1 = self._len//100 * ptrain\n",
        "        p2 = self._len//100 * (100 - pval)\n",
        "        self.train_idx, self.test_idx, self.val_idx = np.split(idx, [p1, p2])\n",
        "\n",
        "        self.label_encoder = LabelEncoder()\n",
        "        self.label_encoder.fit(self._table['Class'].values)\n",
        "\n",
        "    class _Data(Dataset):\n",
        "        \"\"\"\n",
        "        Класс, который будет подаваться в Dataloader\n",
        "        \"\"\"\n",
        "        def __init__(self, upper, idx, train=True):\n",
        "            super().__init__()\n",
        "            self._upper = upper\n",
        "            self.train = train\n",
        "\n",
        "            self._table = self._upper._table.iloc[idx]\n",
        "\n",
        "        def __len__(self):\n",
        "            return len(self._table)\n",
        "\n",
        "        def load_sample(self, file):\n",
        "            image = Image.open(file)\n",
        "            image.load()\n",
        "            return image\n",
        "\n",
        "        def __getitem__(self, index):\n",
        "            path_to_img = os.path.join(self._upper._path_to_file_name,\n",
        "                                       self._table['File_name'].iloc[index])\n",
        "            image = self.load_sample(path_to_img)\n",
        "\n",
        "            make_transforms = {True: transforms.Compose([transforms.Resize(224, antialias=True),\n",
        "                                                         transforms.RandomRotation((0, 5)),\n",
        "                                                         transforms.ColorJitter(hue=0.1, saturation=0.1),\n",
        "                                                         transforms.RandomAdjustSharpness(1, p=0.5),\n",
        "                                                         transforms.RandomAutocontrast(p=0.5),\n",
        "                                                         transforms.RandomEqualize(p=0.5),\n",
        "                                                         transforms.RandomHorizontalFlip(p=0.5),\n",
        "                                                         transforms.ToTensor(),\n",
        "                                                         transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])]),\n",
        "                               False: transforms.Compose([transforms.Resize(224, antialias=True),\n",
        "                                                          transforms.ToTensor(),\n",
        "                                                          transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])}\n",
        "\n",
        "            image = make_transforms[self.train](image)\n",
        "            label =  self._upper.label_encoder.transform([self._table['Class'].iloc[index]])[0]\n",
        "\n",
        "            return image, label\n",
        "\n",
        "    def get_train(self):\n",
        "        return self._Data(self, self.train_idx)\n",
        "    def get_test(self):\n",
        "        return self._Data(self, self.test_idx, False)\n",
        "    def get_val(self):\n",
        "        return self._Data(self, self.val_idx, False)\n",
        "    def get_train_and_test(self):\n",
        "        return self.get_train(), self.get_test()\n"
      ],
      "metadata": {
        "id": "-rBPwtBPS3yB"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def fit_epoch(model, train_loader, loss_fn, optimaizer, DEVICE):\n",
        "    loss_per_epoch = 0\n",
        "    accuracy_per_epoch = 0\n",
        "    processed = 0\n",
        "\n",
        "    num_batch = len(train_loader)\n",
        "\n",
        "    log_template = \"batch: {n:d}/{all:d} train loss: {t_loss:0.4f} train acc {t_acc:0.4f}\"\n",
        "\n",
        "    model.train()\n",
        "    for i, (images, labels) in enumerate(train_loader):\n",
        "        images = images.to(DEVICE)\n",
        "        labels = labels.to(DEVICE)\n",
        "\n",
        "        # forward and backward\n",
        "        optimaizer.zero_grad()\n",
        "\n",
        "        outputs = model(images, labels)\n",
        "        loss = loss_fn(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimaizer.step()\n",
        "\n",
        "        #statistic\n",
        "        preds = torch.argmax(outputs, 1)\n",
        "        loss_per_epoch += loss.item() * images.size(0)\n",
        "        accuracy_per_epoch += torch.sum(preds == labels.data).item()\n",
        "\n",
        "        processed += images.size(0)\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "        print(log_template.format(n=i+1, all=num_batch, t_loss=loss_per_epoch/processed, t_acc=accuracy_per_epoch/processed))\n",
        "\n",
        "\n",
        "    loss_per_epoch = loss_per_epoch / processed\n",
        "    accuracy_per_epoch = accuracy_per_epoch / processed\n",
        "    torch.cuda.empty_cache()\n",
        "    return loss_per_epoch, accuracy_per_epoch"
      ],
      "metadata": {
        "id": "YQUFpQjPuSse"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test_epoch(model, test_loader, loss_fn, DEVICE):\n",
        "    loss_per_epoch = 0\n",
        "    accuracy_per_epoch = 0\n",
        "    processed = 0\n",
        "\n",
        "    model.eval()\n",
        "    for images, labels in test_loader:\n",
        "        images = images.to(DEVICE)\n",
        "        labels = labels.to(DEVICE)\n",
        "        logits = []\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = model(images, labels)\n",
        "            loss = loss_fn(outputs, labels)\n",
        "            logits.append(outputs)\n",
        "\n",
        "            #statistic\n",
        "            preds = torch.argmax(outputs, 1)\n",
        "            loss_per_epoch += loss.item() * images.size(0)\n",
        "            accuracy_per_epoch += torch.sum(preds == labels.data).item()\n",
        "\n",
        "            processed += images.size(0)\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "    loss_per_epoch = loss_per_epoch / processed\n",
        "    accuracy_per_epoch = accuracy_per_epoch / processed\n",
        "    torch.cuda.empty_cache()\n",
        "    return loss_per_epoch, accuracy_per_epoch"
      ],
      "metadata": {
        "id": "zv0xRYkXuVfk"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(model, train_loader, test_loader, num_epoch, loss_fn, optimaizer, scheduler, DEVICE):\n",
        "    history = {'train loss':[], 'test loss': [],\n",
        "               'train accuracy': [], 'test accuracy': []}\n",
        "\n",
        "    log_template = \"\\nEpoch {ep:d} train loss: {t_loss:0.4f} test loss {v_loss:0.4f} train acc {t_acc:0.4f} test acc {v_acc:0.4f}\"\n",
        "\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "    for epoch in range(num_epoch):\n",
        "        print(f'Epoch {epoch+1}/{num_epoch}')\n",
        "\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "        train_loss, train_accuracy = fit_epoch(model, train_loader, loss_fn, optimaizer, DEVICE)\n",
        "        test_loss, test_accuracy = test_epoch(model, test_loader, loss_fn, DEVICE)\n",
        "\n",
        "        history['train loss'].append(train_loss)\n",
        "        history['test loss'].append(test_loss)\n",
        "        history['train accuracy'].append(train_accuracy)\n",
        "        history['test accuracy'].append(test_accuracy)\n",
        "\n",
        "        scheduler.step()\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "        print(log_template.format(ep=epoch+1, t_loss=train_loss, v_loss=test_loss,\n",
        "                                  t_acc=train_accuracy, v_acc=test_accuracy))\n",
        "\n",
        "    torch.cuda.empty_cache()\n",
        "    return history"
      ],
      "metadata": {
        "id": "cgfTpJ2huY6G"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "from torch.nn.functional import linear, normalize\n",
        "class ArcFace(nn.Module):\n",
        "    def __init__(self, in_feature, out_feature, s, m):\n",
        "        super().__init__()\n",
        "        self.s = s\n",
        "        self.margin = m\n",
        "\n",
        "        self.weights = nn.Parameter(torch.FloatTensor(out_feature, in_feature))\n",
        "        nn.init.xavier_normal_(self.weights)\n",
        "\n",
        "        # for cos(th + m) = cos(th)cos(m) - sin(th)sin(m)\n",
        "        self.cos_m = math.cos(m)\n",
        "        self.sin_m = math.sin(m)\n",
        "\n",
        "        # see (*)\n",
        "        self.th = math.cos(math.pi - m)\n",
        "        self.mm = math.sin(math.pi - m) * m\n",
        "\n",
        "    def forward(self, embending, labels=None):\n",
        "        cos_th = linear(normalize(embending), normalize(self.weights))\n",
        "        sin_th = torch.sqrt(1 - torch.pow(cos_th, 2))\n",
        "\n",
        "        #cos(th + m) = cos(th)cos(m) - sin(th)sin(m)\n",
        "        cos_th_m = cos_th * self.cos_m - sin_th * self.sin_m\n",
        "\n",
        "        # see (*)\n",
        "        cos_th_m = torch.where(cos_th > self.th, cos_th_m, cos_th - self.mm)\n",
        "\n",
        "        onehot = torch.zeros(cos_th.size(), device='cuda')\n",
        "        onehot.scatter_(1, labels.view(-1, 1).long(), 1)\n",
        "        # if it target class when cos(th + m else cos(th)\n",
        "        outputs = onehot * cos_th_m + (1 - onehot) * cos_th\n",
        "        outputs *= self.s\n",
        "        return outputs"
      ],
      "metadata": {
        "id": "iSLPlUzqQ967"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "(*) https://habr.com/ru/companies/ntechlab/articles/531842/ [Сравнение функций потерь]"
      ],
      "metadata": {
        "id": "ELhng5yUmvzf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision.models import mobilenet_v3_large\n",
        "\n",
        "def block(in_channel, out_channel):\n",
        "    return nn.Sequential(nn.ReLU(True),\n",
        "                         nn.BatchNorm1d(in_channel),\n",
        "                         nn.Linear(in_channel, out_channel))\n",
        "\n",
        "class ArcMobileNet(nn.Module):\n",
        "    def __init__(self, embending_size, num_class, s, m, weights=None):\n",
        "        super(ArcMobileNet, self).__init__()\n",
        "\n",
        "        self.backbone = mobilenet_v3_large(weights=weights)\n",
        "        self.backbone.classifier = self.backbone.classifier[0]\n",
        "\n",
        "        in_features = self.backbone.classifier.out_features\n",
        "        self.linear_block = block(in_features, embending_size)\n",
        "        self.arcface = ArcFace(embending_size, num_class, s, m)\n",
        "\n",
        "    def forward(self, image, labels=None):\n",
        "        # get result from last linear layer\n",
        "        embending = self.backbone(image)\n",
        "        # resize vector size to size embending\n",
        "        embending = self.linear_block(embending)\n",
        "\n",
        "        if labels is not None:\n",
        "             return self.arcface(embending, labels)\n",
        "        else:\n",
        "            return embending\n"
      ],
      "metadata": {
        "id": "7NmlU93Nd_09"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "import multiprocessing\n",
        "\n",
        "NUM_WORKER = multiprocessing.cpu_count()\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "celebA = CelebADataSet(ptrain=85)\n",
        "train, test = celebA.get_train_and_test()\n",
        "\n",
        "train_loader = DataLoader(train, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKER)\n",
        "test_loader = DataLoader(test, batch_size=BATCH_SIZE, num_workers=NUM_WORKER)"
      ],
      "metadata": {
        "id": "2fEPKp4joi22"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision.models import MobileNet_V3_Large_Weights\n",
        "\n",
        "NUM_CLASS = len(celebA.label_encoder.classes_)\n",
        "s = 8 # math.sqrt(2) * math.log(NUM_CLASS - 1)\n",
        "margin = 0.2\n",
        "\n",
        "arc_model = ArcMobileNet(embending_size=256, num_class=NUM_CLASS, s=s, m=margin, weights=MobileNet_V3_Large_Weights.IMAGENET1K_V2)"
      ],
      "metadata": {
        "id": "Q7O0-RobpUZs"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = torch.optim.AdamW(arc_model.parameters())\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 3, 0.5)\n",
        "loss_fn = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "arc_model = arc_model.to(DEVICE)"
      ],
      "metadata": {
        "id": "0DtVb74p7Lo8"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cuda.empty_cache()\n",
        "\n",
        "NUM_EPOCH = 25\n",
        "history = train_model(arc_model, train_loader, test_loader, NUM_EPOCH, loss_fn, optimizer, scheduler, DEVICE)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZHiTsaJGrwrg",
        "outputId": "04bd250e-c6eb-43c2-df64-5a49f6ba0d63"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/25\n",
            "batch: 1/160 train loss: 7.8696 train acc 0.0000\n",
            "batch: 2/160 train loss: 7.8124 train acc 0.0000\n",
            "batch: 3/160 train loss: 7.8044 train acc 0.0000\n",
            "batch: 4/160 train loss: 7.8154 train acc 0.0000\n",
            "batch: 5/160 train loss: 7.7847 train acc 0.0000\n",
            "batch: 6/160 train loss: 7.7818 train acc 0.0000\n",
            "batch: 7/160 train loss: 7.7662 train acc 0.0000\n",
            "batch: 8/160 train loss: 7.7477 train acc 0.0000\n",
            "batch: 9/160 train loss: 7.7410 train acc 0.0000\n",
            "batch: 10/160 train loss: 7.7378 train acc 0.0000\n",
            "batch: 11/160 train loss: 7.7362 train acc 0.0000\n",
            "batch: 12/160 train loss: 7.7157 train acc 0.0000\n",
            "batch: 13/160 train loss: 7.7110 train acc 0.0000\n",
            "batch: 14/160 train loss: 7.7005 train acc 0.0000\n",
            "batch: 15/160 train loss: 7.6933 train acc 0.0000\n",
            "batch: 16/160 train loss: 7.6794 train acc 0.0000\n",
            "batch: 17/160 train loss: 7.6812 train acc 0.0000\n",
            "batch: 18/160 train loss: 7.6790 train acc 0.0000\n",
            "batch: 19/160 train loss: 7.6718 train acc 0.0000\n",
            "batch: 20/160 train loss: 7.6739 train acc 0.0000\n",
            "batch: 21/160 train loss: 7.6668 train acc 0.0000\n",
            "batch: 22/160 train loss: 7.6561 train acc 0.0000\n",
            "batch: 23/160 train loss: 7.6482 train acc 0.0000\n",
            "batch: 24/160 train loss: 7.6416 train acc 0.0000\n",
            "batch: 25/160 train loss: 7.6385 train acc 0.0000\n",
            "batch: 26/160 train loss: 7.6311 train acc 0.0000\n",
            "batch: 27/160 train loss: 7.6262 train acc 0.0000\n",
            "batch: 28/160 train loss: 7.6183 train acc 0.0000\n",
            "batch: 29/160 train loss: 7.6135 train acc 0.0000\n",
            "batch: 30/160 train loss: 7.6055 train acc 0.0000\n",
            "batch: 31/160 train loss: 7.5955 train acc 0.0000\n",
            "batch: 32/160 train loss: 7.5894 train acc 0.0000\n",
            "batch: 33/160 train loss: 7.5829 train acc 0.0000\n",
            "batch: 34/160 train loss: 7.5716 train acc 0.0000\n",
            "batch: 35/160 train loss: 7.5652 train acc 0.0000\n",
            "batch: 36/160 train loss: 7.5590 train acc 0.0000\n",
            "batch: 37/160 train loss: 7.5453 train acc 0.0004\n",
            "batch: 38/160 train loss: 7.5407 train acc 0.0004\n",
            "batch: 39/160 train loss: 7.5329 train acc 0.0004\n",
            "batch: 40/160 train loss: 7.5273 train acc 0.0004\n",
            "batch: 41/160 train loss: 7.5173 train acc 0.0004\n",
            "batch: 42/160 train loss: 7.5117 train acc 0.0004\n",
            "batch: 43/160 train loss: 7.5052 train acc 0.0004\n",
            "batch: 44/160 train loss: 7.5004 train acc 0.0004\n",
            "batch: 45/160 train loss: 7.4920 train acc 0.0003\n",
            "batch: 46/160 train loss: 7.4831 train acc 0.0003\n",
            "batch: 47/160 train loss: 7.4738 train acc 0.0003\n",
            "batch: 48/160 train loss: 7.4702 train acc 0.0003\n",
            "batch: 49/160 train loss: 7.4626 train acc 0.0003\n",
            "batch: 50/160 train loss: 7.4527 train acc 0.0003\n",
            "batch: 51/160 train loss: 7.4467 train acc 0.0003\n",
            "batch: 52/160 train loss: 7.4391 train acc 0.0003\n",
            "batch: 53/160 train loss: 7.4336 train acc 0.0003\n",
            "batch: 54/160 train loss: 7.4238 train acc 0.0003\n",
            "batch: 55/160 train loss: 7.4137 train acc 0.0003\n",
            "batch: 56/160 train loss: 7.4023 train acc 0.0003\n",
            "batch: 57/160 train loss: 7.3934 train acc 0.0003\n",
            "batch: 58/160 train loss: 7.3842 train acc 0.0003\n",
            "batch: 59/160 train loss: 7.3781 train acc 0.0003\n",
            "batch: 60/160 train loss: 7.3711 train acc 0.0005\n",
            "batch: 61/160 train loss: 7.3626 train acc 0.0005\n",
            "batch: 62/160 train loss: 7.3573 train acc 0.0005\n",
            "batch: 63/160 train loss: 7.3494 train acc 0.0007\n",
            "batch: 64/160 train loss: 7.3430 train acc 0.0007\n",
            "batch: 65/160 train loss: 7.3400 train acc 0.0007\n",
            "batch: 66/160 train loss: 7.3321 train acc 0.0009\n",
            "batch: 67/160 train loss: 7.3200 train acc 0.0009\n",
            "batch: 68/160 train loss: 7.3148 train acc 0.0009\n",
            "batch: 69/160 train loss: 7.3091 train acc 0.0009\n",
            "batch: 70/160 train loss: 7.3003 train acc 0.0009\n",
            "batch: 71/160 train loss: 7.2921 train acc 0.0009\n",
            "batch: 72/160 train loss: 7.2863 train acc 0.0011\n",
            "batch: 73/160 train loss: 7.2808 train acc 0.0011\n",
            "batch: 74/160 train loss: 7.2732 train acc 0.0011\n",
            "batch: 75/160 train loss: 7.2662 train acc 0.0010\n",
            "batch: 76/160 train loss: 7.2607 train acc 0.0010\n",
            "batch: 77/160 train loss: 7.2553 train acc 0.0010\n",
            "batch: 78/160 train loss: 7.2485 train acc 0.0010\n",
            "batch: 79/160 train loss: 7.2371 train acc 0.0010\n",
            "batch: 80/160 train loss: 7.2319 train acc 0.0010\n",
            "batch: 81/160 train loss: 7.2222 train acc 0.0010\n",
            "batch: 82/160 train loss: 7.2164 train acc 0.0010\n",
            "batch: 83/160 train loss: 7.2097 train acc 0.0009\n",
            "batch: 84/160 train loss: 7.2038 train acc 0.0011\n",
            "batch: 85/160 train loss: 7.1968 train acc 0.0011\n",
            "batch: 86/160 train loss: 7.1873 train acc 0.0013\n",
            "batch: 87/160 train loss: 7.1787 train acc 0.0013\n",
            "batch: 88/160 train loss: 7.1731 train acc 0.0012\n",
            "batch: 89/160 train loss: 7.1674 train acc 0.0014\n",
            "batch: 90/160 train loss: 7.1602 train acc 0.0016\n",
            "batch: 91/160 train loss: 7.1525 train acc 0.0015\n",
            "batch: 92/160 train loss: 7.1427 train acc 0.0015\n",
            "batch: 93/160 train loss: 7.1358 train acc 0.0017\n",
            "batch: 94/160 train loss: 7.1291 train acc 0.0017\n",
            "batch: 95/160 train loss: 7.1220 train acc 0.0016\n",
            "batch: 96/160 train loss: 7.1129 train acc 0.0018\n",
            "batch: 97/160 train loss: 7.1050 train acc 0.0019\n",
            "batch: 98/160 train loss: 7.0982 train acc 0.0022\n",
            "batch: 99/160 train loss: 7.0932 train acc 0.0022\n",
            "batch: 100/160 train loss: 7.0873 train acc 0.0022\n",
            "batch: 101/160 train loss: 7.0799 train acc 0.0023\n",
            "batch: 102/160 train loss: 7.0731 train acc 0.0023\n",
            "batch: 103/160 train loss: 7.0676 train acc 0.0024\n",
            "batch: 104/160 train loss: 7.0600 train acc 0.0027\n",
            "batch: 105/160 train loss: 7.0539 train acc 0.0027\n",
            "batch: 106/160 train loss: 7.0466 train acc 0.0027\n",
            "batch: 107/160 train loss: 7.0400 train acc 0.0026\n",
            "batch: 108/160 train loss: 7.0348 train acc 0.0026\n",
            "batch: 109/160 train loss: 7.0279 train acc 0.0026\n",
            "batch: 110/160 train loss: 7.0205 train acc 0.0026\n",
            "batch: 111/160 train loss: 7.0138 train acc 0.0025\n",
            "batch: 112/160 train loss: 7.0072 train acc 0.0027\n",
            "batch: 113/160 train loss: 7.0015 train acc 0.0028\n",
            "batch: 114/160 train loss: 6.9939 train acc 0.0029\n",
            "batch: 115/160 train loss: 6.9887 train acc 0.0029\n",
            "batch: 116/160 train loss: 6.9814 train acc 0.0028\n",
            "batch: 117/160 train loss: 6.9747 train acc 0.0028\n",
            "batch: 118/160 train loss: 6.9684 train acc 0.0030\n",
            "batch: 119/160 train loss: 6.9614 train acc 0.0030\n",
            "batch: 120/160 train loss: 6.9554 train acc 0.0030\n",
            "batch: 121/160 train loss: 6.9485 train acc 0.0030\n",
            "batch: 122/160 train loss: 6.9449 train acc 0.0031\n",
            "batch: 123/160 train loss: 6.9391 train acc 0.0032\n",
            "batch: 124/160 train loss: 6.9317 train acc 0.0032\n",
            "batch: 125/160 train loss: 6.9255 train acc 0.0032\n",
            "batch: 126/160 train loss: 6.9193 train acc 0.0033\n",
            "batch: 127/160 train loss: 6.9116 train acc 0.0034\n",
            "batch: 128/160 train loss: 6.9052 train acc 0.0034\n",
            "batch: 129/160 train loss: 6.8986 train acc 0.0034\n",
            "batch: 130/160 train loss: 6.8958 train acc 0.0034\n",
            "batch: 131/160 train loss: 6.8907 train acc 0.0035\n",
            "batch: 132/160 train loss: 6.8856 train acc 0.0034\n",
            "batch: 133/160 train loss: 6.8805 train acc 0.0034\n",
            "batch: 134/160 train loss: 6.8734 train acc 0.0036\n",
            "batch: 135/160 train loss: 6.8666 train acc 0.0036\n",
            "batch: 136/160 train loss: 6.8602 train acc 0.0037\n",
            "batch: 137/160 train loss: 6.8553 train acc 0.0038\n",
            "batch: 138/160 train loss: 6.8482 train acc 0.0038\n",
            "batch: 139/160 train loss: 6.8431 train acc 0.0038\n",
            "batch: 140/160 train loss: 6.8365 train acc 0.0039\n",
            "batch: 141/160 train loss: 6.8313 train acc 0.0039\n",
            "batch: 142/160 train loss: 6.8244 train acc 0.0040\n",
            "batch: 143/160 train loss: 6.8180 train acc 0.0040\n",
            "batch: 144/160 train loss: 6.8114 train acc 0.0040\n",
            "batch: 145/160 train loss: 6.8075 train acc 0.0041\n",
            "batch: 146/160 train loss: 6.8032 train acc 0.0041\n",
            "batch: 147/160 train loss: 6.7983 train acc 0.0040\n",
            "batch: 148/160 train loss: 6.7926 train acc 0.0043\n",
            "batch: 149/160 train loss: 6.7860 train acc 0.0046\n",
            "batch: 150/160 train loss: 6.7811 train acc 0.0046\n",
            "batch: 151/160 train loss: 6.7753 train acc 0.0046\n",
            "batch: 152/160 train loss: 6.7717 train acc 0.0045\n",
            "batch: 153/160 train loss: 6.7666 train acc 0.0046\n",
            "batch: 154/160 train loss: 6.7608 train acc 0.0049\n",
            "batch: 155/160 train loss: 6.7561 train acc 0.0049\n",
            "batch: 156/160 train loss: 6.7536 train acc 0.0049\n",
            "batch: 157/160 train loss: 6.7451 train acc 0.0056\n",
            "batch: 158/160 train loss: 6.7407 train acc 0.0057\n",
            "batch: 159/160 train loss: 6.7349 train acc 0.0060\n",
            "batch: 160/160 train loss: 6.7330 train acc 0.0060\n",
            "\n",
            "Epoch 1 train loss: 6.7330 test loss 6.1079 train acc 0.0060 test acc 0.0158\n",
            "Epoch 2/25\n",
            "batch: 1/160 train loss: 5.5270 train acc 0.0156\n",
            "batch: 2/160 train loss: 5.6147 train acc 0.0234\n",
            "batch: 3/160 train loss: 5.5997 train acc 0.0260\n",
            "batch: 4/160 train loss: 5.5474 train acc 0.0234\n",
            "batch: 5/160 train loss: 5.5498 train acc 0.0250\n",
            "batch: 6/160 train loss: 5.5385 train acc 0.0339\n",
            "batch: 7/160 train loss: 5.5252 train acc 0.0357\n",
            "batch: 8/160 train loss: 5.5028 train acc 0.0352\n",
            "batch: 9/160 train loss: 5.4849 train acc 0.0365\n",
            "batch: 10/160 train loss: 5.4965 train acc 0.0375\n",
            "batch: 11/160 train loss: 5.4741 train acc 0.0369\n",
            "batch: 12/160 train loss: 5.4835 train acc 0.0378\n",
            "batch: 13/160 train loss: 5.4716 train acc 0.0385\n",
            "batch: 14/160 train loss: 5.4579 train acc 0.0391\n",
            "batch: 15/160 train loss: 5.4561 train acc 0.0385\n",
            "batch: 16/160 train loss: 5.4487 train acc 0.0381\n",
            "batch: 17/160 train loss: 5.4421 train acc 0.0377\n",
            "batch: 18/160 train loss: 5.4365 train acc 0.0373\n",
            "batch: 19/160 train loss: 5.4188 train acc 0.0387\n",
            "batch: 20/160 train loss: 5.4238 train acc 0.0367\n",
            "batch: 21/160 train loss: 5.4146 train acc 0.0372\n",
            "batch: 22/160 train loss: 5.4096 train acc 0.0398\n",
            "batch: 23/160 train loss: 5.4112 train acc 0.0394\n",
            "batch: 24/160 train loss: 5.4082 train acc 0.0384\n",
            "batch: 25/160 train loss: 5.4007 train acc 0.0394\n",
            "batch: 26/160 train loss: 5.3894 train acc 0.0403\n",
            "batch: 27/160 train loss: 5.3913 train acc 0.0394\n",
            "batch: 28/160 train loss: 5.3936 train acc 0.0379\n",
            "batch: 29/160 train loss: 5.3914 train acc 0.0393\n",
            "batch: 30/160 train loss: 5.3829 train acc 0.0401\n",
            "batch: 31/160 train loss: 5.3885 train acc 0.0398\n",
            "batch: 32/160 train loss: 5.3731 train acc 0.0435\n",
            "batch: 33/160 train loss: 5.3656 train acc 0.0436\n",
            "batch: 34/160 train loss: 5.3648 train acc 0.0427\n",
            "batch: 35/160 train loss: 5.3679 train acc 0.0429\n",
            "batch: 36/160 train loss: 5.3588 train acc 0.0425\n",
            "batch: 37/160 train loss: 5.3545 train acc 0.0431\n",
            "batch: 38/160 train loss: 5.3522 train acc 0.0424\n",
            "batch: 39/160 train loss: 5.3424 train acc 0.0429\n",
            "batch: 40/160 train loss: 5.3412 train acc 0.0426\n",
            "batch: 41/160 train loss: 5.3376 train acc 0.0431\n",
            "batch: 42/160 train loss: 5.3369 train acc 0.0420\n",
            "batch: 43/160 train loss: 5.3350 train acc 0.0425\n",
            "batch: 44/160 train loss: 5.3357 train acc 0.0419\n",
            "batch: 45/160 train loss: 5.3324 train acc 0.0420\n",
            "batch: 46/160 train loss: 5.3276 train acc 0.0421\n",
            "batch: 47/160 train loss: 5.3247 train acc 0.0422\n",
            "batch: 48/160 train loss: 5.3247 train acc 0.0430\n",
            "batch: 49/160 train loss: 5.3242 train acc 0.0440\n",
            "batch: 50/160 train loss: 5.3211 train acc 0.0441\n",
            "batch: 51/160 train loss: 5.3179 train acc 0.0441\n",
            "batch: 52/160 train loss: 5.3174 train acc 0.0442\n",
            "batch: 53/160 train loss: 5.3109 train acc 0.0439\n",
            "batch: 54/160 train loss: 5.3055 train acc 0.0446\n",
            "batch: 55/160 train loss: 5.3036 train acc 0.0443\n",
            "batch: 56/160 train loss: 5.2941 train acc 0.0460\n",
            "batch: 57/160 train loss: 5.2932 train acc 0.0455\n",
            "batch: 58/160 train loss: 5.2896 train acc 0.0458\n",
            "batch: 59/160 train loss: 5.2858 train acc 0.0453\n",
            "batch: 60/160 train loss: 5.2813 train acc 0.0466\n",
            "batch: 61/160 train loss: 5.2831 train acc 0.0466\n",
            "batch: 62/160 train loss: 5.2761 train acc 0.0471\n",
            "batch: 63/160 train loss: 5.2713 train acc 0.0484\n",
            "batch: 64/160 train loss: 5.2662 train acc 0.0491\n",
            "batch: 65/160 train loss: 5.2655 train acc 0.0490\n",
            "batch: 66/160 train loss: 5.2630 train acc 0.0492\n",
            "batch: 67/160 train loss: 5.2626 train acc 0.0492\n",
            "batch: 68/160 train loss: 5.2586 train acc 0.0496\n",
            "batch: 69/160 train loss: 5.2540 train acc 0.0500\n",
            "batch: 70/160 train loss: 5.2537 train acc 0.0504\n",
            "batch: 71/160 train loss: 5.2516 train acc 0.0506\n",
            "batch: 72/160 train loss: 5.2498 train acc 0.0506\n",
            "batch: 73/160 train loss: 5.2440 train acc 0.0512\n",
            "batch: 74/160 train loss: 5.2406 train acc 0.0519\n",
            "batch: 75/160 train loss: 5.2392 train acc 0.0521\n",
            "batch: 76/160 train loss: 5.2367 train acc 0.0522\n",
            "batch: 77/160 train loss: 5.2330 train acc 0.0530\n",
            "batch: 78/160 train loss: 5.2306 train acc 0.0527\n",
            "batch: 79/160 train loss: 5.2280 train acc 0.0528\n",
            "batch: 80/160 train loss: 5.2251 train acc 0.0531\n",
            "batch: 81/160 train loss: 5.2232 train acc 0.0530\n",
            "batch: 82/160 train loss: 5.2173 train acc 0.0537\n",
            "batch: 83/160 train loss: 5.2138 train acc 0.0540\n",
            "batch: 84/160 train loss: 5.2092 train acc 0.0547\n",
            "batch: 85/160 train loss: 5.2056 train acc 0.0548\n",
            "batch: 86/160 train loss: 5.2026 train acc 0.0556\n",
            "batch: 87/160 train loss: 5.1976 train acc 0.0560\n",
            "batch: 88/160 train loss: 5.1948 train acc 0.0563\n",
            "batch: 89/160 train loss: 5.1887 train acc 0.0572\n",
            "batch: 90/160 train loss: 5.1880 train acc 0.0575\n",
            "batch: 91/160 train loss: 5.1847 train acc 0.0577\n",
            "batch: 92/160 train loss: 5.1795 train acc 0.0589\n",
            "batch: 93/160 train loss: 5.1768 train acc 0.0593\n",
            "batch: 94/160 train loss: 5.1728 train acc 0.0602\n",
            "batch: 95/160 train loss: 5.1707 train acc 0.0602\n",
            "batch: 96/160 train loss: 5.1703 train acc 0.0601\n",
            "batch: 97/160 train loss: 5.1704 train acc 0.0599\n",
            "batch: 98/160 train loss: 5.1656 train acc 0.0604\n",
            "batch: 99/160 train loss: 5.1615 train acc 0.0604\n",
            "batch: 100/160 train loss: 5.1587 train acc 0.0603\n",
            "batch: 101/160 train loss: 5.1600 train acc 0.0602\n",
            "batch: 102/160 train loss: 5.1600 train acc 0.0599\n",
            "batch: 103/160 train loss: 5.1576 train acc 0.0599\n",
            "batch: 104/160 train loss: 5.1541 train acc 0.0602\n",
            "batch: 105/160 train loss: 5.1509 train acc 0.0609\n",
            "batch: 106/160 train loss: 5.1499 train acc 0.0609\n",
            "batch: 107/160 train loss: 5.1449 train acc 0.0613\n",
            "batch: 108/160 train loss: 5.1436 train acc 0.0612\n",
            "batch: 109/160 train loss: 5.1377 train acc 0.0621\n",
            "batch: 110/160 train loss: 5.1311 train acc 0.0632\n",
            "batch: 111/160 train loss: 5.1278 train acc 0.0635\n",
            "batch: 112/160 train loss: 5.1241 train acc 0.0643\n",
            "batch: 113/160 train loss: 5.1222 train acc 0.0646\n",
            "batch: 114/160 train loss: 5.1181 train acc 0.0651\n",
            "batch: 115/160 train loss: 5.1135 train acc 0.0660\n",
            "batch: 116/160 train loss: 5.1105 train acc 0.0660\n",
            "batch: 117/160 train loss: 5.1070 train acc 0.0661\n",
            "batch: 118/160 train loss: 5.1027 train acc 0.0663\n",
            "batch: 119/160 train loss: 5.1009 train acc 0.0663\n",
            "batch: 120/160 train loss: 5.0995 train acc 0.0668\n",
            "batch: 121/160 train loss: 5.1002 train acc 0.0668\n",
            "batch: 122/160 train loss: 5.0990 train acc 0.0666\n",
            "batch: 123/160 train loss: 5.0977 train acc 0.0666\n",
            "batch: 124/160 train loss: 5.0948 train acc 0.0669\n",
            "batch: 125/160 train loss: 5.0936 train acc 0.0670\n",
            "batch: 126/160 train loss: 5.0927 train acc 0.0666\n",
            "batch: 127/160 train loss: 5.0876 train acc 0.0674\n",
            "batch: 128/160 train loss: 5.0850 train acc 0.0677\n",
            "batch: 129/160 train loss: 5.0805 train acc 0.0682\n",
            "batch: 130/160 train loss: 5.0740 train acc 0.0688\n",
            "batch: 131/160 train loss: 5.0703 train acc 0.0688\n",
            "batch: 132/160 train loss: 5.0670 train acc 0.0696\n",
            "batch: 133/160 train loss: 5.0637 train acc 0.0697\n",
            "batch: 134/160 train loss: 5.0636 train acc 0.0695\n",
            "batch: 135/160 train loss: 5.0599 train acc 0.0696\n",
            "batch: 136/160 train loss: 5.0563 train acc 0.0696\n",
            "batch: 137/160 train loss: 5.0524 train acc 0.0703\n",
            "batch: 138/160 train loss: 5.0506 train acc 0.0702\n",
            "batch: 139/160 train loss: 5.0459 train acc 0.0709\n",
            "batch: 140/160 train loss: 5.0436 train acc 0.0710\n",
            "batch: 141/160 train loss: 5.0413 train acc 0.0715\n",
            "batch: 142/160 train loss: 5.0403 train acc 0.0715\n",
            "batch: 143/160 train loss: 5.0363 train acc 0.0719\n",
            "batch: 144/160 train loss: 5.0355 train acc 0.0719\n",
            "batch: 145/160 train loss: 5.0320 train acc 0.0723\n",
            "batch: 146/160 train loss: 5.0301 train acc 0.0726\n",
            "batch: 147/160 train loss: 5.0278 train acc 0.0731\n",
            "batch: 148/160 train loss: 5.0271 train acc 0.0731\n",
            "batch: 149/160 train loss: 5.0228 train acc 0.0732\n",
            "batch: 150/160 train loss: 5.0164 train acc 0.0740\n",
            "batch: 151/160 train loss: 5.0142 train acc 0.0740\n",
            "batch: 152/160 train loss: 5.0117 train acc 0.0743\n",
            "batch: 153/160 train loss: 5.0099 train acc 0.0744\n",
            "batch: 154/160 train loss: 5.0063 train acc 0.0747\n",
            "batch: 155/160 train loss: 5.0022 train acc 0.0748\n",
            "batch: 156/160 train loss: 4.9988 train acc 0.0750\n",
            "batch: 157/160 train loss: 4.9954 train acc 0.0752\n",
            "batch: 158/160 train loss: 4.9933 train acc 0.0755\n",
            "batch: 159/160 train loss: 4.9911 train acc 0.0757\n",
            "batch: 160/160 train loss: 4.9907 train acc 0.0756\n",
            "\n",
            "Epoch 2 train loss: 4.9907 test loss 4.8953 train acc 0.0756 test acc 0.0883\n",
            "Epoch 3/25\n",
            "batch: 1/160 train loss: 4.0469 train acc 0.1875\n",
            "batch: 2/160 train loss: 3.9956 train acc 0.1953\n",
            "batch: 3/160 train loss: 4.0105 train acc 0.1771\n",
            "batch: 4/160 train loss: 3.9995 train acc 0.1758\n",
            "batch: 5/160 train loss: 3.9728 train acc 0.1750\n",
            "batch: 6/160 train loss: 3.9326 train acc 0.1927\n",
            "batch: 7/160 train loss: 3.9431 train acc 0.1964\n",
            "batch: 8/160 train loss: 3.9458 train acc 0.1895\n",
            "batch: 9/160 train loss: 3.9516 train acc 0.1927\n",
            "batch: 10/160 train loss: 3.9777 train acc 0.1891\n",
            "batch: 11/160 train loss: 3.9909 train acc 0.1875\n",
            "batch: 12/160 train loss: 3.9909 train acc 0.1875\n",
            "batch: 13/160 train loss: 4.0274 train acc 0.1827\n",
            "batch: 14/160 train loss: 4.0159 train acc 0.1853\n",
            "batch: 15/160 train loss: 4.0110 train acc 0.1875\n",
            "batch: 16/160 train loss: 4.0124 train acc 0.1865\n",
            "batch: 17/160 train loss: 4.0069 train acc 0.1921\n",
            "batch: 18/160 train loss: 4.0096 train acc 0.1918\n",
            "batch: 19/160 train loss: 4.0127 train acc 0.1957\n",
            "batch: 20/160 train loss: 4.0104 train acc 0.1969\n",
            "batch: 21/160 train loss: 4.0067 train acc 0.1964\n",
            "batch: 22/160 train loss: 4.0145 train acc 0.1974\n",
            "batch: 23/160 train loss: 4.0117 train acc 0.1990\n",
            "batch: 24/160 train loss: 4.0111 train acc 0.1986\n",
            "batch: 25/160 train loss: 4.0109 train acc 0.1969\n",
            "batch: 26/160 train loss: 4.0058 train acc 0.1983\n",
            "batch: 27/160 train loss: 4.0014 train acc 0.1991\n",
            "batch: 28/160 train loss: 3.9996 train acc 0.1975\n",
            "batch: 29/160 train loss: 4.0040 train acc 0.1977\n",
            "batch: 30/160 train loss: 4.0076 train acc 0.1958\n",
            "batch: 31/160 train loss: 4.0006 train acc 0.1991\n",
            "batch: 32/160 train loss: 4.0138 train acc 0.1958\n",
            "batch: 33/160 train loss: 4.0064 train acc 0.1965\n",
            "batch: 34/160 train loss: 4.0014 train acc 0.1953\n",
            "batch: 35/160 train loss: 3.9993 train acc 0.1973\n",
            "batch: 36/160 train loss: 4.0020 train acc 0.1966\n",
            "batch: 37/160 train loss: 3.9911 train acc 0.1985\n",
            "batch: 38/160 train loss: 3.9878 train acc 0.1994\n",
            "batch: 39/160 train loss: 3.9908 train acc 0.1983\n",
            "batch: 40/160 train loss: 3.9885 train acc 0.1969\n",
            "batch: 41/160 train loss: 3.9842 train acc 0.1974\n",
            "batch: 42/160 train loss: 3.9832 train acc 0.1964\n",
            "batch: 43/160 train loss: 3.9836 train acc 0.1951\n",
            "batch: 44/160 train loss: 3.9878 train acc 0.1950\n",
            "batch: 45/160 train loss: 3.9851 train acc 0.1965\n",
            "batch: 46/160 train loss: 3.9882 train acc 0.1967\n",
            "batch: 47/160 train loss: 3.9887 train acc 0.1955\n",
            "batch: 48/160 train loss: 3.9888 train acc 0.1956\n",
            "batch: 49/160 train loss: 3.9788 train acc 0.1974\n",
            "batch: 50/160 train loss: 3.9832 train acc 0.1966\n",
            "batch: 51/160 train loss: 3.9800 train acc 0.1970\n",
            "batch: 52/160 train loss: 3.9764 train acc 0.1983\n",
            "batch: 53/160 train loss: 3.9700 train acc 0.1984\n",
            "batch: 54/160 train loss: 3.9699 train acc 0.1991\n",
            "batch: 55/160 train loss: 3.9690 train acc 0.2000\n",
            "batch: 56/160 train loss: 3.9659 train acc 0.1995\n",
            "batch: 57/160 train loss: 3.9647 train acc 0.1998\n",
            "batch: 58/160 train loss: 3.9617 train acc 0.2007\n",
            "batch: 59/160 train loss: 3.9581 train acc 0.2013\n",
            "batch: 60/160 train loss: 3.9585 train acc 0.2003\n",
            "batch: 61/160 train loss: 3.9568 train acc 0.2003\n",
            "batch: 62/160 train loss: 3.9570 train acc 0.2006\n",
            "batch: 63/160 train loss: 3.9561 train acc 0.2016\n",
            "batch: 64/160 train loss: 3.9587 train acc 0.2014\n",
            "batch: 65/160 train loss: 3.9596 train acc 0.2012\n",
            "batch: 66/160 train loss: 3.9614 train acc 0.2008\n",
            "batch: 67/160 train loss: 3.9590 train acc 0.2013\n",
            "batch: 68/160 train loss: 3.9610 train acc 0.2004\n",
            "batch: 69/160 train loss: 3.9669 train acc 0.1990\n",
            "batch: 70/160 train loss: 3.9696 train acc 0.1973\n",
            "batch: 71/160 train loss: 3.9683 train acc 0.1976\n",
            "batch: 72/160 train loss: 3.9630 train acc 0.1992\n",
            "batch: 73/160 train loss: 3.9614 train acc 0.1991\n",
            "batch: 74/160 train loss: 3.9583 train acc 0.2002\n",
            "batch: 75/160 train loss: 3.9564 train acc 0.1998\n",
            "batch: 76/160 train loss: 3.9570 train acc 0.2002\n",
            "batch: 77/160 train loss: 3.9555 train acc 0.2003\n",
            "batch: 78/160 train loss: 3.9546 train acc 0.2001\n",
            "batch: 79/160 train loss: 3.9559 train acc 0.2000\n",
            "batch: 80/160 train loss: 3.9544 train acc 0.2000\n",
            "batch: 81/160 train loss: 3.9536 train acc 0.2000\n",
            "batch: 82/160 train loss: 3.9528 train acc 0.1999\n",
            "batch: 83/160 train loss: 3.9528 train acc 0.1999\n",
            "batch: 84/160 train loss: 3.9514 train acc 0.2000\n",
            "batch: 85/160 train loss: 3.9491 train acc 0.2007\n",
            "batch: 86/160 train loss: 3.9472 train acc 0.2009\n",
            "batch: 87/160 train loss: 3.9481 train acc 0.2011\n",
            "batch: 88/160 train loss: 3.9493 train acc 0.2006\n",
            "batch: 89/160 train loss: 3.9521 train acc 0.2005\n",
            "batch: 90/160 train loss: 3.9519 train acc 0.2010\n",
            "batch: 91/160 train loss: 3.9511 train acc 0.2016\n",
            "batch: 92/160 train loss: 3.9523 train acc 0.2013\n",
            "batch: 93/160 train loss: 3.9522 train acc 0.2014\n",
            "batch: 94/160 train loss: 3.9502 train acc 0.2021\n",
            "batch: 95/160 train loss: 3.9492 train acc 0.2021\n",
            "batch: 96/160 train loss: 3.9478 train acc 0.2025\n",
            "batch: 97/160 train loss: 3.9466 train acc 0.2015\n",
            "batch: 98/160 train loss: 3.9496 train acc 0.2006\n",
            "batch: 99/160 train loss: 3.9496 train acc 0.2008\n",
            "batch: 100/160 train loss: 3.9517 train acc 0.2006\n",
            "batch: 101/160 train loss: 3.9471 train acc 0.2020\n",
            "batch: 102/160 train loss: 3.9473 train acc 0.2022\n",
            "batch: 103/160 train loss: 3.9456 train acc 0.2022\n",
            "batch: 104/160 train loss: 3.9437 train acc 0.2030\n",
            "batch: 105/160 train loss: 3.9431 train acc 0.2025\n",
            "batch: 106/160 train loss: 3.9387 train acc 0.2036\n",
            "batch: 107/160 train loss: 3.9396 train acc 0.2030\n",
            "batch: 108/160 train loss: 3.9405 train acc 0.2027\n",
            "batch: 109/160 train loss: 3.9392 train acc 0.2031\n",
            "batch: 110/160 train loss: 3.9375 train acc 0.2040\n",
            "batch: 111/160 train loss: 3.9348 train acc 0.2045\n",
            "batch: 112/160 train loss: 3.9295 train acc 0.2058\n",
            "batch: 113/160 train loss: 3.9285 train acc 0.2059\n",
            "batch: 114/160 train loss: 3.9268 train acc 0.2059\n",
            "batch: 115/160 train loss: 3.9269 train acc 0.2058\n",
            "batch: 116/160 train loss: 3.9221 train acc 0.2066\n",
            "batch: 117/160 train loss: 3.9216 train acc 0.2069\n",
            "batch: 118/160 train loss: 3.9214 train acc 0.2068\n",
            "batch: 119/160 train loss: 3.9210 train acc 0.2068\n",
            "batch: 120/160 train loss: 3.9172 train acc 0.2076\n",
            "batch: 121/160 train loss: 3.9193 train acc 0.2069\n",
            "batch: 122/160 train loss: 3.9177 train acc 0.2074\n",
            "batch: 123/160 train loss: 3.9177 train acc 0.2074\n",
            "batch: 124/160 train loss: 3.9162 train acc 0.2080\n",
            "batch: 125/160 train loss: 3.9153 train acc 0.2081\n",
            "batch: 126/160 train loss: 3.9127 train acc 0.2086\n",
            "batch: 127/160 train loss: 3.9120 train acc 0.2083\n",
            "batch: 128/160 train loss: 3.9093 train acc 0.2089\n",
            "batch: 129/160 train loss: 3.9080 train acc 0.2093\n",
            "batch: 130/160 train loss: 3.9065 train acc 0.2094\n",
            "batch: 131/160 train loss: 3.9080 train acc 0.2087\n",
            "batch: 132/160 train loss: 3.9078 train acc 0.2090\n",
            "batch: 133/160 train loss: 3.9080 train acc 0.2091\n",
            "batch: 134/160 train loss: 3.9086 train acc 0.2091\n",
            "batch: 135/160 train loss: 3.9071 train acc 0.2094\n",
            "batch: 136/160 train loss: 3.9044 train acc 0.2094\n",
            "batch: 137/160 train loss: 3.9033 train acc 0.2093\n",
            "batch: 138/160 train loss: 3.9004 train acc 0.2097\n",
            "batch: 139/160 train loss: 3.8959 train acc 0.2110\n",
            "batch: 140/160 train loss: 3.8960 train acc 0.2116\n",
            "batch: 141/160 train loss: 3.8951 train acc 0.2118\n",
            "batch: 142/160 train loss: 3.8947 train acc 0.2126\n",
            "batch: 143/160 train loss: 3.8937 train acc 0.2133\n",
            "batch: 144/160 train loss: 3.8927 train acc 0.2137\n",
            "batch: 145/160 train loss: 3.8913 train acc 0.2136\n",
            "batch: 146/160 train loss: 3.8898 train acc 0.2137\n",
            "batch: 147/160 train loss: 3.8875 train acc 0.2140\n",
            "batch: 148/160 train loss: 3.8856 train acc 0.2145\n",
            "batch: 149/160 train loss: 3.8851 train acc 0.2149\n",
            "batch: 150/160 train loss: 3.8841 train acc 0.2147\n",
            "batch: 151/160 train loss: 3.8810 train acc 0.2153\n",
            "batch: 152/160 train loss: 3.8802 train acc 0.2155\n",
            "batch: 153/160 train loss: 3.8787 train acc 0.2160\n",
            "batch: 154/160 train loss: 3.8749 train acc 0.2163\n",
            "batch: 155/160 train loss: 3.8722 train acc 0.2168\n",
            "batch: 156/160 train loss: 3.8696 train acc 0.2175\n",
            "batch: 157/160 train loss: 3.8683 train acc 0.2179\n",
            "batch: 158/160 train loss: 3.8679 train acc 0.2183\n",
            "batch: 159/160 train loss: 3.8689 train acc 0.2179\n",
            "batch: 160/160 train loss: 3.8692 train acc 0.2177\n",
            "\n",
            "Epoch 3 train loss: 3.8692 test loss 4.5093 train acc 0.2177 test acc 0.1250\n",
            "Epoch 4/25\n",
            "batch: 1/160 train loss: 3.1290 train acc 0.4062\n",
            "batch: 2/160 train loss: 3.1876 train acc 0.3750\n",
            "batch: 3/160 train loss: 3.2020 train acc 0.4010\n",
            "batch: 4/160 train loss: 3.1740 train acc 0.3945\n",
            "batch: 5/160 train loss: 3.1488 train acc 0.4062\n",
            "batch: 6/160 train loss: 3.1017 train acc 0.4141\n",
            "batch: 7/160 train loss: 3.0781 train acc 0.4241\n",
            "batch: 8/160 train loss: 3.1025 train acc 0.4160\n",
            "batch: 9/160 train loss: 3.0916 train acc 0.4149\n",
            "batch: 10/160 train loss: 3.0920 train acc 0.4109\n",
            "batch: 11/160 train loss: 3.0806 train acc 0.4134\n",
            "batch: 12/160 train loss: 3.0830 train acc 0.4115\n",
            "batch: 13/160 train loss: 3.0605 train acc 0.4171\n",
            "batch: 14/160 train loss: 3.0527 train acc 0.4185\n",
            "batch: 15/160 train loss: 3.0580 train acc 0.4156\n",
            "batch: 16/160 train loss: 3.0583 train acc 0.4170\n",
            "batch: 17/160 train loss: 3.0575 train acc 0.4136\n",
            "batch: 18/160 train loss: 3.0493 train acc 0.4158\n",
            "batch: 19/160 train loss: 3.0509 train acc 0.4153\n",
            "batch: 20/160 train loss: 3.0490 train acc 0.4109\n",
            "batch: 21/160 train loss: 3.0331 train acc 0.4167\n",
            "batch: 22/160 train loss: 3.0385 train acc 0.4148\n",
            "batch: 23/160 train loss: 3.0358 train acc 0.4151\n",
            "batch: 24/160 train loss: 3.0363 train acc 0.4147\n",
            "batch: 25/160 train loss: 3.0220 train acc 0.4169\n",
            "batch: 26/160 train loss: 3.0122 train acc 0.4201\n",
            "batch: 27/160 train loss: 3.0059 train acc 0.4230\n",
            "batch: 28/160 train loss: 2.9990 train acc 0.4241\n",
            "batch: 29/160 train loss: 2.9949 train acc 0.4256\n",
            "batch: 30/160 train loss: 2.9943 train acc 0.4281\n",
            "batch: 31/160 train loss: 2.9916 train acc 0.4274\n",
            "batch: 32/160 train loss: 2.9922 train acc 0.4268\n",
            "batch: 33/160 train loss: 2.9952 train acc 0.4252\n",
            "batch: 34/160 train loss: 2.9959 train acc 0.4237\n",
            "batch: 35/160 train loss: 2.9908 train acc 0.4263\n",
            "batch: 36/160 train loss: 2.9882 train acc 0.4253\n",
            "batch: 37/160 train loss: 2.9802 train acc 0.4278\n",
            "batch: 38/160 train loss: 2.9808 train acc 0.4285\n",
            "batch: 39/160 train loss: 2.9736 train acc 0.4307\n",
            "batch: 40/160 train loss: 2.9726 train acc 0.4309\n",
            "batch: 41/160 train loss: 2.9710 train acc 0.4318\n",
            "batch: 42/160 train loss: 2.9741 train acc 0.4297\n",
            "batch: 43/160 train loss: 2.9735 train acc 0.4310\n",
            "batch: 44/160 train loss: 2.9788 train acc 0.4293\n",
            "batch: 45/160 train loss: 2.9775 train acc 0.4288\n",
            "batch: 46/160 train loss: 2.9784 train acc 0.4304\n",
            "batch: 47/160 train loss: 2.9780 train acc 0.4295\n",
            "batch: 48/160 train loss: 2.9759 train acc 0.4307\n",
            "batch: 49/160 train loss: 2.9841 train acc 0.4283\n",
            "batch: 50/160 train loss: 2.9860 train acc 0.4266\n",
            "batch: 51/160 train loss: 2.9862 train acc 0.4265\n",
            "batch: 52/160 train loss: 2.9889 train acc 0.4255\n",
            "batch: 53/160 train loss: 2.9890 train acc 0.4269\n",
            "batch: 54/160 train loss: 2.9888 train acc 0.4274\n",
            "batch: 55/160 train loss: 2.9877 train acc 0.4267\n",
            "batch: 56/160 train loss: 2.9861 train acc 0.4263\n",
            "batch: 57/160 train loss: 2.9920 train acc 0.4252\n",
            "batch: 58/160 train loss: 2.9926 train acc 0.4265\n",
            "batch: 59/160 train loss: 2.9934 train acc 0.4261\n",
            "batch: 60/160 train loss: 2.9925 train acc 0.4266\n",
            "batch: 61/160 train loss: 2.9904 train acc 0.4267\n",
            "batch: 62/160 train loss: 2.9877 train acc 0.4277\n",
            "batch: 63/160 train loss: 2.9837 train acc 0.4286\n",
            "batch: 64/160 train loss: 2.9862 train acc 0.4285\n",
            "batch: 65/160 train loss: 2.9867 train acc 0.4284\n",
            "batch: 66/160 train loss: 2.9858 train acc 0.4287\n",
            "batch: 67/160 train loss: 2.9850 train acc 0.4291\n",
            "batch: 68/160 train loss: 2.9869 train acc 0.4288\n",
            "batch: 69/160 train loss: 2.9902 train acc 0.4280\n",
            "batch: 70/160 train loss: 2.9855 train acc 0.4295\n",
            "batch: 71/160 train loss: 2.9791 train acc 0.4322\n",
            "batch: 72/160 train loss: 2.9773 train acc 0.4327\n",
            "batch: 73/160 train loss: 2.9742 train acc 0.4332\n",
            "batch: 74/160 train loss: 2.9776 train acc 0.4320\n",
            "batch: 75/160 train loss: 2.9757 train acc 0.4329\n",
            "batch: 76/160 train loss: 2.9740 train acc 0.4336\n",
            "batch: 77/160 train loss: 2.9726 train acc 0.4338\n",
            "batch: 78/160 train loss: 2.9734 train acc 0.4327\n",
            "batch: 79/160 train loss: 2.9699 train acc 0.4337\n",
            "batch: 80/160 train loss: 2.9687 train acc 0.4336\n",
            "batch: 81/160 train loss: 2.9694 train acc 0.4327\n",
            "batch: 82/160 train loss: 2.9688 train acc 0.4329\n",
            "batch: 83/160 train loss: 2.9663 train acc 0.4332\n",
            "batch: 84/160 train loss: 2.9689 train acc 0.4323\n",
            "batch: 85/160 train loss: 2.9702 train acc 0.4318\n",
            "batch: 86/160 train loss: 2.9686 train acc 0.4322\n",
            "batch: 87/160 train loss: 2.9659 train acc 0.4327\n",
            "batch: 88/160 train loss: 2.9647 train acc 0.4329\n",
            "batch: 89/160 train loss: 2.9625 train acc 0.4329\n",
            "batch: 90/160 train loss: 2.9595 train acc 0.4335\n",
            "batch: 91/160 train loss: 2.9581 train acc 0.4330\n",
            "batch: 92/160 train loss: 2.9580 train acc 0.4326\n",
            "batch: 93/160 train loss: 2.9588 train acc 0.4313\n",
            "batch: 94/160 train loss: 2.9560 train acc 0.4322\n",
            "batch: 95/160 train loss: 2.9556 train acc 0.4321\n",
            "batch: 96/160 train loss: 2.9530 train acc 0.4321\n",
            "batch: 97/160 train loss: 2.9526 train acc 0.4315\n",
            "batch: 98/160 train loss: 2.9516 train acc 0.4314\n",
            "batch: 99/160 train loss: 2.9500 train acc 0.4313\n",
            "batch: 100/160 train loss: 2.9486 train acc 0.4316\n",
            "batch: 101/160 train loss: 2.9481 train acc 0.4312\n",
            "batch: 102/160 train loss: 2.9479 train acc 0.4312\n",
            "batch: 103/160 train loss: 2.9485 train acc 0.4310\n",
            "batch: 104/160 train loss: 2.9497 train acc 0.4313\n",
            "batch: 105/160 train loss: 2.9491 train acc 0.4315\n",
            "batch: 106/160 train loss: 2.9468 train acc 0.4316\n",
            "batch: 107/160 train loss: 2.9465 train acc 0.4312\n",
            "batch: 108/160 train loss: 2.9426 train acc 0.4319\n",
            "batch: 109/160 train loss: 2.9420 train acc 0.4321\n",
            "batch: 110/160 train loss: 2.9411 train acc 0.4320\n",
            "batch: 111/160 train loss: 2.9409 train acc 0.4317\n",
            "batch: 112/160 train loss: 2.9400 train acc 0.4321\n",
            "batch: 113/160 train loss: 2.9409 train acc 0.4317\n",
            "batch: 114/160 train loss: 2.9418 train acc 0.4320\n",
            "batch: 115/160 train loss: 2.9402 train acc 0.4325\n",
            "batch: 116/160 train loss: 2.9366 train acc 0.4335\n",
            "batch: 117/160 train loss: 2.9373 train acc 0.4334\n",
            "batch: 118/160 train loss: 2.9363 train acc 0.4335\n",
            "batch: 119/160 train loss: 2.9335 train acc 0.4343\n",
            "batch: 120/160 train loss: 2.9318 train acc 0.4348\n",
            "batch: 121/160 train loss: 2.9321 train acc 0.4353\n",
            "batch: 122/160 train loss: 2.9309 train acc 0.4358\n",
            "batch: 123/160 train loss: 2.9273 train acc 0.4365\n",
            "batch: 124/160 train loss: 2.9254 train acc 0.4366\n",
            "batch: 125/160 train loss: 2.9256 train acc 0.4363\n",
            "batch: 126/160 train loss: 2.9238 train acc 0.4365\n",
            "batch: 127/160 train loss: 2.9246 train acc 0.4364\n",
            "batch: 128/160 train loss: 2.9234 train acc 0.4371\n",
            "batch: 129/160 train loss: 2.9233 train acc 0.4371\n",
            "batch: 130/160 train loss: 2.9215 train acc 0.4377\n",
            "batch: 131/160 train loss: 2.9208 train acc 0.4386\n",
            "batch: 132/160 train loss: 2.9210 train acc 0.4388\n",
            "batch: 133/160 train loss: 2.9188 train acc 0.4397\n",
            "batch: 134/160 train loss: 2.9174 train acc 0.4399\n",
            "batch: 135/160 train loss: 2.9154 train acc 0.4403\n",
            "batch: 136/160 train loss: 2.9137 train acc 0.4411\n",
            "batch: 137/160 train loss: 2.9134 train acc 0.4406\n",
            "batch: 138/160 train loss: 2.9112 train acc 0.4412\n",
            "batch: 139/160 train loss: 2.9102 train acc 0.4415\n",
            "batch: 140/160 train loss: 2.9093 train acc 0.4419\n",
            "batch: 141/160 train loss: 2.9099 train acc 0.4417\n",
            "batch: 142/160 train loss: 2.9070 train acc 0.4431\n",
            "batch: 143/160 train loss: 2.9070 train acc 0.4429\n",
            "batch: 144/160 train loss: 2.9069 train acc 0.4429\n",
            "batch: 145/160 train loss: 2.9042 train acc 0.4437\n",
            "batch: 146/160 train loss: 2.9039 train acc 0.4439\n",
            "batch: 147/160 train loss: 2.9052 train acc 0.4437\n",
            "batch: 148/160 train loss: 2.9051 train acc 0.4436\n",
            "batch: 149/160 train loss: 2.9049 train acc 0.4436\n",
            "batch: 150/160 train loss: 2.9066 train acc 0.4434\n",
            "batch: 151/160 train loss: 2.9069 train acc 0.4433\n",
            "batch: 152/160 train loss: 2.9085 train acc 0.4429\n",
            "batch: 153/160 train loss: 2.9096 train acc 0.4432\n",
            "batch: 154/160 train loss: 2.9081 train acc 0.4437\n",
            "batch: 155/160 train loss: 2.9068 train acc 0.4436\n",
            "batch: 156/160 train loss: 2.9057 train acc 0.4440\n",
            "batch: 157/160 train loss: 2.9049 train acc 0.4443\n",
            "batch: 158/160 train loss: 2.9033 train acc 0.4450\n",
            "batch: 159/160 train loss: 2.9035 train acc 0.4448\n",
            "batch: 160/160 train loss: 2.9037 train acc 0.4446\n",
            "\n",
            "Epoch 4 train loss: 2.9037 test loss 3.5117 train acc 0.4446 test acc 0.3033\n",
            "Epoch 5/25\n",
            "batch: 1/160 train loss: 2.4090 train acc 0.6875\n",
            "batch: 2/160 train loss: 2.4497 train acc 0.6250\n",
            "batch: 3/160 train loss: 2.4440 train acc 0.6354\n",
            "batch: 4/160 train loss: 2.4537 train acc 0.6094\n",
            "batch: 5/160 train loss: 2.4546 train acc 0.5938\n",
            "batch: 6/160 train loss: 2.4602 train acc 0.5938\n",
            "batch: 7/160 train loss: 2.4583 train acc 0.6027\n",
            "batch: 8/160 train loss: 2.4856 train acc 0.5801\n",
            "batch: 9/160 train loss: 2.4695 train acc 0.5816\n",
            "batch: 10/160 train loss: 2.4519 train acc 0.5859\n",
            "batch: 11/160 train loss: 2.4441 train acc 0.5852\n",
            "batch: 12/160 train loss: 2.4312 train acc 0.5885\n",
            "batch: 13/160 train loss: 2.4365 train acc 0.5817\n",
            "batch: 14/160 train loss: 2.4390 train acc 0.5826\n",
            "batch: 15/160 train loss: 2.4376 train acc 0.5802\n",
            "batch: 16/160 train loss: 2.4283 train acc 0.5859\n",
            "batch: 17/160 train loss: 2.4401 train acc 0.5809\n",
            "batch: 18/160 train loss: 2.4345 train acc 0.5773\n",
            "batch: 19/160 train loss: 2.4344 train acc 0.5789\n",
            "batch: 20/160 train loss: 2.4274 train acc 0.5852\n",
            "batch: 21/160 train loss: 2.4229 train acc 0.5878\n",
            "batch: 22/160 train loss: 2.4130 train acc 0.5923\n",
            "batch: 23/160 train loss: 2.4254 train acc 0.5897\n",
            "batch: 24/160 train loss: 2.4305 train acc 0.5866\n",
            "batch: 25/160 train loss: 2.4307 train acc 0.5863\n",
            "batch: 26/160 train loss: 2.4363 train acc 0.5859\n",
            "batch: 27/160 train loss: 2.4289 train acc 0.5880\n",
            "batch: 28/160 train loss: 2.4297 train acc 0.5854\n",
            "batch: 29/160 train loss: 2.4296 train acc 0.5851\n",
            "batch: 30/160 train loss: 2.4270 train acc 0.5865\n",
            "batch: 31/160 train loss: 2.4266 train acc 0.5872\n",
            "batch: 32/160 train loss: 2.4220 train acc 0.5884\n",
            "batch: 33/160 train loss: 2.4233 train acc 0.5885\n",
            "batch: 34/160 train loss: 2.4303 train acc 0.5859\n",
            "batch: 35/160 train loss: 2.4392 train acc 0.5835\n",
            "batch: 36/160 train loss: 2.4405 train acc 0.5838\n",
            "batch: 37/160 train loss: 2.4330 train acc 0.5849\n",
            "batch: 38/160 train loss: 2.4403 train acc 0.5843\n",
            "batch: 39/160 train loss: 2.4448 train acc 0.5829\n",
            "batch: 40/160 train loss: 2.4461 train acc 0.5832\n",
            "batch: 41/160 train loss: 2.4470 train acc 0.5823\n",
            "batch: 42/160 train loss: 2.4430 train acc 0.5841\n",
            "batch: 43/160 train loss: 2.4425 train acc 0.5843\n",
            "batch: 44/160 train loss: 2.4468 train acc 0.5813\n",
            "batch: 45/160 train loss: 2.4471 train acc 0.5809\n",
            "batch: 46/160 train loss: 2.4435 train acc 0.5822\n",
            "batch: 47/160 train loss: 2.4440 train acc 0.5831\n",
            "batch: 48/160 train loss: 2.4467 train acc 0.5830\n",
            "batch: 49/160 train loss: 2.4463 train acc 0.5835\n",
            "batch: 50/160 train loss: 2.4456 train acc 0.5834\n",
            "batch: 51/160 train loss: 2.4491 train acc 0.5824\n",
            "batch: 52/160 train loss: 2.4522 train acc 0.5814\n",
            "batch: 53/160 train loss: 2.4493 train acc 0.5825\n",
            "batch: 54/160 train loss: 2.4516 train acc 0.5825\n",
            "batch: 55/160 train loss: 2.4500 train acc 0.5821\n",
            "batch: 56/160 train loss: 2.4473 train acc 0.5843\n",
            "batch: 57/160 train loss: 2.4523 train acc 0.5828\n",
            "batch: 58/160 train loss: 2.4488 train acc 0.5843\n",
            "batch: 59/160 train loss: 2.4481 train acc 0.5855\n",
            "batch: 60/160 train loss: 2.4471 train acc 0.5859\n",
            "batch: 61/160 train loss: 2.4453 train acc 0.5868\n",
            "batch: 62/160 train loss: 2.4462 train acc 0.5854\n",
            "batch: 63/160 train loss: 2.4482 train acc 0.5853\n",
            "batch: 64/160 train loss: 2.4484 train acc 0.5864\n",
            "batch: 65/160 train loss: 2.4436 train acc 0.5870\n",
            "batch: 66/160 train loss: 2.4444 train acc 0.5871\n",
            "batch: 67/160 train loss: 2.4441 train acc 0.5879\n",
            "batch: 68/160 train loss: 2.4418 train acc 0.5885\n",
            "batch: 69/160 train loss: 2.4411 train acc 0.5885\n",
            "batch: 70/160 train loss: 2.4401 train acc 0.5879\n",
            "batch: 71/160 train loss: 2.4416 train acc 0.5874\n",
            "batch: 72/160 train loss: 2.4482 train acc 0.5853\n",
            "batch: 73/160 train loss: 2.4448 train acc 0.5863\n",
            "batch: 74/160 train loss: 2.4485 train acc 0.5851\n",
            "batch: 75/160 train loss: 2.4483 train acc 0.5852\n",
            "batch: 76/160 train loss: 2.4503 train acc 0.5847\n",
            "batch: 77/160 train loss: 2.4507 train acc 0.5848\n",
            "batch: 78/160 train loss: 2.4510 train acc 0.5849\n",
            "batch: 79/160 train loss: 2.4534 train acc 0.5850\n",
            "batch: 80/160 train loss: 2.4553 train acc 0.5848\n",
            "batch: 81/160 train loss: 2.4542 train acc 0.5849\n",
            "batch: 82/160 train loss: 2.4536 train acc 0.5852\n",
            "batch: 83/160 train loss: 2.4505 train acc 0.5858\n",
            "batch: 84/160 train loss: 2.4532 train acc 0.5846\n",
            "batch: 85/160 train loss: 2.4545 train acc 0.5838\n",
            "batch: 86/160 train loss: 2.4529 train acc 0.5836\n",
            "batch: 87/160 train loss: 2.4521 train acc 0.5841\n",
            "batch: 88/160 train loss: 2.4510 train acc 0.5849\n",
            "batch: 89/160 train loss: 2.4488 train acc 0.5850\n",
            "batch: 90/160 train loss: 2.4487 train acc 0.5845\n",
            "batch: 91/160 train loss: 2.4479 train acc 0.5850\n",
            "batch: 92/160 train loss: 2.4498 train acc 0.5851\n",
            "batch: 93/160 train loss: 2.4481 train acc 0.5860\n",
            "batch: 94/160 train loss: 2.4494 train acc 0.5854\n",
            "batch: 95/160 train loss: 2.4532 train acc 0.5849\n",
            "batch: 96/160 train loss: 2.4524 train acc 0.5854\n",
            "batch: 97/160 train loss: 2.4522 train acc 0.5860\n",
            "batch: 98/160 train loss: 2.4555 train acc 0.5845\n",
            "batch: 99/160 train loss: 2.4549 train acc 0.5855\n",
            "batch: 100/160 train loss: 2.4567 train acc 0.5847\n",
            "batch: 101/160 train loss: 2.4565 train acc 0.5843\n",
            "batch: 102/160 train loss: 2.4558 train acc 0.5841\n",
            "batch: 103/160 train loss: 2.4576 train acc 0.5836\n",
            "batch: 104/160 train loss: 2.4572 train acc 0.5837\n",
            "batch: 105/160 train loss: 2.4570 train acc 0.5836\n",
            "batch: 106/160 train loss: 2.4585 train acc 0.5839\n",
            "batch: 107/160 train loss: 2.4599 train acc 0.5834\n",
            "batch: 108/160 train loss: 2.4611 train acc 0.5823\n",
            "batch: 109/160 train loss: 2.4600 train acc 0.5831\n",
            "batch: 110/160 train loss: 2.4573 train acc 0.5834\n",
            "batch: 111/160 train loss: 2.4569 train acc 0.5838\n",
            "batch: 112/160 train loss: 2.4583 train acc 0.5830\n",
            "batch: 113/160 train loss: 2.4570 train acc 0.5832\n",
            "batch: 114/160 train loss: 2.4561 train acc 0.5839\n",
            "batch: 115/160 train loss: 2.4544 train acc 0.5841\n",
            "batch: 116/160 train loss: 2.4531 train acc 0.5851\n",
            "batch: 117/160 train loss: 2.4550 train acc 0.5843\n",
            "batch: 118/160 train loss: 2.4540 train acc 0.5842\n",
            "batch: 119/160 train loss: 2.4547 train acc 0.5839\n",
            "batch: 120/160 train loss: 2.4529 train acc 0.5848\n",
            "batch: 121/160 train loss: 2.4536 train acc 0.5843\n",
            "batch: 122/160 train loss: 2.4529 train acc 0.5845\n",
            "batch: 123/160 train loss: 2.4528 train acc 0.5846\n",
            "batch: 124/160 train loss: 2.4517 train acc 0.5847\n",
            "batch: 125/160 train loss: 2.4530 train acc 0.5849\n",
            "batch: 126/160 train loss: 2.4525 train acc 0.5853\n",
            "batch: 127/160 train loss: 2.4527 train acc 0.5851\n",
            "batch: 128/160 train loss: 2.4524 train acc 0.5848\n",
            "batch: 129/160 train loss: 2.4534 train acc 0.5847\n",
            "batch: 130/160 train loss: 2.4549 train acc 0.5840\n",
            "batch: 131/160 train loss: 2.4560 train acc 0.5835\n",
            "batch: 132/160 train loss: 2.4564 train acc 0.5832\n",
            "batch: 133/160 train loss: 2.4554 train acc 0.5832\n",
            "batch: 134/160 train loss: 2.4548 train acc 0.5835\n",
            "batch: 135/160 train loss: 2.4533 train acc 0.5839\n",
            "batch: 136/160 train loss: 2.4514 train acc 0.5844\n",
            "batch: 137/160 train loss: 2.4508 train acc 0.5845\n",
            "batch: 138/160 train loss: 2.4515 train acc 0.5839\n",
            "batch: 139/160 train loss: 2.4510 train acc 0.5842\n",
            "batch: 140/160 train loss: 2.4514 train acc 0.5845\n",
            "batch: 141/160 train loss: 2.4521 train acc 0.5849\n",
            "batch: 142/160 train loss: 2.4498 train acc 0.5856\n",
            "batch: 143/160 train loss: 2.4498 train acc 0.5856\n",
            "batch: 144/160 train loss: 2.4505 train acc 0.5853\n",
            "batch: 145/160 train loss: 2.4485 train acc 0.5860\n",
            "batch: 146/160 train loss: 2.4499 train acc 0.5855\n",
            "batch: 147/160 train loss: 2.4500 train acc 0.5854\n",
            "batch: 148/160 train loss: 2.4485 train acc 0.5855\n",
            "batch: 149/160 train loss: 2.4483 train acc 0.5856\n",
            "batch: 150/160 train loss: 2.4475 train acc 0.5858\n",
            "batch: 151/160 train loss: 2.4463 train acc 0.5860\n",
            "batch: 152/160 train loss: 2.4459 train acc 0.5856\n",
            "batch: 153/160 train loss: 2.4460 train acc 0.5860\n",
            "batch: 154/160 train loss: 2.4452 train acc 0.5860\n",
            "batch: 155/160 train loss: 2.4470 train acc 0.5853\n",
            "batch: 156/160 train loss: 2.4480 train acc 0.5851\n",
            "batch: 157/160 train loss: 2.4461 train acc 0.5855\n",
            "batch: 158/160 train loss: 2.4466 train acc 0.5849\n",
            "batch: 159/160 train loss: 2.4462 train acc 0.5845\n",
            "batch: 160/160 train loss: 2.4466 train acc 0.5842\n",
            "\n",
            "Epoch 5 train loss: 2.4466 test loss 3.2220 train acc 0.5842 test acc 0.3692\n",
            "Epoch 6/25\n",
            "batch: 1/160 train loss: 2.0767 train acc 0.7344\n",
            "batch: 2/160 train loss: 2.0421 train acc 0.7188\n",
            "batch: 3/160 train loss: 2.1593 train acc 0.6510\n",
            "batch: 4/160 train loss: 2.1530 train acc 0.6641\n",
            "batch: 5/160 train loss: 2.1534 train acc 0.6719\n",
            "batch: 6/160 train loss: 2.1502 train acc 0.6771\n",
            "batch: 7/160 train loss: 2.1738 train acc 0.6786\n",
            "batch: 8/160 train loss: 2.1426 train acc 0.6895\n",
            "batch: 9/160 train loss: 2.1236 train acc 0.6944\n",
            "batch: 10/160 train loss: 2.1231 train acc 0.6984\n",
            "batch: 11/160 train loss: 2.1256 train acc 0.6989\n",
            "batch: 12/160 train loss: 2.1330 train acc 0.6953\n",
            "batch: 13/160 train loss: 2.1428 train acc 0.6923\n",
            "batch: 14/160 train loss: 2.1255 train acc 0.6987\n",
            "batch: 15/160 train loss: 2.1206 train acc 0.7010\n",
            "batch: 16/160 train loss: 2.1364 train acc 0.6895\n",
            "batch: 17/160 train loss: 2.1444 train acc 0.6884\n",
            "batch: 18/160 train loss: 2.1407 train acc 0.6901\n",
            "batch: 19/160 train loss: 2.1298 train acc 0.6949\n",
            "batch: 20/160 train loss: 2.1265 train acc 0.6930\n",
            "batch: 21/160 train loss: 2.1278 train acc 0.6920\n",
            "batch: 22/160 train loss: 2.1294 train acc 0.6953\n",
            "batch: 23/160 train loss: 2.1268 train acc 0.6950\n",
            "batch: 24/160 train loss: 2.1257 train acc 0.6953\n",
            "batch: 25/160 train loss: 2.1262 train acc 0.6931\n",
            "batch: 26/160 train loss: 2.1242 train acc 0.6959\n",
            "batch: 27/160 train loss: 2.1191 train acc 0.6991\n",
            "batch: 28/160 train loss: 2.1115 train acc 0.6998\n",
            "batch: 29/160 train loss: 2.1030 train acc 0.7026\n",
            "batch: 30/160 train loss: 2.1105 train acc 0.7010\n",
            "batch: 31/160 train loss: 2.1041 train acc 0.7046\n",
            "batch: 32/160 train loss: 2.1082 train acc 0.7031\n",
            "batch: 33/160 train loss: 2.1075 train acc 0.7036\n",
            "batch: 34/160 train loss: 2.1124 train acc 0.7008\n",
            "batch: 35/160 train loss: 2.1158 train acc 0.7013\n",
            "batch: 36/160 train loss: 2.1139 train acc 0.7023\n",
            "batch: 37/160 train loss: 2.1139 train acc 0.7019\n",
            "batch: 38/160 train loss: 2.1158 train acc 0.7019\n",
            "batch: 39/160 train loss: 2.1116 train acc 0.7035\n",
            "batch: 40/160 train loss: 2.1077 train acc 0.7043\n",
            "batch: 41/160 train loss: 2.1045 train acc 0.7046\n",
            "batch: 42/160 train loss: 2.1043 train acc 0.7016\n",
            "batch: 43/160 train loss: 2.1012 train acc 0.7028\n",
            "batch: 44/160 train loss: 2.1048 train acc 0.7003\n",
            "batch: 45/160 train loss: 2.0990 train acc 0.7007\n",
            "batch: 46/160 train loss: 2.1002 train acc 0.7011\n",
            "batch: 47/160 train loss: 2.1017 train acc 0.6985\n",
            "batch: 48/160 train loss: 2.1025 train acc 0.6989\n",
            "batch: 49/160 train loss: 2.1000 train acc 0.6990\n",
            "batch: 50/160 train loss: 2.1030 train acc 0.6972\n",
            "batch: 51/160 train loss: 2.1032 train acc 0.6970\n",
            "batch: 52/160 train loss: 2.1081 train acc 0.6959\n",
            "batch: 53/160 train loss: 2.1103 train acc 0.6952\n",
            "batch: 54/160 train loss: 2.1096 train acc 0.6936\n",
            "batch: 55/160 train loss: 2.1093 train acc 0.6937\n",
            "batch: 56/160 train loss: 2.1088 train acc 0.6939\n",
            "batch: 57/160 train loss: 2.1069 train acc 0.6941\n",
            "batch: 58/160 train loss: 2.1053 train acc 0.6940\n",
            "batch: 59/160 train loss: 2.1037 train acc 0.6944\n",
            "batch: 60/160 train loss: 2.1054 train acc 0.6932\n",
            "batch: 61/160 train loss: 2.1064 train acc 0.6931\n",
            "batch: 62/160 train loss: 2.1064 train acc 0.6943\n",
            "batch: 63/160 train loss: 2.1072 train acc 0.6944\n",
            "batch: 64/160 train loss: 2.1065 train acc 0.6943\n",
            "batch: 65/160 train loss: 2.1119 train acc 0.6933\n",
            "batch: 66/160 train loss: 2.1127 train acc 0.6934\n",
            "batch: 67/160 train loss: 2.1109 train acc 0.6931\n",
            "batch: 68/160 train loss: 2.1117 train acc 0.6923\n",
            "batch: 69/160 train loss: 2.1114 train acc 0.6929\n",
            "batch: 70/160 train loss: 2.1137 train acc 0.6913\n",
            "batch: 71/160 train loss: 2.1129 train acc 0.6910\n",
            "batch: 72/160 train loss: 2.1120 train acc 0.6916\n",
            "batch: 73/160 train loss: 2.1120 train acc 0.6920\n",
            "batch: 74/160 train loss: 2.1110 train acc 0.6926\n",
            "batch: 75/160 train loss: 2.1103 train acc 0.6931\n",
            "batch: 76/160 train loss: 2.1121 train acc 0.6926\n",
            "batch: 77/160 train loss: 2.1136 train acc 0.6916\n",
            "batch: 78/160 train loss: 2.1119 train acc 0.6915\n",
            "batch: 79/160 train loss: 2.1119 train acc 0.6922\n",
            "batch: 80/160 train loss: 2.1124 train acc 0.6918\n",
            "batch: 81/160 train loss: 2.1121 train acc 0.6921\n",
            "batch: 82/160 train loss: 2.1130 train acc 0.6917\n",
            "batch: 83/160 train loss: 2.1140 train acc 0.6916\n",
            "batch: 84/160 train loss: 2.1131 train acc 0.6920\n",
            "batch: 85/160 train loss: 2.1134 train acc 0.6926\n",
            "batch: 86/160 train loss: 2.1105 train acc 0.6937\n",
            "batch: 87/160 train loss: 2.1122 train acc 0.6931\n",
            "batch: 88/160 train loss: 2.1141 train acc 0.6930\n",
            "batch: 89/160 train loss: 2.1148 train acc 0.6917\n",
            "batch: 90/160 train loss: 2.1156 train acc 0.6917\n",
            "batch: 91/160 train loss: 2.1129 train acc 0.6927\n",
            "batch: 92/160 train loss: 2.1154 train acc 0.6921\n",
            "batch: 93/160 train loss: 2.1168 train acc 0.6909\n",
            "batch: 94/160 train loss: 2.1172 train acc 0.6910\n",
            "batch: 95/160 train loss: 2.1192 train acc 0.6908\n",
            "batch: 96/160 train loss: 2.1189 train acc 0.6908\n",
            "batch: 97/160 train loss: 2.1202 train acc 0.6907\n",
            "batch: 98/160 train loss: 2.1182 train acc 0.6916\n",
            "batch: 99/160 train loss: 2.1185 train acc 0.6905\n",
            "batch: 100/160 train loss: 2.1186 train acc 0.6902\n",
            "batch: 101/160 train loss: 2.1192 train acc 0.6901\n",
            "batch: 102/160 train loss: 2.1203 train acc 0.6890\n",
            "batch: 103/160 train loss: 2.1202 train acc 0.6889\n",
            "batch: 104/160 train loss: 2.1191 train acc 0.6892\n",
            "batch: 105/160 train loss: 2.1208 train acc 0.6881\n",
            "batch: 106/160 train loss: 2.1200 train acc 0.6882\n",
            "batch: 107/160 train loss: 2.1212 train acc 0.6876\n",
            "batch: 108/160 train loss: 2.1211 train acc 0.6871\n",
            "batch: 109/160 train loss: 2.1226 train acc 0.6862\n",
            "batch: 110/160 train loss: 2.1230 train acc 0.6861\n",
            "batch: 111/160 train loss: 2.1240 train acc 0.6852\n",
            "batch: 112/160 train loss: 2.1238 train acc 0.6857\n",
            "batch: 113/160 train loss: 2.1254 train acc 0.6851\n",
            "batch: 114/160 train loss: 2.1246 train acc 0.6852\n",
            "batch: 115/160 train loss: 2.1276 train acc 0.6844\n",
            "batch: 116/160 train loss: 2.1271 train acc 0.6849\n",
            "batch: 117/160 train loss: 2.1255 train acc 0.6856\n",
            "batch: 118/160 train loss: 2.1252 train acc 0.6854\n",
            "batch: 119/160 train loss: 2.1246 train acc 0.6855\n",
            "batch: 120/160 train loss: 2.1261 train acc 0.6852\n",
            "batch: 121/160 train loss: 2.1260 train acc 0.6854\n",
            "batch: 122/160 train loss: 2.1275 train acc 0.6853\n",
            "batch: 123/160 train loss: 2.1273 train acc 0.6852\n",
            "batch: 124/160 train loss: 2.1267 train acc 0.6850\n",
            "batch: 125/160 train loss: 2.1280 train acc 0.6835\n",
            "batch: 126/160 train loss: 2.1261 train acc 0.6843\n",
            "batch: 127/160 train loss: 2.1278 train acc 0.6839\n",
            "batch: 128/160 train loss: 2.1281 train acc 0.6835\n",
            "batch: 129/160 train loss: 2.1271 train acc 0.6839\n",
            "batch: 130/160 train loss: 2.1269 train acc 0.6841\n",
            "batch: 131/160 train loss: 2.1266 train acc 0.6843\n",
            "batch: 132/160 train loss: 2.1266 train acc 0.6838\n",
            "batch: 133/160 train loss: 2.1274 train acc 0.6832\n",
            "batch: 134/160 train loss: 2.1281 train acc 0.6820\n",
            "batch: 135/160 train loss: 2.1276 train acc 0.6824\n",
            "batch: 136/160 train loss: 2.1278 train acc 0.6815\n",
            "batch: 137/160 train loss: 2.1266 train acc 0.6824\n",
            "batch: 138/160 train loss: 2.1277 train acc 0.6818\n",
            "batch: 139/160 train loss: 2.1271 train acc 0.6819\n",
            "batch: 140/160 train loss: 2.1266 train acc 0.6823\n",
            "batch: 141/160 train loss: 2.1254 train acc 0.6830\n",
            "batch: 142/160 train loss: 2.1253 train acc 0.6832\n",
            "batch: 143/160 train loss: 2.1268 train acc 0.6825\n",
            "batch: 144/160 train loss: 2.1263 train acc 0.6829\n",
            "batch: 145/160 train loss: 2.1263 train acc 0.6835\n",
            "batch: 146/160 train loss: 2.1269 train acc 0.6836\n",
            "batch: 147/160 train loss: 2.1271 train acc 0.6837\n",
            "batch: 148/160 train loss: 2.1264 train acc 0.6838\n",
            "batch: 149/160 train loss: 2.1286 train acc 0.6830\n",
            "batch: 150/160 train loss: 2.1281 train acc 0.6830\n",
            "batch: 151/160 train loss: 2.1279 train acc 0.6833\n",
            "batch: 152/160 train loss: 2.1265 train acc 0.6835\n",
            "batch: 153/160 train loss: 2.1257 train acc 0.6837\n",
            "batch: 154/160 train loss: 2.1250 train acc 0.6839\n",
            "batch: 155/160 train loss: 2.1243 train acc 0.6842\n",
            "batch: 156/160 train loss: 2.1250 train acc 0.6839\n",
            "batch: 157/160 train loss: 2.1263 train acc 0.6838\n",
            "batch: 158/160 train loss: 2.1271 train acc 0.6832\n",
            "batch: 159/160 train loss: 2.1266 train acc 0.6836\n",
            "batch: 160/160 train loss: 2.1269 train acc 0.6834\n",
            "\n",
            "Epoch 6 train loss: 2.1269 test loss 3.1292 train acc 0.6834 test acc 0.4025\n",
            "Epoch 7/25\n",
            "batch: 1/160 train loss: 1.5306 train acc 0.8438\n",
            "batch: 2/160 train loss: 1.6214 train acc 0.8281\n",
            "batch: 3/160 train loss: 1.6609 train acc 0.8177\n",
            "batch: 4/160 train loss: 1.7387 train acc 0.8047\n",
            "batch: 5/160 train loss: 1.8185 train acc 0.7844\n",
            "batch: 6/160 train loss: 1.8371 train acc 0.7812\n",
            "batch: 7/160 train loss: 1.8420 train acc 0.7812\n",
            "batch: 8/160 train loss: 1.8752 train acc 0.7715\n",
            "batch: 9/160 train loss: 1.8586 train acc 0.7778\n",
            "batch: 10/160 train loss: 1.8814 train acc 0.7703\n",
            "batch: 11/160 train loss: 1.8720 train acc 0.7699\n",
            "batch: 12/160 train loss: 1.8762 train acc 0.7669\n",
            "batch: 13/160 train loss: 1.8813 train acc 0.7608\n",
            "batch: 14/160 train loss: 1.8813 train acc 0.7634\n",
            "batch: 15/160 train loss: 1.8661 train acc 0.7677\n",
            "batch: 16/160 train loss: 1.8647 train acc 0.7705\n",
            "batch: 17/160 train loss: 1.8536 train acc 0.7739\n",
            "batch: 18/160 train loss: 1.8369 train acc 0.7830\n",
            "batch: 19/160 train loss: 1.8367 train acc 0.7812\n",
            "batch: 20/160 train loss: 1.8263 train acc 0.7867\n",
            "batch: 21/160 train loss: 1.8221 train acc 0.7879\n",
            "batch: 22/160 train loss: 1.8217 train acc 0.7891\n",
            "batch: 23/160 train loss: 1.8159 train acc 0.7914\n",
            "batch: 24/160 train loss: 1.8100 train acc 0.7936\n",
            "batch: 25/160 train loss: 1.8132 train acc 0.7925\n",
            "batch: 26/160 train loss: 1.8144 train acc 0.7915\n",
            "batch: 27/160 train loss: 1.8084 train acc 0.7934\n",
            "batch: 28/160 train loss: 1.8049 train acc 0.7941\n",
            "batch: 29/160 train loss: 1.8015 train acc 0.7931\n",
            "batch: 30/160 train loss: 1.7991 train acc 0.7958\n",
            "batch: 31/160 train loss: 1.8014 train acc 0.7959\n",
            "batch: 32/160 train loss: 1.8061 train acc 0.7930\n",
            "batch: 33/160 train loss: 1.8056 train acc 0.7926\n",
            "batch: 34/160 train loss: 1.8032 train acc 0.7937\n",
            "batch: 35/160 train loss: 1.8010 train acc 0.7933\n",
            "batch: 36/160 train loss: 1.8022 train acc 0.7930\n",
            "batch: 37/160 train loss: 1.8047 train acc 0.7918\n",
            "batch: 38/160 train loss: 1.8007 train acc 0.7924\n",
            "batch: 39/160 train loss: 1.8046 train acc 0.7921\n",
            "batch: 40/160 train loss: 1.8021 train acc 0.7926\n",
            "batch: 41/160 train loss: 1.8070 train acc 0.7908\n",
            "batch: 42/160 train loss: 1.8045 train acc 0.7917\n",
            "batch: 43/160 train loss: 1.7993 train acc 0.7911\n",
            "batch: 44/160 train loss: 1.7965 train acc 0.7937\n",
            "batch: 45/160 train loss: 1.7920 train acc 0.7951\n",
            "batch: 46/160 train loss: 1.7859 train acc 0.7979\n",
            "batch: 47/160 train loss: 1.7845 train acc 0.7979\n",
            "batch: 48/160 train loss: 1.7798 train acc 0.8005\n",
            "batch: 49/160 train loss: 1.7804 train acc 0.8004\n",
            "batch: 50/160 train loss: 1.7830 train acc 0.7994\n",
            "batch: 51/160 train loss: 1.7818 train acc 0.7996\n",
            "batch: 52/160 train loss: 1.7811 train acc 0.8002\n",
            "batch: 53/160 train loss: 1.7797 train acc 0.8004\n",
            "batch: 54/160 train loss: 1.7825 train acc 0.8001\n",
            "batch: 55/160 train loss: 1.7821 train acc 0.8006\n",
            "batch: 56/160 train loss: 1.7813 train acc 0.8011\n",
            "batch: 57/160 train loss: 1.7809 train acc 0.8015\n",
            "batch: 58/160 train loss: 1.7793 train acc 0.8009\n",
            "batch: 59/160 train loss: 1.7785 train acc 0.8016\n",
            "batch: 60/160 train loss: 1.7746 train acc 0.8026\n",
            "batch: 61/160 train loss: 1.7709 train acc 0.8038\n",
            "batch: 62/160 train loss: 1.7647 train acc 0.8052\n",
            "batch: 63/160 train loss: 1.7619 train acc 0.8061\n",
            "batch: 64/160 train loss: 1.7588 train acc 0.8074\n",
            "batch: 65/160 train loss: 1.7577 train acc 0.8075\n",
            "batch: 66/160 train loss: 1.7555 train acc 0.8075\n",
            "batch: 67/160 train loss: 1.7555 train acc 0.8069\n",
            "batch: 68/160 train loss: 1.7560 train acc 0.8065\n",
            "batch: 69/160 train loss: 1.7562 train acc 0.8066\n",
            "batch: 70/160 train loss: 1.7555 train acc 0.8067\n",
            "batch: 71/160 train loss: 1.7589 train acc 0.8044\n",
            "batch: 72/160 train loss: 1.7587 train acc 0.8047\n",
            "batch: 73/160 train loss: 1.7587 train acc 0.8037\n",
            "batch: 74/160 train loss: 1.7571 train acc 0.8047\n",
            "batch: 75/160 train loss: 1.7560 train acc 0.8054\n",
            "batch: 76/160 train loss: 1.7536 train acc 0.8063\n",
            "batch: 77/160 train loss: 1.7517 train acc 0.8072\n",
            "batch: 78/160 train loss: 1.7511 train acc 0.8071\n",
            "batch: 79/160 train loss: 1.7484 train acc 0.8072\n",
            "batch: 80/160 train loss: 1.7484 train acc 0.8070\n",
            "batch: 81/160 train loss: 1.7476 train acc 0.8075\n",
            "batch: 82/160 train loss: 1.7490 train acc 0.8077\n",
            "batch: 83/160 train loss: 1.7505 train acc 0.8078\n",
            "batch: 84/160 train loss: 1.7499 train acc 0.8084\n",
            "batch: 85/160 train loss: 1.7477 train acc 0.8090\n",
            "batch: 86/160 train loss: 1.7476 train acc 0.8081\n",
            "batch: 87/160 train loss: 1.7493 train acc 0.8073\n",
            "batch: 88/160 train loss: 1.7502 train acc 0.8066\n",
            "batch: 89/160 train loss: 1.7485 train acc 0.8074\n",
            "batch: 90/160 train loss: 1.7477 train acc 0.8075\n",
            "batch: 91/160 train loss: 1.7479 train acc 0.8073\n",
            "batch: 92/160 train loss: 1.7469 train acc 0.8076\n",
            "batch: 93/160 train loss: 1.7482 train acc 0.8071\n",
            "batch: 94/160 train loss: 1.7485 train acc 0.8072\n",
            "batch: 95/160 train loss: 1.7489 train acc 0.8071\n",
            "batch: 96/160 train loss: 1.7457 train acc 0.8084\n",
            "batch: 97/160 train loss: 1.7456 train acc 0.8082\n",
            "batch: 98/160 train loss: 1.7447 train acc 0.8087\n",
            "batch: 99/160 train loss: 1.7437 train acc 0.8089\n",
            "batch: 100/160 train loss: 1.7448 train acc 0.8086\n",
            "batch: 101/160 train loss: 1.7455 train acc 0.8083\n",
            "batch: 102/160 train loss: 1.7441 train acc 0.8090\n",
            "batch: 103/160 train loss: 1.7441 train acc 0.8086\n",
            "batch: 104/160 train loss: 1.7435 train acc 0.8086\n",
            "batch: 105/160 train loss: 1.7433 train acc 0.8083\n",
            "batch: 106/160 train loss: 1.7431 train acc 0.8078\n",
            "batch: 107/160 train loss: 1.7423 train acc 0.8078\n",
            "batch: 108/160 train loss: 1.7396 train acc 0.8084\n",
            "batch: 109/160 train loss: 1.7391 train acc 0.8082\n",
            "batch: 110/160 train loss: 1.7384 train acc 0.8087\n",
            "batch: 111/160 train loss: 1.7395 train acc 0.8086\n",
            "batch: 112/160 train loss: 1.7394 train acc 0.8093\n",
            "batch: 113/160 train loss: 1.7405 train acc 0.8092\n",
            "batch: 114/160 train loss: 1.7403 train acc 0.8089\n",
            "batch: 115/160 train loss: 1.7408 train acc 0.8084\n",
            "batch: 116/160 train loss: 1.7391 train acc 0.8086\n",
            "batch: 117/160 train loss: 1.7386 train acc 0.8082\n",
            "batch: 118/160 train loss: 1.7372 train acc 0.8084\n",
            "batch: 119/160 train loss: 1.7388 train acc 0.8079\n",
            "batch: 120/160 train loss: 1.7382 train acc 0.8074\n",
            "batch: 121/160 train loss: 1.7371 train acc 0.8076\n",
            "batch: 122/160 train loss: 1.7357 train acc 0.8079\n",
            "batch: 123/160 train loss: 1.7342 train acc 0.8084\n",
            "batch: 124/160 train loss: 1.7336 train acc 0.8090\n",
            "batch: 125/160 train loss: 1.7324 train acc 0.8096\n",
            "batch: 126/160 train loss: 1.7332 train acc 0.8095\n",
            "batch: 127/160 train loss: 1.7317 train acc 0.8099\n",
            "batch: 128/160 train loss: 1.7305 train acc 0.8103\n",
            "batch: 129/160 train loss: 1.7327 train acc 0.8097\n",
            "batch: 130/160 train loss: 1.7342 train acc 0.8089\n",
            "batch: 131/160 train loss: 1.7353 train acc 0.8083\n",
            "batch: 132/160 train loss: 1.7347 train acc 0.8084\n",
            "batch: 133/160 train loss: 1.7347 train acc 0.8082\n",
            "batch: 134/160 train loss: 1.7339 train acc 0.8081\n",
            "batch: 135/160 train loss: 1.7323 train acc 0.8086\n",
            "batch: 136/160 train loss: 1.7330 train acc 0.8081\n",
            "batch: 137/160 train loss: 1.7325 train acc 0.8085\n",
            "batch: 138/160 train loss: 1.7325 train acc 0.8085\n",
            "batch: 139/160 train loss: 1.7315 train acc 0.8087\n",
            "batch: 140/160 train loss: 1.7319 train acc 0.8085\n",
            "batch: 141/160 train loss: 1.7328 train acc 0.8076\n",
            "batch: 142/160 train loss: 1.7319 train acc 0.8074\n",
            "batch: 143/160 train loss: 1.7299 train acc 0.8082\n",
            "batch: 144/160 train loss: 1.7285 train acc 0.8086\n",
            "batch: 145/160 train loss: 1.7280 train acc 0.8088\n",
            "batch: 146/160 train loss: 1.7274 train acc 0.8089\n",
            "batch: 147/160 train loss: 1.7273 train acc 0.8086\n",
            "batch: 148/160 train loss: 1.7279 train acc 0.8084\n",
            "batch: 149/160 train loss: 1.7286 train acc 0.8080\n",
            "batch: 150/160 train loss: 1.7281 train acc 0.8080\n",
            "batch: 151/160 train loss: 1.7285 train acc 0.8081\n",
            "batch: 152/160 train loss: 1.7295 train acc 0.8076\n",
            "batch: 153/160 train loss: 1.7290 train acc 0.8080\n",
            "batch: 154/160 train loss: 1.7295 train acc 0.8073\n",
            "batch: 155/160 train loss: 1.7293 train acc 0.8075\n",
            "batch: 156/160 train loss: 1.7287 train acc 0.8075\n",
            "batch: 157/160 train loss: 1.7304 train acc 0.8065\n",
            "batch: 158/160 train loss: 1.7291 train acc 0.8065\n",
            "batch: 159/160 train loss: 1.7293 train acc 0.8062\n",
            "batch: 160/160 train loss: 1.7292 train acc 0.8063\n",
            "\n",
            "Epoch 7 train loss: 1.7292 test loss 2.7783 train acc 0.8063 test acc 0.5058\n",
            "Epoch 8/25\n",
            "batch: 1/160 train loss: 1.4102 train acc 0.8906\n",
            "batch: 2/160 train loss: 1.4407 train acc 0.8906\n",
            "batch: 3/160 train loss: 1.4881 train acc 0.8802\n",
            "batch: 4/160 train loss: 1.5283 train acc 0.8477\n",
            "batch: 5/160 train loss: 1.5312 train acc 0.8438\n",
            "batch: 6/160 train loss: 1.5554 train acc 0.8359\n",
            "batch: 7/160 train loss: 1.5794 train acc 0.8326\n",
            "batch: 8/160 train loss: 1.5769 train acc 0.8301\n",
            "batch: 9/160 train loss: 1.5690 train acc 0.8333\n",
            "batch: 10/160 train loss: 1.5739 train acc 0.8344\n",
            "batch: 11/160 train loss: 1.5754 train acc 0.8352\n",
            "batch: 12/160 train loss: 1.5627 train acc 0.8385\n",
            "batch: 13/160 train loss: 1.5485 train acc 0.8413\n",
            "batch: 14/160 train loss: 1.5577 train acc 0.8438\n",
            "batch: 15/160 train loss: 1.5584 train acc 0.8458\n",
            "batch: 16/160 train loss: 1.5600 train acc 0.8457\n",
            "batch: 17/160 train loss: 1.5495 train acc 0.8493\n",
            "batch: 18/160 train loss: 1.5486 train acc 0.8490\n",
            "batch: 19/160 train loss: 1.5479 train acc 0.8512\n",
            "batch: 20/160 train loss: 1.5446 train acc 0.8523\n",
            "batch: 21/160 train loss: 1.5374 train acc 0.8557\n",
            "batch: 22/160 train loss: 1.5386 train acc 0.8594\n",
            "batch: 23/160 train loss: 1.5389 train acc 0.8594\n",
            "batch: 24/160 train loss: 1.5347 train acc 0.8594\n",
            "batch: 25/160 train loss: 1.5379 train acc 0.8588\n",
            "batch: 26/160 train loss: 1.5357 train acc 0.8606\n",
            "batch: 27/160 train loss: 1.5336 train acc 0.8634\n",
            "batch: 28/160 train loss: 1.5367 train acc 0.8627\n",
            "batch: 29/160 train loss: 1.5341 train acc 0.8637\n",
            "batch: 30/160 train loss: 1.5328 train acc 0.8615\n",
            "batch: 31/160 train loss: 1.5296 train acc 0.8624\n",
            "batch: 32/160 train loss: 1.5311 train acc 0.8628\n",
            "batch: 33/160 train loss: 1.5332 train acc 0.8636\n",
            "batch: 34/160 train loss: 1.5347 train acc 0.8617\n",
            "batch: 35/160 train loss: 1.5378 train acc 0.8616\n",
            "batch: 36/160 train loss: 1.5369 train acc 0.8611\n",
            "batch: 37/160 train loss: 1.5364 train acc 0.8594\n",
            "batch: 38/160 train loss: 1.5399 train acc 0.8602\n",
            "batch: 39/160 train loss: 1.5406 train acc 0.8602\n",
            "batch: 40/160 train loss: 1.5411 train acc 0.8598\n",
            "batch: 41/160 train loss: 1.5425 train acc 0.8586\n",
            "batch: 42/160 train loss: 1.5398 train acc 0.8597\n",
            "batch: 43/160 train loss: 1.5402 train acc 0.8608\n",
            "batch: 44/160 train loss: 1.5396 train acc 0.8604\n",
            "batch: 45/160 train loss: 1.5405 train acc 0.8604\n",
            "batch: 46/160 train loss: 1.5375 train acc 0.8611\n",
            "batch: 47/160 train loss: 1.5389 train acc 0.8607\n",
            "batch: 48/160 train loss: 1.5406 train acc 0.8604\n",
            "batch: 49/160 train loss: 1.5381 train acc 0.8607\n",
            "batch: 50/160 train loss: 1.5401 train acc 0.8603\n",
            "batch: 51/160 train loss: 1.5407 train acc 0.8606\n",
            "batch: 52/160 train loss: 1.5401 train acc 0.8597\n",
            "batch: 53/160 train loss: 1.5394 train acc 0.8603\n",
            "batch: 54/160 train loss: 1.5399 train acc 0.8600\n",
            "batch: 55/160 train loss: 1.5374 train acc 0.8608\n",
            "batch: 56/160 train loss: 1.5388 train acc 0.8599\n",
            "batch: 57/160 train loss: 1.5383 train acc 0.8594\n",
            "batch: 58/160 train loss: 1.5390 train acc 0.8596\n",
            "batch: 59/160 train loss: 1.5406 train acc 0.8586\n",
            "batch: 60/160 train loss: 1.5424 train acc 0.8586\n",
            "batch: 61/160 train loss: 1.5424 train acc 0.8584\n",
            "batch: 62/160 train loss: 1.5420 train acc 0.8586\n",
            "batch: 63/160 train loss: 1.5409 train acc 0.8589\n",
            "batch: 64/160 train loss: 1.5425 train acc 0.8591\n",
            "batch: 65/160 train loss: 1.5408 train acc 0.8599\n",
            "batch: 66/160 train loss: 1.5397 train acc 0.8603\n",
            "batch: 67/160 train loss: 1.5403 train acc 0.8603\n",
            "batch: 68/160 train loss: 1.5382 train acc 0.8612\n",
            "batch: 69/160 train loss: 1.5406 train acc 0.8607\n",
            "batch: 70/160 train loss: 1.5427 train acc 0.8598\n",
            "batch: 71/160 train loss: 1.5431 train acc 0.8600\n",
            "batch: 72/160 train loss: 1.5429 train acc 0.8596\n",
            "batch: 73/160 train loss: 1.5441 train acc 0.8596\n",
            "batch: 74/160 train loss: 1.5441 train acc 0.8587\n",
            "batch: 75/160 train loss: 1.5429 train acc 0.8588\n",
            "batch: 76/160 train loss: 1.5419 train acc 0.8586\n",
            "batch: 77/160 train loss: 1.5411 train acc 0.8586\n",
            "batch: 78/160 train loss: 1.5405 train acc 0.8592\n",
            "batch: 79/160 train loss: 1.5441 train acc 0.8582\n",
            "batch: 80/160 train loss: 1.5463 train acc 0.8574\n",
            "batch: 81/160 train loss: 1.5513 train acc 0.8567\n",
            "batch: 82/160 train loss: 1.5500 train acc 0.8573\n",
            "batch: 83/160 train loss: 1.5490 train acc 0.8579\n",
            "batch: 84/160 train loss: 1.5498 train acc 0.8573\n",
            "batch: 85/160 train loss: 1.5500 train acc 0.8572\n",
            "batch: 86/160 train loss: 1.5508 train acc 0.8566\n",
            "batch: 87/160 train loss: 1.5497 train acc 0.8567\n",
            "batch: 88/160 train loss: 1.5503 train acc 0.8569\n",
            "batch: 89/160 train loss: 1.5535 train acc 0.8562\n",
            "batch: 90/160 train loss: 1.5531 train acc 0.8557\n",
            "batch: 91/160 train loss: 1.5535 train acc 0.8559\n",
            "batch: 92/160 train loss: 1.5529 train acc 0.8565\n",
            "batch: 93/160 train loss: 1.5537 train acc 0.8560\n",
            "batch: 94/160 train loss: 1.5548 train acc 0.8552\n",
            "batch: 95/160 train loss: 1.5524 train acc 0.8559\n",
            "batch: 96/160 train loss: 1.5500 train acc 0.8569\n",
            "batch: 97/160 train loss: 1.5501 train acc 0.8570\n",
            "batch: 98/160 train loss: 1.5492 train acc 0.8575\n",
            "batch: 99/160 train loss: 1.5512 train acc 0.8572\n",
            "batch: 100/160 train loss: 1.5515 train acc 0.8572\n",
            "batch: 101/160 train loss: 1.5518 train acc 0.8571\n",
            "batch: 102/160 train loss: 1.5516 train acc 0.8568\n",
            "batch: 103/160 train loss: 1.5513 train acc 0.8573\n",
            "batch: 104/160 train loss: 1.5497 train acc 0.8574\n",
            "batch: 105/160 train loss: 1.5489 train acc 0.8580\n",
            "batch: 106/160 train loss: 1.5483 train acc 0.8580\n",
            "batch: 107/160 train loss: 1.5478 train acc 0.8584\n",
            "batch: 108/160 train loss: 1.5487 train acc 0.8582\n",
            "batch: 109/160 train loss: 1.5473 train acc 0.8587\n",
            "batch: 110/160 train loss: 1.5461 train acc 0.8591\n",
            "batch: 111/160 train loss: 1.5468 train acc 0.8590\n",
            "batch: 112/160 train loss: 1.5468 train acc 0.8592\n",
            "batch: 113/160 train loss: 1.5477 train acc 0.8594\n",
            "batch: 114/160 train loss: 1.5480 train acc 0.8590\n",
            "batch: 115/160 train loss: 1.5485 train acc 0.8590\n",
            "batch: 116/160 train loss: 1.5489 train acc 0.8588\n",
            "batch: 117/160 train loss: 1.5495 train acc 0.8588\n",
            "batch: 118/160 train loss: 1.5479 train acc 0.8591\n",
            "batch: 119/160 train loss: 1.5478 train acc 0.8590\n",
            "batch: 120/160 train loss: 1.5487 train acc 0.8585\n",
            "batch: 121/160 train loss: 1.5491 train acc 0.8585\n",
            "batch: 122/160 train loss: 1.5505 train acc 0.8580\n",
            "batch: 123/160 train loss: 1.5516 train acc 0.8580\n",
            "batch: 124/160 train loss: 1.5508 train acc 0.8581\n",
            "batch: 125/160 train loss: 1.5517 train acc 0.8579\n",
            "batch: 126/160 train loss: 1.5520 train acc 0.8578\n",
            "batch: 127/160 train loss: 1.5502 train acc 0.8585\n",
            "batch: 128/160 train loss: 1.5502 train acc 0.8584\n",
            "batch: 129/160 train loss: 1.5516 train acc 0.8576\n",
            "batch: 130/160 train loss: 1.5513 train acc 0.8581\n",
            "batch: 131/160 train loss: 1.5513 train acc 0.8584\n",
            "batch: 132/160 train loss: 1.5520 train acc 0.8582\n",
            "batch: 133/160 train loss: 1.5516 train acc 0.8584\n",
            "batch: 134/160 train loss: 1.5527 train acc 0.8584\n",
            "batch: 135/160 train loss: 1.5524 train acc 0.8583\n",
            "batch: 136/160 train loss: 1.5515 train acc 0.8586\n",
            "batch: 137/160 train loss: 1.5515 train acc 0.8586\n",
            "batch: 138/160 train loss: 1.5524 train acc 0.8585\n",
            "batch: 139/160 train loss: 1.5517 train acc 0.8589\n",
            "batch: 140/160 train loss: 1.5533 train acc 0.8586\n",
            "batch: 141/160 train loss: 1.5531 train acc 0.8586\n",
            "batch: 142/160 train loss: 1.5541 train acc 0.8584\n",
            "batch: 143/160 train loss: 1.5538 train acc 0.8586\n",
            "batch: 144/160 train loss: 1.5537 train acc 0.8587\n",
            "batch: 145/160 train loss: 1.5543 train acc 0.8584\n",
            "batch: 146/160 train loss: 1.5547 train acc 0.8584\n",
            "batch: 147/160 train loss: 1.5539 train acc 0.8587\n",
            "batch: 148/160 train loss: 1.5533 train acc 0.8590\n",
            "batch: 149/160 train loss: 1.5535 train acc 0.8587\n",
            "batch: 150/160 train loss: 1.5536 train acc 0.8588\n",
            "batch: 151/160 train loss: 1.5557 train acc 0.8578\n",
            "batch: 152/160 train loss: 1.5563 train acc 0.8575\n",
            "batch: 153/160 train loss: 1.5563 train acc 0.8574\n",
            "batch: 154/160 train loss: 1.5552 train acc 0.8575\n",
            "batch: 155/160 train loss: 1.5559 train acc 0.8567\n",
            "batch: 156/160 train loss: 1.5552 train acc 0.8570\n",
            "batch: 157/160 train loss: 1.5553 train acc 0.8570\n",
            "batch: 158/160 train loss: 1.5549 train acc 0.8571\n",
            "batch: 159/160 train loss: 1.5550 train acc 0.8571\n",
            "batch: 160/160 train loss: 1.5554 train acc 0.8572\n",
            "\n",
            "Epoch 8 train loss: 1.5554 test loss 2.6918 train acc 0.8572 test acc 0.5167\n",
            "Epoch 9/25\n",
            "batch: 1/160 train loss: 1.3445 train acc 0.9219\n",
            "batch: 2/160 train loss: 1.3581 train acc 0.8828\n",
            "batch: 3/160 train loss: 1.3862 train acc 0.8906\n",
            "batch: 4/160 train loss: 1.4262 train acc 0.8750\n",
            "batch: 5/160 train loss: 1.4137 train acc 0.8875\n",
            "batch: 6/160 train loss: 1.3972 train acc 0.8958\n",
            "batch: 7/160 train loss: 1.3833 train acc 0.8996\n",
            "batch: 8/160 train loss: 1.3903 train acc 0.9043\n",
            "batch: 9/160 train loss: 1.3819 train acc 0.9097\n",
            "batch: 10/160 train loss: 1.3943 train acc 0.9062\n",
            "batch: 11/160 train loss: 1.3979 train acc 0.9048\n",
            "batch: 12/160 train loss: 1.3923 train acc 0.9062\n",
            "batch: 13/160 train loss: 1.3951 train acc 0.9038\n",
            "batch: 14/160 train loss: 1.3978 train acc 0.9040\n",
            "batch: 15/160 train loss: 1.4046 train acc 0.9052\n",
            "batch: 16/160 train loss: 1.4024 train acc 0.9072\n",
            "batch: 17/160 train loss: 1.4039 train acc 0.9081\n",
            "batch: 18/160 train loss: 1.4091 train acc 0.9080\n",
            "batch: 19/160 train loss: 1.4216 train acc 0.9054\n",
            "batch: 20/160 train loss: 1.4145 train acc 0.9070\n",
            "batch: 21/160 train loss: 1.4199 train acc 0.9048\n",
            "batch: 22/160 train loss: 1.4237 train acc 0.9048\n",
            "batch: 23/160 train loss: 1.4274 train acc 0.9056\n",
            "batch: 24/160 train loss: 1.4229 train acc 0.9069\n",
            "batch: 25/160 train loss: 1.4213 train acc 0.9081\n",
            "batch: 26/160 train loss: 1.4143 train acc 0.9111\n",
            "batch: 27/160 train loss: 1.4154 train acc 0.9103\n",
            "batch: 28/160 train loss: 1.4122 train acc 0.9113\n",
            "batch: 29/160 train loss: 1.4112 train acc 0.9100\n",
            "batch: 30/160 train loss: 1.4110 train acc 0.9109\n",
            "batch: 31/160 train loss: 1.4102 train acc 0.9113\n",
            "batch: 32/160 train loss: 1.4144 train acc 0.9087\n",
            "batch: 33/160 train loss: 1.4129 train acc 0.9096\n",
            "batch: 34/160 train loss: 1.4170 train acc 0.9090\n",
            "batch: 35/160 train loss: 1.4111 train acc 0.9107\n",
            "batch: 36/160 train loss: 1.4150 train acc 0.9089\n",
            "batch: 37/160 train loss: 1.4149 train acc 0.9079\n",
            "batch: 38/160 train loss: 1.4131 train acc 0.9067\n",
            "batch: 39/160 train loss: 1.4158 train acc 0.9058\n",
            "batch: 40/160 train loss: 1.4154 train acc 0.9043\n",
            "batch: 41/160 train loss: 1.4149 train acc 0.9032\n",
            "batch: 42/160 train loss: 1.4149 train acc 0.9025\n",
            "batch: 43/160 train loss: 1.4143 train acc 0.9019\n",
            "batch: 44/160 train loss: 1.4142 train acc 0.9016\n",
            "batch: 45/160 train loss: 1.4161 train acc 0.9014\n",
            "batch: 46/160 train loss: 1.4142 train acc 0.9015\n",
            "batch: 47/160 train loss: 1.4151 train acc 0.9009\n",
            "batch: 48/160 train loss: 1.4167 train acc 0.9014\n",
            "batch: 49/160 train loss: 1.4124 train acc 0.9021\n",
            "batch: 50/160 train loss: 1.4109 train acc 0.9022\n",
            "batch: 51/160 train loss: 1.4107 train acc 0.9020\n",
            "batch: 52/160 train loss: 1.4116 train acc 0.9011\n",
            "batch: 53/160 train loss: 1.4124 train acc 0.9001\n",
            "batch: 54/160 train loss: 1.4102 train acc 0.9005\n",
            "batch: 55/160 train loss: 1.4086 train acc 0.9009\n",
            "batch: 56/160 train loss: 1.4089 train acc 0.9012\n",
            "batch: 57/160 train loss: 1.4106 train acc 0.8994\n",
            "batch: 58/160 train loss: 1.4129 train acc 0.8987\n",
            "batch: 59/160 train loss: 1.4098 train acc 0.8994\n",
            "batch: 60/160 train loss: 1.4095 train acc 0.8990\n",
            "batch: 61/160 train loss: 1.4092 train acc 0.8991\n",
            "batch: 62/160 train loss: 1.4118 train acc 0.8979\n",
            "batch: 63/160 train loss: 1.4118 train acc 0.8978\n",
            "batch: 64/160 train loss: 1.4119 train acc 0.8975\n",
            "batch: 65/160 train loss: 1.4129 train acc 0.8964\n",
            "batch: 66/160 train loss: 1.4132 train acc 0.8968\n",
            "batch: 67/160 train loss: 1.4156 train acc 0.8969\n",
            "batch: 68/160 train loss: 1.4151 train acc 0.8968\n",
            "batch: 69/160 train loss: 1.4158 train acc 0.8961\n",
            "batch: 70/160 train loss: 1.4133 train acc 0.8967\n",
            "batch: 71/160 train loss: 1.4138 train acc 0.8968\n",
            "batch: 72/160 train loss: 1.4142 train acc 0.8971\n",
            "batch: 73/160 train loss: 1.4132 train acc 0.8968\n",
            "batch: 74/160 train loss: 1.4111 train acc 0.8980\n",
            "batch: 75/160 train loss: 1.4126 train acc 0.8979\n",
            "batch: 76/160 train loss: 1.4133 train acc 0.8974\n",
            "batch: 77/160 train loss: 1.4128 train acc 0.8973\n",
            "batch: 78/160 train loss: 1.4113 train acc 0.8976\n",
            "batch: 79/160 train loss: 1.4146 train acc 0.8968\n",
            "batch: 80/160 train loss: 1.4131 train acc 0.8979\n",
            "batch: 81/160 train loss: 1.4123 train acc 0.8987\n",
            "batch: 82/160 train loss: 1.4122 train acc 0.8982\n",
            "batch: 83/160 train loss: 1.4112 train acc 0.8982\n",
            "batch: 84/160 train loss: 1.4112 train acc 0.8984\n",
            "batch: 85/160 train loss: 1.4096 train acc 0.8991\n",
            "batch: 86/160 train loss: 1.4089 train acc 0.8995\n",
            "batch: 87/160 train loss: 1.4079 train acc 0.9000\n",
            "batch: 88/160 train loss: 1.4077 train acc 0.9000\n",
            "batch: 89/160 train loss: 1.4087 train acc 0.8991\n",
            "batch: 90/160 train loss: 1.4079 train acc 0.8991\n",
            "batch: 91/160 train loss: 1.4077 train acc 0.8990\n",
            "batch: 92/160 train loss: 1.4088 train acc 0.8986\n",
            "batch: 93/160 train loss: 1.4071 train acc 0.8992\n",
            "batch: 94/160 train loss: 1.4074 train acc 0.8994\n",
            "batch: 95/160 train loss: 1.4082 train acc 0.8990\n",
            "batch: 96/160 train loss: 1.4078 train acc 0.8989\n",
            "batch: 97/160 train loss: 1.4083 train acc 0.8990\n",
            "batch: 98/160 train loss: 1.4085 train acc 0.8984\n",
            "batch: 99/160 train loss: 1.4089 train acc 0.8985\n",
            "batch: 100/160 train loss: 1.4099 train acc 0.8981\n",
            "batch: 101/160 train loss: 1.4104 train acc 0.8974\n",
            "batch: 102/160 train loss: 1.4123 train acc 0.8968\n",
            "batch: 103/160 train loss: 1.4118 train acc 0.8971\n",
            "batch: 104/160 train loss: 1.4135 train acc 0.8968\n",
            "batch: 105/160 train loss: 1.4139 train acc 0.8963\n",
            "batch: 106/160 train loss: 1.4141 train acc 0.8965\n",
            "batch: 107/160 train loss: 1.4153 train acc 0.8963\n",
            "batch: 108/160 train loss: 1.4155 train acc 0.8958\n",
            "batch: 109/160 train loss: 1.4168 train acc 0.8952\n",
            "batch: 110/160 train loss: 1.4180 train acc 0.8949\n",
            "batch: 111/160 train loss: 1.4176 train acc 0.8946\n",
            "batch: 112/160 train loss: 1.4185 train acc 0.8940\n",
            "batch: 113/160 train loss: 1.4195 train acc 0.8937\n",
            "batch: 114/160 train loss: 1.4194 train acc 0.8938\n",
            "batch: 115/160 train loss: 1.4202 train acc 0.8935\n",
            "batch: 116/160 train loss: 1.4205 train acc 0.8933\n",
            "batch: 117/160 train loss: 1.4192 train acc 0.8933\n",
            "batch: 118/160 train loss: 1.4206 train acc 0.8931\n",
            "batch: 119/160 train loss: 1.4219 train acc 0.8930\n",
            "batch: 120/160 train loss: 1.4216 train acc 0.8928\n",
            "batch: 121/160 train loss: 1.4220 train acc 0.8926\n",
            "batch: 122/160 train loss: 1.4219 train acc 0.8928\n",
            "batch: 123/160 train loss: 1.4215 train acc 0.8927\n",
            "batch: 124/160 train loss: 1.4210 train acc 0.8925\n",
            "batch: 125/160 train loss: 1.4213 train acc 0.8919\n",
            "batch: 126/160 train loss: 1.4213 train acc 0.8920\n",
            "batch: 127/160 train loss: 1.4215 train acc 0.8920\n",
            "batch: 128/160 train loss: 1.4215 train acc 0.8920\n",
            "batch: 129/160 train loss: 1.4212 train acc 0.8924\n",
            "batch: 130/160 train loss: 1.4221 train acc 0.8923\n",
            "batch: 131/160 train loss: 1.4213 train acc 0.8927\n",
            "batch: 132/160 train loss: 1.4213 train acc 0.8924\n",
            "batch: 133/160 train loss: 1.4218 train acc 0.8922\n",
            "batch: 134/160 train loss: 1.4227 train acc 0.8918\n",
            "batch: 135/160 train loss: 1.4236 train acc 0.8913\n",
            "batch: 136/160 train loss: 1.4238 train acc 0.8914\n",
            "batch: 137/160 train loss: 1.4236 train acc 0.8914\n",
            "batch: 138/160 train loss: 1.4230 train acc 0.8914\n",
            "batch: 139/160 train loss: 1.4235 train acc 0.8908\n",
            "batch: 140/160 train loss: 1.4241 train acc 0.8910\n",
            "batch: 141/160 train loss: 1.4256 train acc 0.8904\n",
            "batch: 142/160 train loss: 1.4249 train acc 0.8905\n",
            "batch: 143/160 train loss: 1.4261 train acc 0.8900\n",
            "batch: 144/160 train loss: 1.4266 train acc 0.8895\n",
            "batch: 145/160 train loss: 1.4265 train acc 0.8894\n",
            "batch: 146/160 train loss: 1.4274 train acc 0.8891\n",
            "batch: 147/160 train loss: 1.4265 train acc 0.8893\n",
            "batch: 148/160 train loss: 1.4264 train acc 0.8893\n",
            "batch: 149/160 train loss: 1.4277 train acc 0.8885\n",
            "batch: 150/160 train loss: 1.4276 train acc 0.8884\n",
            "batch: 151/160 train loss: 1.4273 train acc 0.8885\n",
            "batch: 152/160 train loss: 1.4279 train acc 0.8878\n",
            "batch: 153/160 train loss: 1.4274 train acc 0.8882\n",
            "batch: 154/160 train loss: 1.4259 train acc 0.8886\n",
            "batch: 155/160 train loss: 1.4260 train acc 0.8886\n",
            "batch: 156/160 train loss: 1.4257 train acc 0.8888\n",
            "batch: 157/160 train loss: 1.4263 train acc 0.8888\n",
            "batch: 158/160 train loss: 1.4264 train acc 0.8886\n",
            "batch: 159/160 train loss: 1.4269 train acc 0.8883\n",
            "batch: 160/160 train loss: 1.4281 train acc 0.8879\n",
            "\n",
            "Epoch 9 train loss: 1.4281 test loss 2.6346 train acc 0.8879 test acc 0.5425\n",
            "Epoch 10/25\n",
            "batch: 1/160 train loss: 1.3126 train acc 0.9531\n",
            "batch: 2/160 train loss: 1.2450 train acc 0.9531\n",
            "batch: 3/160 train loss: 1.2254 train acc 0.9427\n",
            "batch: 4/160 train loss: 1.2666 train acc 0.9297\n",
            "batch: 5/160 train loss: 1.2595 train acc 0.9406\n",
            "batch: 6/160 train loss: 1.2855 train acc 0.9297\n",
            "batch: 7/160 train loss: 1.2905 train acc 0.9286\n",
            "batch: 8/160 train loss: 1.2875 train acc 0.9277\n",
            "batch: 9/160 train loss: 1.2993 train acc 0.9219\n",
            "batch: 10/160 train loss: 1.3124 train acc 0.9203\n",
            "batch: 11/160 train loss: 1.3124 train acc 0.9205\n",
            "batch: 12/160 train loss: 1.3257 train acc 0.9167\n",
            "batch: 13/160 train loss: 1.3259 train acc 0.9159\n",
            "batch: 14/160 train loss: 1.3230 train acc 0.9152\n",
            "batch: 15/160 train loss: 1.3230 train acc 0.9167\n",
            "batch: 16/160 train loss: 1.3214 train acc 0.9180\n",
            "batch: 17/160 train loss: 1.3232 train acc 0.9182\n",
            "batch: 18/160 train loss: 1.3194 train acc 0.9210\n",
            "batch: 19/160 train loss: 1.3216 train acc 0.9211\n",
            "batch: 20/160 train loss: 1.3226 train acc 0.9203\n",
            "batch: 21/160 train loss: 1.3218 train acc 0.9226\n",
            "batch: 22/160 train loss: 1.3361 train acc 0.9205\n",
            "batch: 23/160 train loss: 1.3387 train acc 0.9205\n",
            "batch: 24/160 train loss: 1.3395 train acc 0.9206\n",
            "batch: 25/160 train loss: 1.3320 train acc 0.9231\n",
            "batch: 26/160 train loss: 1.3294 train acc 0.9225\n",
            "batch: 27/160 train loss: 1.3276 train acc 0.9225\n",
            "batch: 28/160 train loss: 1.3228 train acc 0.9235\n",
            "batch: 29/160 train loss: 1.3199 train acc 0.9246\n",
            "batch: 30/160 train loss: 1.3140 train acc 0.9260\n",
            "batch: 31/160 train loss: 1.3073 train acc 0.9284\n",
            "batch: 32/160 train loss: 1.3074 train acc 0.9292\n",
            "batch: 33/160 train loss: 1.3079 train acc 0.9276\n",
            "batch: 34/160 train loss: 1.3056 train acc 0.9274\n",
            "batch: 35/160 train loss: 1.3034 train acc 0.9268\n",
            "batch: 36/160 train loss: 1.2981 train acc 0.9284\n",
            "batch: 37/160 train loss: 1.3024 train acc 0.9282\n",
            "batch: 38/160 train loss: 1.3045 train acc 0.9276\n",
            "batch: 39/160 train loss: 1.3038 train acc 0.9279\n",
            "batch: 40/160 train loss: 1.3034 train acc 0.9281\n",
            "batch: 41/160 train loss: 1.3023 train acc 0.9280\n",
            "batch: 42/160 train loss: 1.3065 train acc 0.9267\n",
            "batch: 43/160 train loss: 1.3038 train acc 0.9266\n",
            "batch: 44/160 train loss: 1.2991 train acc 0.9276\n",
            "batch: 45/160 train loss: 1.2973 train acc 0.9281\n",
            "batch: 46/160 train loss: 1.2935 train acc 0.9293\n",
            "batch: 47/160 train loss: 1.2923 train acc 0.9299\n",
            "batch: 48/160 train loss: 1.2916 train acc 0.9300\n",
            "batch: 49/160 train loss: 1.2906 train acc 0.9305\n",
            "batch: 50/160 train loss: 1.2897 train acc 0.9306\n",
            "batch: 51/160 train loss: 1.2875 train acc 0.9314\n",
            "batch: 52/160 train loss: 1.2856 train acc 0.9324\n",
            "batch: 53/160 train loss: 1.2831 train acc 0.9322\n",
            "batch: 54/160 train loss: 1.2844 train acc 0.9317\n",
            "batch: 55/160 train loss: 1.2835 train acc 0.9318\n",
            "batch: 56/160 train loss: 1.2821 train acc 0.9316\n",
            "batch: 57/160 train loss: 1.2836 train acc 0.9315\n",
            "batch: 58/160 train loss: 1.2837 train acc 0.9318\n",
            "batch: 59/160 train loss: 1.2859 train acc 0.9311\n",
            "batch: 60/160 train loss: 1.2858 train acc 0.9305\n",
            "batch: 61/160 train loss: 1.2847 train acc 0.9311\n",
            "batch: 62/160 train loss: 1.2833 train acc 0.9309\n",
            "batch: 63/160 train loss: 1.2811 train acc 0.9313\n",
            "batch: 64/160 train loss: 1.2800 train acc 0.9314\n",
            "batch: 65/160 train loss: 1.2814 train acc 0.9310\n",
            "batch: 66/160 train loss: 1.2798 train acc 0.9316\n",
            "batch: 67/160 train loss: 1.2774 train acc 0.9324\n",
            "batch: 68/160 train loss: 1.2760 train acc 0.9324\n",
            "batch: 69/160 train loss: 1.2756 train acc 0.9325\n",
            "batch: 70/160 train loss: 1.2759 train acc 0.9326\n",
            "batch: 71/160 train loss: 1.2754 train acc 0.9329\n",
            "batch: 72/160 train loss: 1.2752 train acc 0.9332\n",
            "batch: 73/160 train loss: 1.2732 train acc 0.9336\n",
            "batch: 74/160 train loss: 1.2742 train acc 0.9329\n",
            "batch: 75/160 train loss: 1.2749 train acc 0.9321\n",
            "batch: 76/160 train loss: 1.2742 train acc 0.9328\n",
            "batch: 77/160 train loss: 1.2747 train acc 0.9318\n",
            "batch: 78/160 train loss: 1.2763 train acc 0.9317\n",
            "batch: 79/160 train loss: 1.2758 train acc 0.9316\n",
            "batch: 80/160 train loss: 1.2752 train acc 0.9316\n",
            "batch: 81/160 train loss: 1.2747 train acc 0.9313\n",
            "batch: 82/160 train loss: 1.2722 train acc 0.9318\n",
            "batch: 83/160 train loss: 1.2723 train acc 0.9317\n",
            "batch: 84/160 train loss: 1.2731 train acc 0.9314\n",
            "batch: 85/160 train loss: 1.2733 train acc 0.9314\n",
            "batch: 86/160 train loss: 1.2739 train acc 0.9311\n",
            "batch: 87/160 train loss: 1.2737 train acc 0.9314\n",
            "batch: 88/160 train loss: 1.2743 train acc 0.9311\n",
            "batch: 89/160 train loss: 1.2724 train acc 0.9315\n",
            "batch: 90/160 train loss: 1.2736 train acc 0.9307\n",
            "batch: 91/160 train loss: 1.2747 train acc 0.9301\n",
            "batch: 92/160 train loss: 1.2753 train acc 0.9295\n",
            "batch: 93/160 train loss: 1.2730 train acc 0.9303\n",
            "batch: 94/160 train loss: 1.2716 train acc 0.9304\n",
            "batch: 95/160 train loss: 1.2707 train acc 0.9308\n",
            "batch: 96/160 train loss: 1.2706 train acc 0.9312\n",
            "batch: 97/160 train loss: 1.2709 train acc 0.9312\n",
            "batch: 98/160 train loss: 1.2696 train acc 0.9314\n",
            "batch: 99/160 train loss: 1.2707 train acc 0.9312\n",
            "batch: 100/160 train loss: 1.2727 train acc 0.9311\n",
            "batch: 101/160 train loss: 1.2718 train acc 0.9312\n",
            "batch: 102/160 train loss: 1.2733 train acc 0.9309\n",
            "batch: 103/160 train loss: 1.2725 train acc 0.9310\n",
            "batch: 104/160 train loss: 1.2721 train acc 0.9310\n",
            "batch: 105/160 train loss: 1.2729 train acc 0.9307\n",
            "batch: 106/160 train loss: 1.2737 train acc 0.9304\n",
            "batch: 107/160 train loss: 1.2740 train acc 0.9302\n",
            "batch: 108/160 train loss: 1.2736 train acc 0.9304\n",
            "batch: 109/160 train loss: 1.2744 train acc 0.9302\n",
            "batch: 110/160 train loss: 1.2735 train acc 0.9305\n",
            "batch: 111/160 train loss: 1.2733 train acc 0.9302\n",
            "batch: 112/160 train loss: 1.2720 train acc 0.9304\n",
            "batch: 113/160 train loss: 1.2720 train acc 0.9303\n",
            "batch: 114/160 train loss: 1.2708 train acc 0.9304\n",
            "batch: 115/160 train loss: 1.2700 train acc 0.9303\n",
            "batch: 116/160 train loss: 1.2717 train acc 0.9298\n",
            "batch: 117/160 train loss: 1.2705 train acc 0.9303\n",
            "batch: 118/160 train loss: 1.2698 train acc 0.9302\n",
            "batch: 119/160 train loss: 1.2709 train acc 0.9295\n",
            "batch: 120/160 train loss: 1.2711 train acc 0.9298\n",
            "batch: 121/160 train loss: 1.2707 train acc 0.9294\n",
            "batch: 122/160 train loss: 1.2709 train acc 0.9293\n",
            "batch: 123/160 train loss: 1.2709 train acc 0.9292\n",
            "batch: 124/160 train loss: 1.2707 train acc 0.9294\n",
            "batch: 125/160 train loss: 1.2708 train acc 0.9293\n",
            "batch: 126/160 train loss: 1.2707 train acc 0.9291\n",
            "batch: 127/160 train loss: 1.2705 train acc 0.9293\n",
            "batch: 128/160 train loss: 1.2699 train acc 0.9293\n",
            "batch: 129/160 train loss: 1.2699 train acc 0.9294\n",
            "batch: 130/160 train loss: 1.2705 train acc 0.9287\n",
            "batch: 131/160 train loss: 1.2705 train acc 0.9286\n",
            "batch: 132/160 train loss: 1.2700 train acc 0.9289\n",
            "batch: 133/160 train loss: 1.2700 train acc 0.9289\n",
            "batch: 134/160 train loss: 1.2697 train acc 0.9292\n",
            "batch: 135/160 train loss: 1.2696 train acc 0.9293\n",
            "batch: 136/160 train loss: 1.2682 train acc 0.9293\n",
            "batch: 137/160 train loss: 1.2687 train acc 0.9291\n",
            "batch: 138/160 train loss: 1.2689 train acc 0.9289\n",
            "batch: 139/160 train loss: 1.2692 train acc 0.9285\n",
            "batch: 140/160 train loss: 1.2693 train acc 0.9285\n",
            "batch: 141/160 train loss: 1.2685 train acc 0.9284\n",
            "batch: 142/160 train loss: 1.2681 train acc 0.9286\n",
            "batch: 143/160 train loss: 1.2673 train acc 0.9288\n",
            "batch: 144/160 train loss: 1.2664 train acc 0.9289\n",
            "batch: 145/160 train loss: 1.2667 train acc 0.9289\n",
            "batch: 146/160 train loss: 1.2662 train acc 0.9290\n",
            "batch: 147/160 train loss: 1.2659 train acc 0.9293\n",
            "batch: 148/160 train loss: 1.2667 train acc 0.9288\n",
            "batch: 149/160 train loss: 1.2673 train acc 0.9286\n",
            "batch: 150/160 train loss: 1.2667 train acc 0.9289\n",
            "batch: 151/160 train loss: 1.2666 train acc 0.9287\n",
            "batch: 152/160 train loss: 1.2658 train acc 0.9289\n",
            "batch: 153/160 train loss: 1.2655 train acc 0.9288\n",
            "batch: 154/160 train loss: 1.2646 train acc 0.9292\n",
            "batch: 155/160 train loss: 1.2660 train acc 0.9287\n",
            "batch: 156/160 train loss: 1.2656 train acc 0.9289\n",
            "batch: 157/160 train loss: 1.2667 train acc 0.9283\n",
            "batch: 158/160 train loss: 1.2672 train acc 0.9283\n",
            "batch: 159/160 train loss: 1.2683 train acc 0.9282\n",
            "batch: 160/160 train loss: 1.2696 train acc 0.9280\n",
            "\n",
            "Epoch 10 train loss: 1.2696 test loss 2.5279 train acc 0.9280 test acc 0.5750\n",
            "Epoch 11/25\n",
            "batch: 1/160 train loss: 1.1988 train acc 0.8906\n",
            "batch: 2/160 train loss: 1.1613 train acc 0.9219\n",
            "batch: 3/160 train loss: 1.1645 train acc 0.9271\n",
            "batch: 4/160 train loss: 1.1603 train acc 0.9336\n",
            "batch: 5/160 train loss: 1.1603 train acc 0.9406\n",
            "batch: 6/160 train loss: 1.1674 train acc 0.9401\n",
            "batch: 7/160 train loss: 1.1564 train acc 0.9442\n",
            "batch: 8/160 train loss: 1.1581 train acc 0.9434\n",
            "batch: 9/160 train loss: 1.1634 train acc 0.9462\n",
            "batch: 10/160 train loss: 1.1566 train acc 0.9500\n",
            "batch: 11/160 train loss: 1.1720 train acc 0.9460\n",
            "batch: 12/160 train loss: 1.1674 train acc 0.9479\n",
            "batch: 13/160 train loss: 1.1693 train acc 0.9483\n",
            "batch: 14/160 train loss: 1.1680 train acc 0.9498\n",
            "batch: 15/160 train loss: 1.1653 train acc 0.9521\n",
            "batch: 16/160 train loss: 1.1634 train acc 0.9512\n",
            "batch: 17/160 train loss: 1.1627 train acc 0.9513\n",
            "batch: 18/160 train loss: 1.1630 train acc 0.9531\n",
            "batch: 19/160 train loss: 1.1600 train acc 0.9548\n",
            "batch: 20/160 train loss: 1.1589 train acc 0.9547\n",
            "batch: 21/160 train loss: 1.1587 train acc 0.9531\n",
            "batch: 22/160 train loss: 1.1606 train acc 0.9531\n",
            "batch: 23/160 train loss: 1.1596 train acc 0.9538\n",
            "batch: 24/160 train loss: 1.1576 train acc 0.9544\n",
            "batch: 25/160 train loss: 1.1550 train acc 0.9550\n",
            "batch: 26/160 train loss: 1.1526 train acc 0.9561\n",
            "batch: 27/160 train loss: 1.1531 train acc 0.9572\n",
            "batch: 28/160 train loss: 1.1573 train acc 0.9565\n",
            "batch: 29/160 train loss: 1.1584 train acc 0.9553\n",
            "batch: 30/160 train loss: 1.1594 train acc 0.9531\n",
            "batch: 31/160 train loss: 1.1658 train acc 0.9521\n",
            "batch: 32/160 train loss: 1.1702 train acc 0.9512\n",
            "batch: 33/160 train loss: 1.1715 train acc 0.9508\n",
            "batch: 34/160 train loss: 1.1728 train acc 0.9504\n",
            "batch: 35/160 train loss: 1.1723 train acc 0.9500\n",
            "batch: 36/160 train loss: 1.1693 train acc 0.9501\n",
            "batch: 37/160 train loss: 1.1675 train acc 0.9510\n",
            "batch: 38/160 train loss: 1.1706 train acc 0.9502\n",
            "batch: 39/160 train loss: 1.1721 train acc 0.9503\n",
            "batch: 40/160 train loss: 1.1724 train acc 0.9500\n",
            "batch: 41/160 train loss: 1.1731 train acc 0.9489\n",
            "batch: 42/160 train loss: 1.1754 train acc 0.9487\n",
            "batch: 43/160 train loss: 1.1765 train acc 0.9484\n",
            "batch: 44/160 train loss: 1.1771 train acc 0.9482\n",
            "batch: 45/160 train loss: 1.1780 train acc 0.9483\n",
            "batch: 46/160 train loss: 1.1752 train acc 0.9494\n",
            "batch: 47/160 train loss: 1.1791 train acc 0.9478\n",
            "batch: 48/160 train loss: 1.1833 train acc 0.9469\n",
            "batch: 49/160 train loss: 1.1810 train acc 0.9474\n",
            "batch: 50/160 train loss: 1.1789 train acc 0.9475\n",
            "batch: 51/160 train loss: 1.1804 train acc 0.9476\n",
            "batch: 52/160 train loss: 1.1801 train acc 0.9471\n",
            "batch: 53/160 train loss: 1.1802 train acc 0.9475\n",
            "batch: 54/160 train loss: 1.1800 train acc 0.9476\n",
            "batch: 55/160 train loss: 1.1799 train acc 0.9472\n",
            "batch: 56/160 train loss: 1.1804 train acc 0.9473\n",
            "batch: 57/160 train loss: 1.1817 train acc 0.9471\n",
            "batch: 58/160 train loss: 1.1808 train acc 0.9475\n",
            "batch: 59/160 train loss: 1.1819 train acc 0.9470\n",
            "batch: 60/160 train loss: 1.1796 train acc 0.9471\n",
            "batch: 61/160 train loss: 1.1807 train acc 0.9465\n",
            "batch: 62/160 train loss: 1.1830 train acc 0.9458\n",
            "batch: 63/160 train loss: 1.1821 train acc 0.9462\n",
            "batch: 64/160 train loss: 1.1851 train acc 0.9458\n",
            "batch: 65/160 train loss: 1.1857 train acc 0.9457\n",
            "batch: 66/160 train loss: 1.1859 train acc 0.9458\n",
            "batch: 67/160 train loss: 1.1854 train acc 0.9464\n",
            "batch: 68/160 train loss: 1.1865 train acc 0.9462\n",
            "batch: 69/160 train loss: 1.1857 train acc 0.9468\n",
            "batch: 70/160 train loss: 1.1870 train acc 0.9471\n",
            "batch: 71/160 train loss: 1.1870 train acc 0.9474\n",
            "batch: 72/160 train loss: 1.1881 train acc 0.9470\n",
            "batch: 73/160 train loss: 1.1882 train acc 0.9469\n",
            "batch: 74/160 train loss: 1.1893 train acc 0.9464\n",
            "batch: 75/160 train loss: 1.1888 train acc 0.9463\n",
            "batch: 76/160 train loss: 1.1898 train acc 0.9465\n",
            "batch: 77/160 train loss: 1.1929 train acc 0.9454\n",
            "batch: 78/160 train loss: 1.1940 train acc 0.9453\n",
            "batch: 79/160 train loss: 1.1943 train acc 0.9454\n",
            "batch: 80/160 train loss: 1.1952 train acc 0.9455\n",
            "batch: 81/160 train loss: 1.1959 train acc 0.9452\n",
            "batch: 82/160 train loss: 1.1976 train acc 0.9449\n",
            "batch: 83/160 train loss: 1.1972 train acc 0.9450\n",
            "batch: 84/160 train loss: 1.1977 train acc 0.9449\n",
            "batch: 85/160 train loss: 1.1981 train acc 0.9449\n",
            "batch: 86/160 train loss: 1.1974 train acc 0.9448\n",
            "batch: 87/160 train loss: 1.1965 train acc 0.9449\n",
            "batch: 88/160 train loss: 1.1972 train acc 0.9451\n",
            "batch: 89/160 train loss: 1.1972 train acc 0.9452\n",
            "batch: 90/160 train loss: 1.1980 train acc 0.9451\n",
            "batch: 91/160 train loss: 1.1997 train acc 0.9447\n",
            "batch: 92/160 train loss: 1.1995 train acc 0.9450\n",
            "batch: 93/160 train loss: 1.2000 train acc 0.9446\n",
            "batch: 94/160 train loss: 1.2002 train acc 0.9443\n",
            "batch: 95/160 train loss: 1.2015 train acc 0.9439\n",
            "batch: 96/160 train loss: 1.2014 train acc 0.9437\n",
            "batch: 97/160 train loss: 1.2008 train acc 0.9439\n",
            "batch: 98/160 train loss: 1.2007 train acc 0.9437\n",
            "batch: 99/160 train loss: 1.2011 train acc 0.9437\n",
            "batch: 100/160 train loss: 1.2005 train acc 0.9437\n",
            "batch: 101/160 train loss: 1.1994 train acc 0.9440\n",
            "batch: 102/160 train loss: 1.2001 train acc 0.9438\n",
            "batch: 103/160 train loss: 1.1999 train acc 0.9439\n",
            "batch: 104/160 train loss: 1.1994 train acc 0.9441\n",
            "batch: 105/160 train loss: 1.2003 train acc 0.9443\n",
            "batch: 106/160 train loss: 1.2004 train acc 0.9446\n",
            "batch: 107/160 train loss: 1.2010 train acc 0.9442\n",
            "batch: 108/160 train loss: 1.2016 train acc 0.9442\n",
            "batch: 109/160 train loss: 1.2015 train acc 0.9441\n",
            "batch: 110/160 train loss: 1.2016 train acc 0.9442\n",
            "batch: 111/160 train loss: 1.2013 train acc 0.9443\n",
            "batch: 112/160 train loss: 1.2019 train acc 0.9439\n",
            "batch: 113/160 train loss: 1.2008 train acc 0.9443\n",
            "batch: 114/160 train loss: 1.2001 train acc 0.9442\n",
            "batch: 115/160 train loss: 1.1999 train acc 0.9444\n",
            "batch: 116/160 train loss: 1.1997 train acc 0.9444\n",
            "batch: 117/160 train loss: 1.1997 train acc 0.9446\n",
            "batch: 118/160 train loss: 1.2005 train acc 0.9443\n",
            "batch: 119/160 train loss: 1.2006 train acc 0.9442\n",
            "batch: 120/160 train loss: 1.2004 train acc 0.9445\n",
            "batch: 121/160 train loss: 1.2004 train acc 0.9443\n",
            "batch: 122/160 train loss: 1.1994 train acc 0.9444\n",
            "batch: 123/160 train loss: 1.1989 train acc 0.9447\n",
            "batch: 124/160 train loss: 1.1986 train acc 0.9444\n",
            "batch: 125/160 train loss: 1.1991 train acc 0.9443\n",
            "batch: 126/160 train loss: 1.1991 train acc 0.9446\n",
            "batch: 127/160 train loss: 1.1983 train acc 0.9446\n",
            "batch: 128/160 train loss: 1.1987 train acc 0.9441\n",
            "batch: 129/160 train loss: 1.1986 train acc 0.9440\n",
            "batch: 130/160 train loss: 1.1999 train acc 0.9439\n",
            "batch: 131/160 train loss: 1.1996 train acc 0.9438\n",
            "batch: 132/160 train loss: 1.1996 train acc 0.9439\n",
            "batch: 133/160 train loss: 1.1998 train acc 0.9440\n",
            "batch: 134/160 train loss: 1.1997 train acc 0.9438\n",
            "batch: 135/160 train loss: 1.1997 train acc 0.9440\n",
            "batch: 136/160 train loss: 1.1995 train acc 0.9439\n",
            "batch: 137/160 train loss: 1.1980 train acc 0.9443\n",
            "batch: 138/160 train loss: 1.1975 train acc 0.9446\n",
            "batch: 139/160 train loss: 1.1972 train acc 0.9447\n",
            "batch: 140/160 train loss: 1.1974 train acc 0.9446\n",
            "batch: 141/160 train loss: 1.1971 train acc 0.9449\n",
            "batch: 142/160 train loss: 1.1975 train acc 0.9450\n",
            "batch: 143/160 train loss: 1.1985 train acc 0.9451\n",
            "batch: 144/160 train loss: 1.1988 train acc 0.9449\n",
            "batch: 145/160 train loss: 1.1988 train acc 0.9449\n",
            "batch: 146/160 train loss: 1.1989 train acc 0.9449\n",
            "batch: 147/160 train loss: 1.1984 train acc 0.9450\n",
            "batch: 148/160 train loss: 1.1983 train acc 0.9450\n",
            "batch: 149/160 train loss: 1.1983 train acc 0.9449\n",
            "batch: 150/160 train loss: 1.1993 train acc 0.9447\n",
            "batch: 151/160 train loss: 1.1994 train acc 0.9444\n",
            "batch: 152/160 train loss: 1.1993 train acc 0.9444\n",
            "batch: 153/160 train loss: 1.1983 train acc 0.9448\n",
            "batch: 154/160 train loss: 1.1977 train acc 0.9450\n",
            "batch: 155/160 train loss: 1.1973 train acc 0.9452\n",
            "batch: 156/160 train loss: 1.1984 train acc 0.9449\n",
            "batch: 157/160 train loss: 1.1988 train acc 0.9449\n",
            "batch: 158/160 train loss: 1.1987 train acc 0.9451\n",
            "batch: 159/160 train loss: 1.1992 train acc 0.9451\n",
            "batch: 160/160 train loss: 1.1995 train acc 0.9450\n",
            "\n",
            "Epoch 11 train loss: 1.1995 test loss 2.4907 train acc 0.9450 test acc 0.5883\n",
            "Epoch 12/25\n",
            "batch: 1/160 train loss: 1.1200 train acc 0.9375\n",
            "batch: 2/160 train loss: 1.1647 train acc 0.9531\n",
            "batch: 3/160 train loss: 1.1356 train acc 0.9531\n",
            "batch: 4/160 train loss: 1.1272 train acc 0.9414\n",
            "batch: 5/160 train loss: 1.1170 train acc 0.9437\n",
            "batch: 6/160 train loss: 1.1004 train acc 0.9479\n",
            "batch: 7/160 train loss: 1.1020 train acc 0.9464\n",
            "batch: 8/160 train loss: 1.0988 train acc 0.9492\n",
            "batch: 9/160 train loss: 1.1057 train acc 0.9514\n",
            "batch: 10/160 train loss: 1.1183 train acc 0.9469\n",
            "batch: 11/160 train loss: 1.1205 train acc 0.9474\n",
            "batch: 12/160 train loss: 1.1339 train acc 0.9427\n",
            "batch: 13/160 train loss: 1.1279 train acc 0.9423\n",
            "batch: 14/160 train loss: 1.1308 train acc 0.9408\n",
            "batch: 15/160 train loss: 1.1399 train acc 0.9365\n",
            "batch: 16/160 train loss: 1.1367 train acc 0.9375\n",
            "batch: 17/160 train loss: 1.1330 train acc 0.9384\n",
            "batch: 18/160 train loss: 1.1330 train acc 0.9384\n",
            "batch: 19/160 train loss: 1.1342 train acc 0.9391\n",
            "batch: 20/160 train loss: 1.1302 train acc 0.9414\n",
            "batch: 21/160 train loss: 1.1248 train acc 0.9435\n",
            "batch: 22/160 train loss: 1.1225 train acc 0.9439\n",
            "batch: 23/160 train loss: 1.1251 train acc 0.9443\n",
            "batch: 24/160 train loss: 1.1296 train acc 0.9447\n",
            "batch: 25/160 train loss: 1.1280 train acc 0.9450\n",
            "batch: 26/160 train loss: 1.1335 train acc 0.9447\n",
            "batch: 27/160 train loss: 1.1327 train acc 0.9450\n",
            "batch: 28/160 train loss: 1.1341 train acc 0.9459\n",
            "batch: 29/160 train loss: 1.1317 train acc 0.9472\n",
            "batch: 30/160 train loss: 1.1359 train acc 0.9448\n",
            "batch: 31/160 train loss: 1.1360 train acc 0.9456\n",
            "batch: 32/160 train loss: 1.1359 train acc 0.9453\n",
            "batch: 33/160 train loss: 1.1365 train acc 0.9455\n",
            "batch: 34/160 train loss: 1.1350 train acc 0.9458\n",
            "batch: 35/160 train loss: 1.1333 train acc 0.9460\n",
            "batch: 36/160 train loss: 1.1294 train acc 0.9466\n",
            "batch: 37/160 train loss: 1.1297 train acc 0.9476\n",
            "batch: 38/160 train loss: 1.1276 train acc 0.9470\n",
            "batch: 39/160 train loss: 1.1246 train acc 0.9483\n",
            "batch: 40/160 train loss: 1.1237 train acc 0.9492\n",
            "batch: 41/160 train loss: 1.1230 train acc 0.9497\n",
            "batch: 42/160 train loss: 1.1250 train acc 0.9487\n",
            "batch: 43/160 train loss: 1.1254 train acc 0.9491\n",
            "batch: 44/160 train loss: 1.1239 train acc 0.9499\n",
            "batch: 45/160 train loss: 1.1230 train acc 0.9500\n",
            "batch: 46/160 train loss: 1.1231 train acc 0.9490\n",
            "batch: 47/160 train loss: 1.1199 train acc 0.9491\n",
            "batch: 48/160 train loss: 1.1194 train acc 0.9495\n",
            "batch: 49/160 train loss: 1.1199 train acc 0.9499\n",
            "batch: 50/160 train loss: 1.1184 train acc 0.9500\n",
            "batch: 51/160 train loss: 1.1202 train acc 0.9491\n",
            "batch: 52/160 train loss: 1.1191 train acc 0.9498\n",
            "batch: 53/160 train loss: 1.1209 train acc 0.9490\n",
            "batch: 54/160 train loss: 1.1209 train acc 0.9494\n",
            "batch: 55/160 train loss: 1.1218 train acc 0.9491\n",
            "batch: 56/160 train loss: 1.1230 train acc 0.9489\n",
            "batch: 57/160 train loss: 1.1242 train acc 0.9487\n",
            "batch: 58/160 train loss: 1.1238 train acc 0.9485\n",
            "batch: 59/160 train loss: 1.1229 train acc 0.9486\n",
            "batch: 60/160 train loss: 1.1226 train acc 0.9487\n",
            "batch: 61/160 train loss: 1.1222 train acc 0.9485\n",
            "batch: 62/160 train loss: 1.1210 train acc 0.9493\n",
            "batch: 63/160 train loss: 1.1211 train acc 0.9494\n",
            "batch: 64/160 train loss: 1.1192 train acc 0.9500\n",
            "batch: 65/160 train loss: 1.1195 train acc 0.9507\n",
            "batch: 66/160 train loss: 1.1184 train acc 0.9510\n",
            "batch: 67/160 train loss: 1.1176 train acc 0.9510\n",
            "batch: 68/160 train loss: 1.1181 train acc 0.9515\n",
            "batch: 69/160 train loss: 1.1185 train acc 0.9520\n",
            "batch: 70/160 train loss: 1.1193 train acc 0.9516\n",
            "batch: 71/160 train loss: 1.1198 train acc 0.9518\n",
            "batch: 72/160 train loss: 1.1206 train acc 0.9520\n",
            "batch: 73/160 train loss: 1.1219 train acc 0.9521\n",
            "batch: 74/160 train loss: 1.1218 train acc 0.9521\n",
            "batch: 75/160 train loss: 1.1232 train acc 0.9521\n",
            "batch: 76/160 train loss: 1.1226 train acc 0.9517\n",
            "batch: 77/160 train loss: 1.1229 train acc 0.9509\n",
            "batch: 78/160 train loss: 1.1251 train acc 0.9509\n",
            "batch: 79/160 train loss: 1.1249 train acc 0.9515\n",
            "batch: 80/160 train loss: 1.1259 train acc 0.9518\n",
            "batch: 81/160 train loss: 1.1266 train acc 0.9518\n",
            "batch: 82/160 train loss: 1.1266 train acc 0.9518\n",
            "batch: 83/160 train loss: 1.1252 train acc 0.9522\n",
            "batch: 84/160 train loss: 1.1247 train acc 0.9524\n",
            "batch: 85/160 train loss: 1.1244 train acc 0.9524\n",
            "batch: 86/160 train loss: 1.1253 train acc 0.9528\n",
            "batch: 87/160 train loss: 1.1252 train acc 0.9524\n",
            "batch: 88/160 train loss: 1.1259 train acc 0.9519\n",
            "batch: 89/160 train loss: 1.1270 train acc 0.9515\n",
            "batch: 90/160 train loss: 1.1279 train acc 0.9512\n",
            "batch: 91/160 train loss: 1.1273 train acc 0.9516\n",
            "batch: 92/160 train loss: 1.1274 train acc 0.9513\n",
            "batch: 93/160 train loss: 1.1275 train acc 0.9513\n",
            "batch: 94/160 train loss: 1.1275 train acc 0.9518\n",
            "batch: 95/160 train loss: 1.1287 train acc 0.9512\n",
            "batch: 96/160 train loss: 1.1292 train acc 0.9515\n",
            "batch: 97/160 train loss: 1.1287 train acc 0.9515\n",
            "batch: 98/160 train loss: 1.1292 train acc 0.9515\n",
            "batch: 99/160 train loss: 1.1289 train acc 0.9517\n",
            "batch: 100/160 train loss: 1.1295 train acc 0.9519\n",
            "batch: 101/160 train loss: 1.1299 train acc 0.9517\n",
            "batch: 102/160 train loss: 1.1308 train acc 0.9517\n",
            "batch: 103/160 train loss: 1.1315 train acc 0.9518\n",
            "batch: 104/160 train loss: 1.1314 train acc 0.9518\n",
            "batch: 105/160 train loss: 1.1319 train acc 0.9518\n",
            "batch: 106/160 train loss: 1.1327 train acc 0.9518\n",
            "batch: 107/160 train loss: 1.1322 train acc 0.9520\n",
            "batch: 108/160 train loss: 1.1321 train acc 0.9521\n",
            "batch: 109/160 train loss: 1.1326 train acc 0.9521\n",
            "batch: 110/160 train loss: 1.1318 train acc 0.9524\n",
            "batch: 111/160 train loss: 1.1312 train acc 0.9527\n",
            "batch: 112/160 train loss: 1.1313 train acc 0.9530\n",
            "batch: 113/160 train loss: 1.1316 train acc 0.9530\n",
            "batch: 114/160 train loss: 1.1324 train acc 0.9527\n",
            "batch: 115/160 train loss: 1.1337 train acc 0.9522\n",
            "batch: 116/160 train loss: 1.1341 train acc 0.9522\n",
            "batch: 117/160 train loss: 1.1334 train acc 0.9525\n",
            "batch: 118/160 train loss: 1.1337 train acc 0.9525\n",
            "batch: 119/160 train loss: 1.1340 train acc 0.9527\n",
            "batch: 120/160 train loss: 1.1339 train acc 0.9525\n",
            "batch: 121/160 train loss: 1.1339 train acc 0.9524\n",
            "batch: 122/160 train loss: 1.1334 train acc 0.9526\n",
            "batch: 123/160 train loss: 1.1329 train acc 0.9527\n",
            "batch: 124/160 train loss: 1.1328 train acc 0.9525\n",
            "batch: 125/160 train loss: 1.1329 train acc 0.9526\n",
            "batch: 126/160 train loss: 1.1318 train acc 0.9529\n",
            "batch: 127/160 train loss: 1.1320 train acc 0.9528\n",
            "batch: 128/160 train loss: 1.1323 train acc 0.9529\n",
            "batch: 129/160 train loss: 1.1329 train acc 0.9526\n",
            "batch: 130/160 train loss: 1.1337 train acc 0.9524\n",
            "batch: 131/160 train loss: 1.1334 train acc 0.9526\n",
            "batch: 132/160 train loss: 1.1336 train acc 0.9527\n",
            "batch: 133/160 train loss: 1.1338 train acc 0.9523\n",
            "batch: 134/160 train loss: 1.1344 train acc 0.9523\n",
            "batch: 135/160 train loss: 1.1339 train acc 0.9524\n",
            "batch: 136/160 train loss: 1.1337 train acc 0.9524\n",
            "batch: 137/160 train loss: 1.1344 train acc 0.9524\n",
            "batch: 138/160 train loss: 1.1339 train acc 0.9527\n",
            "batch: 139/160 train loss: 1.1344 train acc 0.9528\n",
            "batch: 140/160 train loss: 1.1348 train acc 0.9529\n",
            "batch: 141/160 train loss: 1.1351 train acc 0.9527\n",
            "batch: 142/160 train loss: 1.1347 train acc 0.9529\n",
            "batch: 143/160 train loss: 1.1342 train acc 0.9531\n",
            "batch: 144/160 train loss: 1.1349 train acc 0.9532\n",
            "batch: 145/160 train loss: 1.1366 train acc 0.9529\n",
            "batch: 146/160 train loss: 1.1364 train acc 0.9529\n",
            "batch: 147/160 train loss: 1.1366 train acc 0.9531\n",
            "batch: 148/160 train loss: 1.1363 train acc 0.9532\n",
            "batch: 149/160 train loss: 1.1365 train acc 0.9533\n",
            "batch: 150/160 train loss: 1.1371 train acc 0.9533\n",
            "batch: 151/160 train loss: 1.1372 train acc 0.9533\n",
            "batch: 152/160 train loss: 1.1372 train acc 0.9533\n",
            "batch: 153/160 train loss: 1.1379 train acc 0.9528\n",
            "batch: 154/160 train loss: 1.1383 train acc 0.9529\n",
            "batch: 155/160 train loss: 1.1394 train acc 0.9528\n",
            "batch: 156/160 train loss: 1.1389 train acc 0.9530\n",
            "batch: 157/160 train loss: 1.1388 train acc 0.9531\n",
            "batch: 158/160 train loss: 1.1385 train acc 0.9532\n",
            "batch: 159/160 train loss: 1.1393 train acc 0.9530\n",
            "batch: 160/160 train loss: 1.1403 train acc 0.9530\n",
            "\n",
            "Epoch 12 train loss: 1.1403 test loss 2.4777 train acc 0.9530 test acc 0.5842\n",
            "Epoch 13/25\n",
            "batch: 1/160 train loss: 1.1371 train acc 0.9844\n",
            "batch: 2/160 train loss: 1.0829 train acc 0.9922\n",
            "batch: 3/160 train loss: 1.0356 train acc 0.9948\n",
            "batch: 4/160 train loss: 1.0390 train acc 0.9844\n",
            "batch: 5/160 train loss: 1.0615 train acc 0.9781\n",
            "batch: 6/160 train loss: 1.0637 train acc 0.9766\n",
            "batch: 7/160 train loss: 1.0574 train acc 0.9732\n",
            "batch: 8/160 train loss: 1.0580 train acc 0.9707\n",
            "batch: 9/160 train loss: 1.0554 train acc 0.9705\n",
            "batch: 10/160 train loss: 1.0530 train acc 0.9688\n",
            "batch: 11/160 train loss: 1.0553 train acc 0.9688\n",
            "batch: 12/160 train loss: 1.0588 train acc 0.9674\n",
            "batch: 13/160 train loss: 1.0518 train acc 0.9688\n",
            "batch: 14/160 train loss: 1.0577 train acc 0.9665\n",
            "batch: 15/160 train loss: 1.0688 train acc 0.9635\n",
            "batch: 16/160 train loss: 1.0740 train acc 0.9619\n",
            "batch: 17/160 train loss: 1.0703 train acc 0.9614\n",
            "batch: 18/160 train loss: 1.0720 train acc 0.9618\n",
            "batch: 19/160 train loss: 1.0716 train acc 0.9622\n",
            "batch: 20/160 train loss: 1.0664 train acc 0.9641\n",
            "batch: 21/160 train loss: 1.0662 train acc 0.9635\n",
            "batch: 22/160 train loss: 1.0766 train acc 0.9616\n",
            "batch: 23/160 train loss: 1.0770 train acc 0.9613\n",
            "batch: 24/160 train loss: 1.0778 train acc 0.9603\n",
            "batch: 25/160 train loss: 1.0787 train acc 0.9600\n",
            "batch: 26/160 train loss: 1.0778 train acc 0.9615\n",
            "batch: 27/160 train loss: 1.0806 train acc 0.9606\n",
            "batch: 28/160 train loss: 1.0813 train acc 0.9615\n",
            "batch: 29/160 train loss: 1.0836 train acc 0.9617\n",
            "batch: 30/160 train loss: 1.0839 train acc 0.9615\n",
            "batch: 31/160 train loss: 1.0833 train acc 0.9612\n",
            "batch: 32/160 train loss: 1.0830 train acc 0.9614\n",
            "batch: 33/160 train loss: 1.0842 train acc 0.9612\n",
            "batch: 34/160 train loss: 1.0832 train acc 0.9609\n",
            "batch: 35/160 train loss: 1.0821 train acc 0.9612\n",
            "batch: 36/160 train loss: 1.0836 train acc 0.9592\n",
            "batch: 37/160 train loss: 1.0833 train acc 0.9595\n",
            "batch: 38/160 train loss: 1.0806 train acc 0.9597\n",
            "batch: 39/160 train loss: 1.0844 train acc 0.9587\n",
            "batch: 40/160 train loss: 1.0849 train acc 0.9590\n",
            "batch: 41/160 train loss: 1.0856 train acc 0.9592\n",
            "batch: 42/160 train loss: 1.0834 train acc 0.9598\n",
            "batch: 43/160 train loss: 1.0825 train acc 0.9600\n",
            "batch: 44/160 train loss: 1.0851 train acc 0.9599\n",
            "batch: 45/160 train loss: 1.0825 train acc 0.9604\n",
            "batch: 46/160 train loss: 1.0821 train acc 0.9606\n",
            "batch: 47/160 train loss: 1.0804 train acc 0.9611\n",
            "batch: 48/160 train loss: 1.0777 train acc 0.9616\n",
            "batch: 49/160 train loss: 1.0804 train acc 0.9611\n",
            "batch: 50/160 train loss: 1.0780 train acc 0.9619\n",
            "batch: 51/160 train loss: 1.0769 train acc 0.9617\n",
            "batch: 52/160 train loss: 1.0767 train acc 0.9618\n",
            "batch: 53/160 train loss: 1.0766 train acc 0.9611\n",
            "batch: 54/160 train loss: 1.0773 train acc 0.9612\n",
            "batch: 55/160 train loss: 1.0784 train acc 0.9608\n",
            "batch: 56/160 train loss: 1.0776 train acc 0.9609\n",
            "batch: 57/160 train loss: 1.0779 train acc 0.9603\n",
            "batch: 58/160 train loss: 1.0773 train acc 0.9604\n",
            "batch: 59/160 train loss: 1.0775 train acc 0.9608\n",
            "batch: 60/160 train loss: 1.0775 train acc 0.9604\n",
            "batch: 61/160 train loss: 1.0784 train acc 0.9603\n",
            "batch: 62/160 train loss: 1.0781 train acc 0.9602\n",
            "batch: 63/160 train loss: 1.0792 train acc 0.9598\n",
            "batch: 64/160 train loss: 1.0775 train acc 0.9597\n",
            "batch: 65/160 train loss: 1.0763 train acc 0.9596\n",
            "batch: 66/160 train loss: 1.0776 train acc 0.9593\n",
            "batch: 67/160 train loss: 1.0770 train acc 0.9594\n",
            "batch: 68/160 train loss: 1.0764 train acc 0.9596\n",
            "batch: 69/160 train loss: 1.0753 train acc 0.9599\n",
            "batch: 70/160 train loss: 1.0760 train acc 0.9596\n",
            "batch: 71/160 train loss: 1.0776 train acc 0.9595\n",
            "batch: 72/160 train loss: 1.0782 train acc 0.9592\n",
            "batch: 73/160 train loss: 1.0798 train acc 0.9598\n",
            "batch: 74/160 train loss: 1.0795 train acc 0.9601\n",
            "batch: 75/160 train loss: 1.0791 train acc 0.9606\n",
            "batch: 76/160 train loss: 1.0788 train acc 0.9605\n",
            "batch: 77/160 train loss: 1.0780 train acc 0.9610\n",
            "batch: 78/160 train loss: 1.0780 train acc 0.9609\n",
            "batch: 79/160 train loss: 1.0769 train acc 0.9612\n",
            "batch: 80/160 train loss: 1.0784 train acc 0.9613\n",
            "batch: 81/160 train loss: 1.0781 train acc 0.9618\n",
            "batch: 82/160 train loss: 1.0784 train acc 0.9621\n",
            "batch: 83/160 train loss: 1.0780 train acc 0.9623\n",
            "batch: 84/160 train loss: 1.0779 train acc 0.9622\n",
            "batch: 85/160 train loss: 1.0769 train acc 0.9623\n",
            "batch: 86/160 train loss: 1.0776 train acc 0.9624\n",
            "batch: 87/160 train loss: 1.0769 train acc 0.9623\n",
            "batch: 88/160 train loss: 1.0764 train acc 0.9625\n",
            "batch: 89/160 train loss: 1.0783 train acc 0.9623\n",
            "batch: 90/160 train loss: 1.0767 train acc 0.9627\n",
            "batch: 91/160 train loss: 1.0760 train acc 0.9627\n",
            "batch: 92/160 train loss: 1.0769 train acc 0.9625\n",
            "batch: 93/160 train loss: 1.0754 train acc 0.9627\n",
            "batch: 94/160 train loss: 1.0744 train acc 0.9629\n",
            "batch: 95/160 train loss: 1.0746 train acc 0.9628\n",
            "batch: 96/160 train loss: 1.0747 train acc 0.9627\n",
            "batch: 97/160 train loss: 1.0745 train acc 0.9628\n",
            "batch: 98/160 train loss: 1.0745 train acc 0.9627\n",
            "batch: 99/160 train loss: 1.0748 train acc 0.9624\n",
            "batch: 100/160 train loss: 1.0752 train acc 0.9616\n",
            "batch: 101/160 train loss: 1.0743 train acc 0.9618\n",
            "batch: 102/160 train loss: 1.0752 train acc 0.9619\n",
            "batch: 103/160 train loss: 1.0744 train acc 0.9619\n",
            "batch: 104/160 train loss: 1.0742 train acc 0.9618\n",
            "batch: 105/160 train loss: 1.0739 train acc 0.9621\n",
            "batch: 106/160 train loss: 1.0739 train acc 0.9618\n",
            "batch: 107/160 train loss: 1.0733 train acc 0.9619\n",
            "batch: 108/160 train loss: 1.0738 train acc 0.9620\n",
            "batch: 109/160 train loss: 1.0765 train acc 0.9614\n",
            "batch: 110/160 train loss: 1.0770 train acc 0.9614\n",
            "batch: 111/160 train loss: 1.0769 train acc 0.9617\n",
            "batch: 112/160 train loss: 1.0781 train acc 0.9614\n",
            "batch: 113/160 train loss: 1.0775 train acc 0.9616\n",
            "batch: 114/160 train loss: 1.0771 train acc 0.9616\n",
            "batch: 115/160 train loss: 1.0777 train acc 0.9618\n",
            "batch: 116/160 train loss: 1.0777 train acc 0.9616\n",
            "batch: 117/160 train loss: 1.0772 train acc 0.9619\n",
            "batch: 118/160 train loss: 1.0777 train acc 0.9620\n",
            "batch: 119/160 train loss: 1.0773 train acc 0.9619\n",
            "batch: 120/160 train loss: 1.0777 train acc 0.9620\n",
            "batch: 121/160 train loss: 1.0771 train acc 0.9623\n",
            "batch: 122/160 train loss: 1.0772 train acc 0.9625\n",
            "batch: 123/160 train loss: 1.0768 train acc 0.9625\n",
            "batch: 124/160 train loss: 1.0773 train acc 0.9626\n",
            "batch: 125/160 train loss: 1.0785 train acc 0.9625\n",
            "batch: 126/160 train loss: 1.0778 train acc 0.9625\n",
            "batch: 127/160 train loss: 1.0778 train acc 0.9626\n",
            "batch: 128/160 train loss: 1.0777 train acc 0.9628\n",
            "batch: 129/160 train loss: 1.0775 train acc 0.9631\n",
            "batch: 130/160 train loss: 1.0769 train acc 0.9632\n",
            "batch: 131/160 train loss: 1.0773 train acc 0.9635\n",
            "batch: 132/160 train loss: 1.0771 train acc 0.9634\n",
            "batch: 133/160 train loss: 1.0770 train acc 0.9635\n",
            "batch: 134/160 train loss: 1.0771 train acc 0.9635\n",
            "batch: 135/160 train loss: 1.0776 train acc 0.9634\n",
            "batch: 136/160 train loss: 1.0777 train acc 0.9636\n",
            "batch: 137/160 train loss: 1.0789 train acc 0.9634\n",
            "batch: 138/160 train loss: 1.0789 train acc 0.9633\n",
            "batch: 139/160 train loss: 1.0784 train acc 0.9635\n",
            "batch: 140/160 train loss: 1.0779 train acc 0.9634\n",
            "batch: 141/160 train loss: 1.0789 train acc 0.9632\n",
            "batch: 142/160 train loss: 1.0786 train acc 0.9631\n",
            "batch: 143/160 train loss: 1.0779 train acc 0.9631\n",
            "batch: 144/160 train loss: 1.0784 train acc 0.9628\n",
            "batch: 145/160 train loss: 1.0779 train acc 0.9628\n",
            "batch: 146/160 train loss: 1.0778 train acc 0.9630\n",
            "batch: 147/160 train loss: 1.0778 train acc 0.9631\n",
            "batch: 148/160 train loss: 1.0773 train acc 0.9633\n",
            "batch: 149/160 train loss: 1.0778 train acc 0.9633\n",
            "batch: 150/160 train loss: 1.0786 train acc 0.9630\n",
            "batch: 151/160 train loss: 1.0794 train acc 0.9626\n",
            "batch: 152/160 train loss: 1.0789 train acc 0.9628\n",
            "batch: 153/160 train loss: 1.0798 train acc 0.9626\n",
            "batch: 154/160 train loss: 1.0792 train acc 0.9628\n",
            "batch: 155/160 train loss: 1.0799 train acc 0.9628\n",
            "batch: 156/160 train loss: 1.0798 train acc 0.9625\n",
            "batch: 157/160 train loss: 1.0794 train acc 0.9626\n",
            "batch: 158/160 train loss: 1.0790 train acc 0.9627\n",
            "batch: 159/160 train loss: 1.0791 train acc 0.9625\n",
            "batch: 160/160 train loss: 1.0790 train acc 0.9625\n",
            "\n",
            "Epoch 13 train loss: 1.0790 test loss 2.4460 train acc 0.9625 test acc 0.5933\n",
            "Epoch 14/25\n",
            "batch: 1/160 train loss: 0.9534 train acc 1.0000\n",
            "batch: 2/160 train loss: 0.9355 train acc 0.9922\n",
            "batch: 3/160 train loss: 0.9645 train acc 0.9896\n",
            "batch: 4/160 train loss: 0.9847 train acc 0.9844\n",
            "batch: 5/160 train loss: 0.9807 train acc 0.9844\n",
            "batch: 6/160 train loss: 1.0093 train acc 0.9766\n",
            "batch: 7/160 train loss: 0.9996 train acc 0.9754\n",
            "batch: 8/160 train loss: 1.0113 train acc 0.9707\n",
            "batch: 9/160 train loss: 1.0104 train acc 0.9722\n",
            "batch: 10/160 train loss: 1.0131 train acc 0.9703\n",
            "batch: 11/160 train loss: 1.0194 train acc 0.9659\n",
            "batch: 12/160 train loss: 1.0168 train acc 0.9674\n",
            "batch: 13/160 train loss: 1.0179 train acc 0.9688\n",
            "batch: 14/160 train loss: 1.0180 train acc 0.9699\n",
            "batch: 15/160 train loss: 1.0260 train acc 0.9719\n",
            "batch: 16/160 train loss: 1.0307 train acc 0.9717\n",
            "batch: 17/160 train loss: 1.0335 train acc 0.9697\n",
            "batch: 18/160 train loss: 1.0363 train acc 0.9705\n",
            "batch: 19/160 train loss: 1.0308 train acc 0.9704\n",
            "batch: 20/160 train loss: 1.0322 train acc 0.9711\n",
            "batch: 21/160 train loss: 1.0308 train acc 0.9702\n",
            "batch: 22/160 train loss: 1.0254 train acc 0.9716\n",
            "batch: 23/160 train loss: 1.0282 train acc 0.9715\n",
            "batch: 24/160 train loss: 1.0250 train acc 0.9727\n",
            "batch: 25/160 train loss: 1.0308 train acc 0.9725\n",
            "batch: 26/160 train loss: 1.0286 train acc 0.9730\n",
            "batch: 27/160 train loss: 1.0280 train acc 0.9740\n",
            "batch: 28/160 train loss: 1.0277 train acc 0.9743\n",
            "batch: 29/160 train loss: 1.0295 train acc 0.9741\n",
            "batch: 30/160 train loss: 1.0300 train acc 0.9740\n",
            "batch: 31/160 train loss: 1.0317 train acc 0.9733\n",
            "batch: 32/160 train loss: 1.0338 train acc 0.9731\n",
            "batch: 33/160 train loss: 1.0319 train acc 0.9740\n",
            "batch: 34/160 train loss: 1.0310 train acc 0.9738\n",
            "batch: 35/160 train loss: 1.0317 train acc 0.9732\n",
            "batch: 36/160 train loss: 1.0313 train acc 0.9731\n",
            "batch: 37/160 train loss: 1.0305 train acc 0.9734\n",
            "batch: 38/160 train loss: 1.0297 train acc 0.9737\n",
            "batch: 39/160 train loss: 1.0281 train acc 0.9740\n",
            "batch: 40/160 train loss: 1.0288 train acc 0.9734\n",
            "batch: 41/160 train loss: 1.0272 train acc 0.9733\n",
            "batch: 42/160 train loss: 1.0272 train acc 0.9736\n",
            "batch: 43/160 train loss: 1.0285 train acc 0.9731\n",
            "batch: 44/160 train loss: 1.0313 train acc 0.9719\n",
            "batch: 45/160 train loss: 1.0320 train acc 0.9719\n",
            "batch: 46/160 train loss: 1.0335 train acc 0.9718\n",
            "batch: 47/160 train loss: 1.0324 train acc 0.9721\n",
            "batch: 48/160 train loss: 1.0340 train acc 0.9723\n",
            "batch: 49/160 train loss: 1.0349 train acc 0.9726\n",
            "batch: 50/160 train loss: 1.0355 train acc 0.9722\n",
            "batch: 51/160 train loss: 1.0343 train acc 0.9721\n",
            "batch: 52/160 train loss: 1.0340 train acc 0.9724\n",
            "batch: 53/160 train loss: 1.0342 train acc 0.9720\n",
            "batch: 54/160 train loss: 1.0341 train acc 0.9719\n",
            "batch: 55/160 train loss: 1.0346 train acc 0.9713\n",
            "batch: 56/160 train loss: 1.0351 train acc 0.9713\n",
            "batch: 57/160 train loss: 1.0354 train acc 0.9718\n",
            "batch: 58/160 train loss: 1.0343 train acc 0.9717\n",
            "batch: 59/160 train loss: 1.0350 train acc 0.9714\n",
            "batch: 60/160 train loss: 1.0351 train acc 0.9714\n",
            "batch: 61/160 train loss: 1.0334 train acc 0.9716\n",
            "batch: 62/160 train loss: 1.0332 train acc 0.9715\n",
            "batch: 63/160 train loss: 1.0347 train acc 0.9715\n",
            "batch: 64/160 train loss: 1.0344 train acc 0.9714\n",
            "batch: 65/160 train loss: 1.0380 train acc 0.9707\n",
            "batch: 66/160 train loss: 1.0383 train acc 0.9709\n",
            "batch: 67/160 train loss: 1.0381 train acc 0.9711\n",
            "batch: 68/160 train loss: 1.0362 train acc 0.9715\n",
            "batch: 69/160 train loss: 1.0366 train acc 0.9715\n",
            "batch: 70/160 train loss: 1.0375 train acc 0.9714\n",
            "batch: 71/160 train loss: 1.0366 train acc 0.9718\n",
            "batch: 72/160 train loss: 1.0356 train acc 0.9722\n",
            "batch: 73/160 train loss: 1.0356 train acc 0.9720\n",
            "batch: 74/160 train loss: 1.0374 train acc 0.9713\n",
            "batch: 75/160 train loss: 1.0363 train acc 0.9715\n",
            "batch: 76/160 train loss: 1.0354 train acc 0.9714\n",
            "batch: 77/160 train loss: 1.0344 train acc 0.9718\n",
            "batch: 78/160 train loss: 1.0342 train acc 0.9716\n",
            "batch: 79/160 train loss: 1.0367 train acc 0.9709\n",
            "batch: 80/160 train loss: 1.0366 train acc 0.9707\n",
            "batch: 81/160 train loss: 1.0371 train acc 0.9707\n",
            "batch: 82/160 train loss: 1.0374 train acc 0.9705\n",
            "batch: 83/160 train loss: 1.0379 train acc 0.9704\n",
            "batch: 84/160 train loss: 1.0378 train acc 0.9708\n",
            "batch: 85/160 train loss: 1.0376 train acc 0.9710\n",
            "batch: 86/160 train loss: 1.0373 train acc 0.9713\n",
            "batch: 87/160 train loss: 1.0377 train acc 0.9709\n",
            "batch: 88/160 train loss: 1.0379 train acc 0.9711\n",
            "batch: 89/160 train loss: 1.0382 train acc 0.9710\n",
            "batch: 90/160 train loss: 1.0382 train acc 0.9703\n",
            "batch: 91/160 train loss: 1.0394 train acc 0.9705\n",
            "batch: 92/160 train loss: 1.0415 train acc 0.9701\n",
            "batch: 93/160 train loss: 1.0414 train acc 0.9701\n",
            "batch: 94/160 train loss: 1.0400 train acc 0.9704\n",
            "batch: 95/160 train loss: 1.0408 train acc 0.9702\n",
            "batch: 96/160 train loss: 1.0407 train acc 0.9699\n",
            "batch: 97/160 train loss: 1.0409 train acc 0.9699\n",
            "batch: 98/160 train loss: 1.0404 train acc 0.9697\n",
            "batch: 99/160 train loss: 1.0397 train acc 0.9697\n",
            "batch: 100/160 train loss: 1.0394 train acc 0.9698\n",
            "batch: 101/160 train loss: 1.0392 train acc 0.9698\n",
            "batch: 102/160 train loss: 1.0381 train acc 0.9700\n",
            "batch: 103/160 train loss: 1.0383 train acc 0.9700\n",
            "batch: 104/160 train loss: 1.0386 train acc 0.9701\n",
            "batch: 105/160 train loss: 1.0385 train acc 0.9701\n",
            "batch: 106/160 train loss: 1.0381 train acc 0.9702\n",
            "batch: 107/160 train loss: 1.0384 train acc 0.9702\n",
            "batch: 108/160 train loss: 1.0386 train acc 0.9702\n",
            "batch: 109/160 train loss: 1.0379 train acc 0.9702\n",
            "batch: 110/160 train loss: 1.0375 train acc 0.9705\n",
            "batch: 111/160 train loss: 1.0369 train acc 0.9704\n",
            "batch: 112/160 train loss: 1.0369 train acc 0.9704\n",
            "batch: 113/160 train loss: 1.0373 train acc 0.9703\n",
            "batch: 114/160 train loss: 1.0374 train acc 0.9704\n",
            "batch: 115/160 train loss: 1.0389 train acc 0.9698\n",
            "batch: 116/160 train loss: 1.0393 train acc 0.9694\n",
            "batch: 117/160 train loss: 1.0389 train acc 0.9696\n",
            "batch: 118/160 train loss: 1.0390 train acc 0.9693\n",
            "batch: 119/160 train loss: 1.0394 train acc 0.9693\n",
            "batch: 120/160 train loss: 1.0393 train acc 0.9691\n",
            "batch: 121/160 train loss: 1.0392 train acc 0.9691\n",
            "batch: 122/160 train loss: 1.0400 train acc 0.9693\n",
            "batch: 123/160 train loss: 1.0404 train acc 0.9691\n",
            "batch: 124/160 train loss: 1.0402 train acc 0.9691\n",
            "batch: 125/160 train loss: 1.0407 train acc 0.9686\n",
            "batch: 126/160 train loss: 1.0405 train acc 0.9686\n",
            "batch: 127/160 train loss: 1.0417 train acc 0.9683\n",
            "batch: 128/160 train loss: 1.0429 train acc 0.9681\n",
            "batch: 129/160 train loss: 1.0430 train acc 0.9683\n",
            "batch: 130/160 train loss: 1.0439 train acc 0.9678\n",
            "batch: 131/160 train loss: 1.0437 train acc 0.9678\n",
            "batch: 132/160 train loss: 1.0440 train acc 0.9679\n",
            "batch: 133/160 train loss: 1.0435 train acc 0.9680\n",
            "batch: 134/160 train loss: 1.0429 train acc 0.9683\n",
            "batch: 135/160 train loss: 1.0425 train acc 0.9684\n",
            "batch: 136/160 train loss: 1.0421 train acc 0.9686\n",
            "batch: 137/160 train loss: 1.0422 train acc 0.9686\n",
            "batch: 138/160 train loss: 1.0422 train acc 0.9688\n",
            "batch: 139/160 train loss: 1.0418 train acc 0.9689\n",
            "batch: 140/160 train loss: 1.0418 train acc 0.9688\n",
            "batch: 141/160 train loss: 1.0416 train acc 0.9685\n",
            "batch: 142/160 train loss: 1.0419 train acc 0.9684\n",
            "batch: 143/160 train loss: 1.0414 train acc 0.9686\n",
            "batch: 144/160 train loss: 1.0419 train acc 0.9683\n",
            "batch: 145/160 train loss: 1.0422 train acc 0.9683\n",
            "batch: 146/160 train loss: 1.0417 train acc 0.9683\n",
            "batch: 147/160 train loss: 1.0421 train acc 0.9685\n",
            "batch: 148/160 train loss: 1.0427 train acc 0.9684\n",
            "batch: 149/160 train loss: 1.0423 train acc 0.9686\n",
            "batch: 150/160 train loss: 1.0422 train acc 0.9686\n",
            "batch: 151/160 train loss: 1.0422 train acc 0.9689\n",
            "batch: 152/160 train loss: 1.0417 train acc 0.9689\n",
            "batch: 153/160 train loss: 1.0419 train acc 0.9688\n",
            "batch: 154/160 train loss: 1.0416 train acc 0.9688\n",
            "batch: 155/160 train loss: 1.0426 train acc 0.9686\n",
            "batch: 156/160 train loss: 1.0418 train acc 0.9689\n",
            "batch: 157/160 train loss: 1.0416 train acc 0.9688\n",
            "batch: 158/160 train loss: 1.0417 train acc 0.9688\n",
            "batch: 159/160 train loss: 1.0432 train acc 0.9685\n",
            "batch: 160/160 train loss: 1.0440 train acc 0.9684\n",
            "\n",
            "Epoch 14 train loss: 1.0440 test loss 2.4214 train acc 0.9684 test acc 0.6000\n",
            "Epoch 15/25\n",
            "batch: 1/160 train loss: 0.9621 train acc 1.0000\n",
            "batch: 2/160 train loss: 1.0354 train acc 0.9922\n",
            "batch: 3/160 train loss: 1.0175 train acc 0.9844\n",
            "batch: 4/160 train loss: 1.0150 train acc 0.9766\n",
            "batch: 5/160 train loss: 1.0176 train acc 0.9750\n",
            "batch: 6/160 train loss: 1.0087 train acc 0.9792\n",
            "batch: 7/160 train loss: 0.9978 train acc 0.9799\n",
            "batch: 8/160 train loss: 1.0028 train acc 0.9785\n",
            "batch: 9/160 train loss: 0.9960 train acc 0.9809\n",
            "batch: 10/160 train loss: 1.0078 train acc 0.9797\n",
            "batch: 11/160 train loss: 1.0048 train acc 0.9801\n",
            "batch: 12/160 train loss: 1.0079 train acc 0.9779\n",
            "batch: 13/160 train loss: 1.0077 train acc 0.9784\n",
            "batch: 14/160 train loss: 1.0123 train acc 0.9788\n",
            "batch: 15/160 train loss: 1.0132 train acc 0.9792\n",
            "batch: 16/160 train loss: 1.0081 train acc 0.9805\n",
            "batch: 17/160 train loss: 1.0081 train acc 0.9807\n",
            "batch: 18/160 train loss: 1.0027 train acc 0.9818\n",
            "batch: 19/160 train loss: 1.0068 train acc 0.9803\n",
            "batch: 20/160 train loss: 1.0065 train acc 0.9805\n",
            "batch: 21/160 train loss: 1.0051 train acc 0.9799\n",
            "batch: 22/160 train loss: 1.0125 train acc 0.9794\n",
            "batch: 23/160 train loss: 1.0111 train acc 0.9796\n",
            "batch: 24/160 train loss: 1.0141 train acc 0.9792\n",
            "batch: 25/160 train loss: 1.0204 train acc 0.9775\n",
            "batch: 26/160 train loss: 1.0230 train acc 0.9760\n",
            "batch: 27/160 train loss: 1.0223 train acc 0.9757\n",
            "batch: 28/160 train loss: 1.0250 train acc 0.9743\n",
            "batch: 29/160 train loss: 1.0271 train acc 0.9741\n",
            "batch: 30/160 train loss: 1.0267 train acc 0.9745\n",
            "batch: 31/160 train loss: 1.0282 train acc 0.9728\n",
            "batch: 32/160 train loss: 1.0258 train acc 0.9736\n",
            "batch: 33/160 train loss: 1.0247 train acc 0.9730\n",
            "batch: 34/160 train loss: 1.0248 train acc 0.9733\n",
            "batch: 35/160 train loss: 1.0240 train acc 0.9732\n",
            "batch: 36/160 train loss: 1.0236 train acc 0.9731\n",
            "batch: 37/160 train loss: 1.0239 train acc 0.9730\n",
            "batch: 38/160 train loss: 1.0205 train acc 0.9733\n",
            "batch: 39/160 train loss: 1.0188 train acc 0.9740\n",
            "batch: 40/160 train loss: 1.0203 train acc 0.9727\n",
            "batch: 41/160 train loss: 1.0206 train acc 0.9729\n",
            "batch: 42/160 train loss: 1.0210 train acc 0.9728\n",
            "batch: 43/160 train loss: 1.0221 train acc 0.9727\n",
            "batch: 44/160 train loss: 1.0208 train acc 0.9727\n",
            "batch: 45/160 train loss: 1.0221 train acc 0.9726\n",
            "batch: 46/160 train loss: 1.0202 train acc 0.9732\n",
            "batch: 47/160 train loss: 1.0195 train acc 0.9727\n",
            "batch: 48/160 train loss: 1.0215 train acc 0.9720\n",
            "batch: 49/160 train loss: 1.0200 train acc 0.9716\n",
            "batch: 50/160 train loss: 1.0201 train acc 0.9719\n",
            "batch: 51/160 train loss: 1.0195 train acc 0.9715\n",
            "batch: 52/160 train loss: 1.0204 train acc 0.9712\n",
            "batch: 53/160 train loss: 1.0205 train acc 0.9717\n",
            "batch: 54/160 train loss: 1.0190 train acc 0.9722\n",
            "batch: 55/160 train loss: 1.0185 train acc 0.9727\n",
            "batch: 56/160 train loss: 1.0183 train acc 0.9727\n",
            "batch: 57/160 train loss: 1.0180 train acc 0.9723\n",
            "batch: 58/160 train loss: 1.0178 train acc 0.9725\n",
            "batch: 59/160 train loss: 1.0146 train acc 0.9730\n",
            "batch: 60/160 train loss: 1.0141 train acc 0.9729\n",
            "batch: 61/160 train loss: 1.0146 train acc 0.9734\n",
            "batch: 62/160 train loss: 1.0146 train acc 0.9735\n",
            "batch: 63/160 train loss: 1.0159 train acc 0.9732\n",
            "batch: 64/160 train loss: 1.0156 train acc 0.9734\n",
            "batch: 65/160 train loss: 1.0152 train acc 0.9733\n",
            "batch: 66/160 train loss: 1.0157 train acc 0.9728\n",
            "batch: 67/160 train loss: 1.0138 train acc 0.9732\n",
            "batch: 68/160 train loss: 1.0142 train acc 0.9731\n",
            "batch: 69/160 train loss: 1.0144 train acc 0.9731\n",
            "batch: 70/160 train loss: 1.0155 train acc 0.9730\n",
            "batch: 71/160 train loss: 1.0154 train acc 0.9732\n",
            "batch: 72/160 train loss: 1.0149 train acc 0.9735\n",
            "batch: 73/160 train loss: 1.0146 train acc 0.9739\n",
            "batch: 74/160 train loss: 1.0155 train acc 0.9742\n",
            "batch: 75/160 train loss: 1.0152 train acc 0.9746\n",
            "batch: 76/160 train loss: 1.0139 train acc 0.9745\n",
            "batch: 77/160 train loss: 1.0125 train acc 0.9748\n",
            "batch: 78/160 train loss: 1.0121 train acc 0.9748\n",
            "batch: 79/160 train loss: 1.0118 train acc 0.9749\n",
            "batch: 80/160 train loss: 1.0113 train acc 0.9748\n",
            "batch: 81/160 train loss: 1.0098 train acc 0.9747\n",
            "batch: 82/160 train loss: 1.0091 train acc 0.9747\n",
            "batch: 83/160 train loss: 1.0082 train acc 0.9746\n",
            "batch: 84/160 train loss: 1.0076 train acc 0.9747\n",
            "batch: 85/160 train loss: 1.0081 train acc 0.9744\n",
            "batch: 86/160 train loss: 1.0078 train acc 0.9747\n",
            "batch: 87/160 train loss: 1.0078 train acc 0.9750\n",
            "batch: 88/160 train loss: 1.0082 train acc 0.9750\n",
            "batch: 89/160 train loss: 1.0092 train acc 0.9747\n",
            "batch: 90/160 train loss: 1.0083 train acc 0.9748\n",
            "batch: 91/160 train loss: 1.0089 train acc 0.9746\n",
            "batch: 92/160 train loss: 1.0081 train acc 0.9745\n",
            "batch: 93/160 train loss: 1.0091 train acc 0.9745\n",
            "batch: 94/160 train loss: 1.0085 train acc 0.9747\n",
            "batch: 95/160 train loss: 1.0087 train acc 0.9747\n",
            "batch: 96/160 train loss: 1.0093 train acc 0.9748\n",
            "batch: 97/160 train loss: 1.0092 train acc 0.9747\n",
            "batch: 98/160 train loss: 1.0092 train acc 0.9750\n",
            "batch: 99/160 train loss: 1.0081 train acc 0.9752\n",
            "batch: 100/160 train loss: 1.0079 train acc 0.9755\n",
            "batch: 101/160 train loss: 1.0082 train acc 0.9754\n",
            "batch: 102/160 train loss: 1.0079 train acc 0.9755\n",
            "batch: 103/160 train loss: 1.0077 train acc 0.9754\n",
            "batch: 104/160 train loss: 1.0078 train acc 0.9754\n",
            "batch: 105/160 train loss: 1.0089 train acc 0.9754\n",
            "batch: 106/160 train loss: 1.0096 train acc 0.9754\n",
            "batch: 107/160 train loss: 1.0092 train acc 0.9753\n",
            "batch: 108/160 train loss: 1.0088 train acc 0.9754\n",
            "batch: 109/160 train loss: 1.0083 train acc 0.9753\n",
            "batch: 110/160 train loss: 1.0093 train acc 0.9754\n",
            "batch: 111/160 train loss: 1.0105 train acc 0.9748\n",
            "batch: 112/160 train loss: 1.0109 train acc 0.9745\n",
            "batch: 113/160 train loss: 1.0104 train acc 0.9747\n",
            "batch: 114/160 train loss: 1.0106 train acc 0.9748\n",
            "batch: 115/160 train loss: 1.0099 train acc 0.9747\n",
            "batch: 116/160 train loss: 1.0099 train acc 0.9748\n",
            "batch: 117/160 train loss: 1.0098 train acc 0.9750\n",
            "batch: 118/160 train loss: 1.0096 train acc 0.9751\n",
            "batch: 119/160 train loss: 1.0105 train acc 0.9747\n",
            "batch: 120/160 train loss: 1.0110 train acc 0.9746\n",
            "batch: 121/160 train loss: 1.0105 train acc 0.9744\n",
            "batch: 122/160 train loss: 1.0116 train acc 0.9743\n",
            "batch: 123/160 train loss: 1.0118 train acc 0.9741\n",
            "batch: 124/160 train loss: 1.0136 train acc 0.9737\n",
            "batch: 125/160 train loss: 1.0135 train acc 0.9736\n",
            "batch: 126/160 train loss: 1.0129 train acc 0.9738\n",
            "batch: 127/160 train loss: 1.0123 train acc 0.9740\n",
            "batch: 128/160 train loss: 1.0119 train acc 0.9740\n",
            "batch: 129/160 train loss: 1.0123 train acc 0.9740\n",
            "batch: 130/160 train loss: 1.0129 train acc 0.9737\n",
            "batch: 131/160 train loss: 1.0128 train acc 0.9739\n",
            "batch: 132/160 train loss: 1.0136 train acc 0.9736\n",
            "batch: 133/160 train loss: 1.0139 train acc 0.9737\n",
            "batch: 134/160 train loss: 1.0139 train acc 0.9738\n",
            "batch: 135/160 train loss: 1.0137 train acc 0.9737\n",
            "batch: 136/160 train loss: 1.0134 train acc 0.9737\n",
            "batch: 137/160 train loss: 1.0137 train acc 0.9735\n",
            "batch: 138/160 train loss: 1.0139 train acc 0.9736\n",
            "batch: 139/160 train loss: 1.0133 train acc 0.9738\n",
            "batch: 140/160 train loss: 1.0127 train acc 0.9740\n",
            "batch: 141/160 train loss: 1.0128 train acc 0.9738\n",
            "batch: 142/160 train loss: 1.0134 train acc 0.9738\n",
            "batch: 143/160 train loss: 1.0127 train acc 0.9739\n",
            "batch: 144/160 train loss: 1.0130 train acc 0.9737\n",
            "batch: 145/160 train loss: 1.0128 train acc 0.9737\n",
            "batch: 146/160 train loss: 1.0131 train acc 0.9738\n",
            "batch: 147/160 train loss: 1.0133 train acc 0.9735\n",
            "batch: 148/160 train loss: 1.0129 train acc 0.9737\n",
            "batch: 149/160 train loss: 1.0143 train acc 0.9735\n",
            "batch: 150/160 train loss: 1.0141 train acc 0.9733\n",
            "batch: 151/160 train loss: 1.0140 train acc 0.9734\n",
            "batch: 152/160 train loss: 1.0137 train acc 0.9734\n",
            "batch: 153/160 train loss: 1.0140 train acc 0.9733\n",
            "batch: 154/160 train loss: 1.0143 train acc 0.9732\n",
            "batch: 155/160 train loss: 1.0136 train acc 0.9733\n",
            "batch: 156/160 train loss: 1.0137 train acc 0.9733\n",
            "batch: 157/160 train loss: 1.0132 train acc 0.9733\n",
            "batch: 158/160 train loss: 1.0129 train acc 0.9735\n",
            "batch: 159/160 train loss: 1.0122 train acc 0.9737\n",
            "batch: 160/160 train loss: 1.0129 train acc 0.9735\n",
            "\n",
            "Epoch 15 train loss: 1.0129 test loss 2.4126 train acc 0.9735 test acc 0.6025\n",
            "Epoch 16/25\n",
            "batch: 1/160 train loss: 0.9713 train acc 0.9688\n",
            "batch: 2/160 train loss: 1.0165 train acc 0.9766\n",
            "batch: 3/160 train loss: 1.0179 train acc 0.9635\n",
            "batch: 4/160 train loss: 1.0184 train acc 0.9688\n",
            "batch: 5/160 train loss: 1.0323 train acc 0.9656\n",
            "batch: 6/160 train loss: 1.0358 train acc 0.9635\n",
            "batch: 7/160 train loss: 1.0293 train acc 0.9643\n",
            "batch: 8/160 train loss: 1.0252 train acc 0.9648\n",
            "batch: 9/160 train loss: 1.0362 train acc 0.9653\n",
            "batch: 10/160 train loss: 1.0242 train acc 0.9688\n",
            "batch: 11/160 train loss: 1.0223 train acc 0.9702\n",
            "batch: 12/160 train loss: 1.0199 train acc 0.9701\n",
            "batch: 13/160 train loss: 1.0221 train acc 0.9700\n",
            "batch: 14/160 train loss: 1.0181 train acc 0.9710\n",
            "batch: 15/160 train loss: 1.0160 train acc 0.9719\n",
            "batch: 16/160 train loss: 1.0164 train acc 0.9707\n",
            "batch: 17/160 train loss: 1.0115 train acc 0.9715\n",
            "batch: 18/160 train loss: 1.0106 train acc 0.9705\n",
            "batch: 19/160 train loss: 1.0069 train acc 0.9712\n",
            "batch: 20/160 train loss: 1.0024 train acc 0.9727\n",
            "batch: 21/160 train loss: 1.0025 train acc 0.9717\n",
            "batch: 22/160 train loss: 1.0013 train acc 0.9730\n",
            "batch: 23/160 train loss: 1.0024 train acc 0.9742\n",
            "batch: 24/160 train loss: 0.9980 train acc 0.9746\n",
            "batch: 25/160 train loss: 0.9971 train acc 0.9750\n",
            "batch: 26/160 train loss: 0.9957 train acc 0.9748\n",
            "batch: 27/160 train loss: 0.9991 train acc 0.9740\n",
            "batch: 28/160 train loss: 0.9971 train acc 0.9743\n",
            "batch: 29/160 train loss: 0.9978 train acc 0.9736\n",
            "batch: 30/160 train loss: 0.9985 train acc 0.9734\n",
            "batch: 31/160 train loss: 0.9963 train acc 0.9743\n",
            "batch: 32/160 train loss: 0.9941 train acc 0.9751\n",
            "batch: 33/160 train loss: 0.9966 train acc 0.9744\n",
            "batch: 34/160 train loss: 0.9956 train acc 0.9747\n",
            "batch: 35/160 train loss: 0.9955 train acc 0.9754\n",
            "batch: 36/160 train loss: 0.9941 train acc 0.9757\n",
            "batch: 37/160 train loss: 0.9941 train acc 0.9755\n",
            "batch: 38/160 train loss: 0.9936 train acc 0.9757\n",
            "batch: 39/160 train loss: 0.9926 train acc 0.9764\n",
            "batch: 40/160 train loss: 0.9917 train acc 0.9770\n",
            "batch: 41/160 train loss: 0.9945 train acc 0.9768\n",
            "batch: 42/160 train loss: 0.9936 train acc 0.9769\n",
            "batch: 43/160 train loss: 0.9940 train acc 0.9775\n",
            "batch: 44/160 train loss: 0.9937 train acc 0.9773\n",
            "batch: 45/160 train loss: 0.9927 train acc 0.9771\n",
            "batch: 46/160 train loss: 0.9909 train acc 0.9776\n",
            "batch: 47/160 train loss: 0.9890 train acc 0.9781\n",
            "batch: 48/160 train loss: 0.9874 train acc 0.9785\n",
            "batch: 49/160 train loss: 0.9890 train acc 0.9780\n",
            "batch: 50/160 train loss: 0.9906 train acc 0.9781\n",
            "batch: 51/160 train loss: 0.9903 train acc 0.9786\n",
            "batch: 52/160 train loss: 0.9901 train acc 0.9790\n",
            "batch: 53/160 train loss: 0.9897 train acc 0.9791\n",
            "batch: 54/160 train loss: 0.9894 train acc 0.9789\n",
            "batch: 55/160 train loss: 0.9882 train acc 0.9790\n",
            "batch: 56/160 train loss: 0.9893 train acc 0.9788\n",
            "batch: 57/160 train loss: 0.9890 train acc 0.9789\n",
            "batch: 58/160 train loss: 0.9902 train acc 0.9787\n",
            "batch: 59/160 train loss: 0.9908 train acc 0.9785\n",
            "batch: 60/160 train loss: 0.9913 train acc 0.9784\n",
            "batch: 61/160 train loss: 0.9919 train acc 0.9782\n",
            "batch: 62/160 train loss: 0.9919 train acc 0.9781\n",
            "batch: 63/160 train loss: 0.9912 train acc 0.9782\n",
            "batch: 64/160 train loss: 0.9909 train acc 0.9783\n",
            "batch: 65/160 train loss: 0.9915 train acc 0.9784\n",
            "batch: 66/160 train loss: 0.9906 train acc 0.9787\n",
            "batch: 67/160 train loss: 0.9895 train acc 0.9790\n",
            "batch: 68/160 train loss: 0.9888 train acc 0.9791\n",
            "batch: 69/160 train loss: 0.9904 train acc 0.9780\n",
            "batch: 70/160 train loss: 0.9894 train acc 0.9781\n",
            "batch: 71/160 train loss: 0.9894 train acc 0.9776\n",
            "batch: 72/160 train loss: 0.9891 train acc 0.9774\n",
            "batch: 73/160 train loss: 0.9890 train acc 0.9775\n",
            "batch: 74/160 train loss: 0.9901 train acc 0.9772\n",
            "batch: 75/160 train loss: 0.9894 train acc 0.9773\n",
            "batch: 76/160 train loss: 0.9899 train acc 0.9774\n",
            "batch: 77/160 train loss: 0.9901 train acc 0.9773\n",
            "batch: 78/160 train loss: 0.9920 train acc 0.9770\n",
            "batch: 79/160 train loss: 0.9916 train acc 0.9771\n",
            "batch: 80/160 train loss: 0.9912 train acc 0.9771\n",
            "batch: 81/160 train loss: 0.9896 train acc 0.9774\n",
            "batch: 82/160 train loss: 0.9879 train acc 0.9775\n",
            "batch: 83/160 train loss: 0.9882 train acc 0.9772\n",
            "batch: 84/160 train loss: 0.9885 train acc 0.9773\n",
            "batch: 85/160 train loss: 0.9887 train acc 0.9772\n",
            "batch: 86/160 train loss: 0.9882 train acc 0.9771\n",
            "batch: 87/160 train loss: 0.9895 train acc 0.9772\n",
            "batch: 88/160 train loss: 0.9900 train acc 0.9773\n",
            "batch: 89/160 train loss: 0.9891 train acc 0.9772\n",
            "batch: 90/160 train loss: 0.9893 train acc 0.9774\n",
            "batch: 91/160 train loss: 0.9886 train acc 0.9777\n",
            "batch: 92/160 train loss: 0.9887 train acc 0.9779\n",
            "batch: 93/160 train loss: 0.9893 train acc 0.9780\n",
            "batch: 94/160 train loss: 0.9893 train acc 0.9779\n",
            "batch: 95/160 train loss: 0.9894 train acc 0.9771\n",
            "batch: 96/160 train loss: 0.9892 train acc 0.9772\n",
            "batch: 97/160 train loss: 0.9891 train acc 0.9773\n",
            "batch: 98/160 train loss: 0.9892 train acc 0.9770\n",
            "batch: 99/160 train loss: 0.9906 train acc 0.9763\n",
            "batch: 100/160 train loss: 0.9904 train acc 0.9764\n",
            "batch: 101/160 train loss: 0.9891 train acc 0.9766\n",
            "batch: 102/160 train loss: 0.9888 train acc 0.9767\n",
            "batch: 103/160 train loss: 0.9884 train acc 0.9766\n",
            "batch: 104/160 train loss: 0.9887 train acc 0.9767\n",
            "batch: 105/160 train loss: 0.9884 train acc 0.9768\n",
            "batch: 106/160 train loss: 0.9875 train acc 0.9769\n",
            "batch: 107/160 train loss: 0.9882 train acc 0.9769\n",
            "batch: 108/160 train loss: 0.9880 train acc 0.9770\n",
            "batch: 109/160 train loss: 0.9878 train acc 0.9768\n",
            "batch: 110/160 train loss: 0.9885 train acc 0.9768\n",
            "batch: 111/160 train loss: 0.9898 train acc 0.9768\n",
            "batch: 112/160 train loss: 0.9906 train acc 0.9767\n",
            "batch: 113/160 train loss: 0.9908 train acc 0.9765\n",
            "batch: 114/160 train loss: 0.9915 train acc 0.9766\n",
            "batch: 115/160 train loss: 0.9915 train acc 0.9764\n",
            "batch: 116/160 train loss: 0.9919 train acc 0.9762\n",
            "batch: 117/160 train loss: 0.9925 train acc 0.9761\n",
            "batch: 118/160 train loss: 0.9929 train acc 0.9760\n",
            "batch: 119/160 train loss: 0.9926 train acc 0.9760\n",
            "batch: 120/160 train loss: 0.9929 train acc 0.9759\n",
            "batch: 121/160 train loss: 0.9930 train acc 0.9759\n",
            "batch: 122/160 train loss: 0.9924 train acc 0.9759\n",
            "batch: 123/160 train loss: 0.9922 train acc 0.9760\n",
            "batch: 124/160 train loss: 0.9920 train acc 0.9761\n",
            "batch: 125/160 train loss: 0.9925 train acc 0.9762\n",
            "batch: 126/160 train loss: 0.9931 train acc 0.9759\n",
            "batch: 127/160 train loss: 0.9928 train acc 0.9760\n",
            "batch: 128/160 train loss: 0.9924 train acc 0.9762\n",
            "batch: 129/160 train loss: 0.9916 train acc 0.9761\n",
            "batch: 130/160 train loss: 0.9921 train acc 0.9760\n",
            "batch: 131/160 train loss: 0.9913 train acc 0.9761\n",
            "batch: 132/160 train loss: 0.9905 train acc 0.9763\n",
            "batch: 133/160 train loss: 0.9903 train acc 0.9764\n",
            "batch: 134/160 train loss: 0.9900 train acc 0.9763\n",
            "batch: 135/160 train loss: 0.9906 train acc 0.9762\n",
            "batch: 136/160 train loss: 0.9913 train acc 0.9761\n",
            "batch: 137/160 train loss: 0.9916 train acc 0.9762\n",
            "batch: 138/160 train loss: 0.9911 train acc 0.9762\n",
            "batch: 139/160 train loss: 0.9913 train acc 0.9761\n",
            "batch: 140/160 train loss: 0.9915 train acc 0.9760\n",
            "batch: 141/160 train loss: 0.9912 train acc 0.9762\n",
            "batch: 142/160 train loss: 0.9914 train acc 0.9760\n",
            "batch: 143/160 train loss: 0.9920 train acc 0.9761\n",
            "batch: 144/160 train loss: 0.9920 train acc 0.9762\n",
            "batch: 145/160 train loss: 0.9922 train acc 0.9760\n",
            "batch: 146/160 train loss: 0.9914 train acc 0.9761\n",
            "batch: 147/160 train loss: 0.9909 train acc 0.9763\n",
            "batch: 148/160 train loss: 0.9907 train acc 0.9764\n",
            "batch: 149/160 train loss: 0.9907 train acc 0.9765\n",
            "batch: 150/160 train loss: 0.9907 train acc 0.9765\n",
            "batch: 151/160 train loss: 0.9915 train acc 0.9762\n",
            "batch: 152/160 train loss: 0.9915 train acc 0.9760\n",
            "batch: 153/160 train loss: 0.9909 train acc 0.9762\n",
            "batch: 154/160 train loss: 0.9910 train acc 0.9762\n",
            "batch: 155/160 train loss: 0.9909 train acc 0.9761\n",
            "batch: 156/160 train loss: 0.9907 train acc 0.9761\n",
            "batch: 157/160 train loss: 0.9907 train acc 0.9761\n",
            "batch: 158/160 train loss: 0.9908 train acc 0.9763\n",
            "batch: 159/160 train loss: 0.9905 train acc 0.9763\n",
            "batch: 160/160 train loss: 0.9911 train acc 0.9763\n",
            "\n",
            "Epoch 16 train loss: 0.9911 test loss 2.3950 train acc 0.9763 test acc 0.6042\n",
            "Epoch 17/25\n",
            "batch: 1/160 train loss: 0.9434 train acc 0.9688\n",
            "batch: 2/160 train loss: 0.9628 train acc 0.9766\n",
            "batch: 3/160 train loss: 0.9675 train acc 0.9844\n",
            "batch: 4/160 train loss: 0.9442 train acc 0.9883\n",
            "batch: 5/160 train loss: 0.9525 train acc 0.9875\n",
            "batch: 6/160 train loss: 0.9593 train acc 0.9870\n",
            "batch: 7/160 train loss: 0.9676 train acc 0.9821\n",
            "batch: 8/160 train loss: 0.9583 train acc 0.9824\n",
            "batch: 9/160 train loss: 0.9527 train acc 0.9844\n",
            "batch: 10/160 train loss: 0.9496 train acc 0.9859\n",
            "batch: 11/160 train loss: 0.9518 train acc 0.9872\n",
            "batch: 12/160 train loss: 0.9518 train acc 0.9870\n",
            "batch: 13/160 train loss: 0.9571 train acc 0.9868\n",
            "batch: 14/160 train loss: 0.9644 train acc 0.9844\n",
            "batch: 15/160 train loss: 0.9588 train acc 0.9844\n",
            "batch: 16/160 train loss: 0.9557 train acc 0.9844\n",
            "batch: 17/160 train loss: 0.9503 train acc 0.9835\n",
            "batch: 18/160 train loss: 0.9462 train acc 0.9844\n",
            "batch: 19/160 train loss: 0.9455 train acc 0.9836\n",
            "batch: 20/160 train loss: 0.9490 train acc 0.9820\n",
            "batch: 21/160 train loss: 0.9566 train acc 0.9814\n",
            "batch: 22/160 train loss: 0.9617 train acc 0.9801\n",
            "batch: 23/160 train loss: 0.9629 train acc 0.9789\n",
            "batch: 24/160 train loss: 0.9624 train acc 0.9798\n",
            "batch: 25/160 train loss: 0.9672 train acc 0.9794\n",
            "batch: 26/160 train loss: 0.9663 train acc 0.9802\n",
            "batch: 27/160 train loss: 0.9651 train acc 0.9809\n",
            "batch: 28/160 train loss: 0.9632 train acc 0.9816\n",
            "batch: 29/160 train loss: 0.9636 train acc 0.9817\n",
            "batch: 30/160 train loss: 0.9620 train acc 0.9818\n",
            "batch: 31/160 train loss: 0.9603 train acc 0.9819\n",
            "batch: 32/160 train loss: 0.9612 train acc 0.9810\n",
            "batch: 33/160 train loss: 0.9598 train acc 0.9806\n",
            "batch: 34/160 train loss: 0.9612 train acc 0.9807\n",
            "batch: 35/160 train loss: 0.9605 train acc 0.9808\n",
            "batch: 36/160 train loss: 0.9618 train acc 0.9813\n",
            "batch: 37/160 train loss: 0.9627 train acc 0.9810\n",
            "batch: 38/160 train loss: 0.9653 train acc 0.9803\n",
            "batch: 39/160 train loss: 0.9648 train acc 0.9808\n",
            "batch: 40/160 train loss: 0.9682 train acc 0.9805\n",
            "batch: 41/160 train loss: 0.9674 train acc 0.9806\n",
            "batch: 42/160 train loss: 0.9651 train acc 0.9810\n",
            "batch: 43/160 train loss: 0.9651 train acc 0.9811\n",
            "batch: 44/160 train loss: 0.9649 train acc 0.9808\n",
            "batch: 45/160 train loss: 0.9677 train acc 0.9809\n",
            "batch: 46/160 train loss: 0.9677 train acc 0.9810\n",
            "batch: 47/160 train loss: 0.9681 train acc 0.9811\n",
            "batch: 48/160 train loss: 0.9680 train acc 0.9811\n",
            "batch: 49/160 train loss: 0.9690 train acc 0.9812\n",
            "batch: 50/160 train loss: 0.9709 train acc 0.9809\n",
            "batch: 51/160 train loss: 0.9731 train acc 0.9804\n",
            "batch: 52/160 train loss: 0.9719 train acc 0.9802\n",
            "batch: 53/160 train loss: 0.9720 train acc 0.9805\n",
            "batch: 54/160 train loss: 0.9731 train acc 0.9800\n",
            "batch: 55/160 train loss: 0.9734 train acc 0.9801\n",
            "batch: 56/160 train loss: 0.9730 train acc 0.9799\n",
            "batch: 57/160 train loss: 0.9748 train acc 0.9800\n",
            "batch: 58/160 train loss: 0.9755 train acc 0.9801\n",
            "batch: 59/160 train loss: 0.9745 train acc 0.9804\n",
            "batch: 60/160 train loss: 0.9753 train acc 0.9805\n",
            "batch: 61/160 train loss: 0.9742 train acc 0.9808\n",
            "batch: 62/160 train loss: 0.9740 train acc 0.9808\n",
            "batch: 63/160 train loss: 0.9748 train acc 0.9807\n",
            "batch: 64/160 train loss: 0.9744 train acc 0.9810\n",
            "batch: 65/160 train loss: 0.9743 train acc 0.9808\n",
            "batch: 66/160 train loss: 0.9747 train acc 0.9806\n",
            "batch: 67/160 train loss: 0.9748 train acc 0.9806\n",
            "batch: 68/160 train loss: 0.9736 train acc 0.9809\n",
            "batch: 69/160 train loss: 0.9744 train acc 0.9810\n",
            "batch: 70/160 train loss: 0.9739 train acc 0.9810\n",
            "batch: 71/160 train loss: 0.9752 train acc 0.9802\n",
            "batch: 72/160 train loss: 0.9739 train acc 0.9805\n",
            "batch: 73/160 train loss: 0.9747 train acc 0.9797\n",
            "batch: 74/160 train loss: 0.9739 train acc 0.9799\n",
            "batch: 75/160 train loss: 0.9732 train acc 0.9796\n",
            "batch: 76/160 train loss: 0.9736 train acc 0.9794\n",
            "batch: 77/160 train loss: 0.9727 train acc 0.9795\n",
            "batch: 78/160 train loss: 0.9726 train acc 0.9796\n",
            "batch: 79/160 train loss: 0.9730 train acc 0.9796\n",
            "batch: 80/160 train loss: 0.9728 train acc 0.9795\n",
            "batch: 81/160 train loss: 0.9733 train acc 0.9792\n",
            "batch: 82/160 train loss: 0.9749 train acc 0.9788\n",
            "batch: 83/160 train loss: 0.9739 train acc 0.9791\n",
            "batch: 84/160 train loss: 0.9742 train acc 0.9792\n",
            "batch: 85/160 train loss: 0.9734 train acc 0.9792\n",
            "batch: 86/160 train loss: 0.9726 train acc 0.9795\n",
            "batch: 87/160 train loss: 0.9718 train acc 0.9795\n",
            "batch: 88/160 train loss: 0.9717 train acc 0.9798\n",
            "batch: 89/160 train loss: 0.9712 train acc 0.9800\n",
            "batch: 90/160 train loss: 0.9727 train acc 0.9797\n",
            "batch: 91/160 train loss: 0.9721 train acc 0.9799\n",
            "batch: 92/160 train loss: 0.9727 train acc 0.9798\n",
            "batch: 93/160 train loss: 0.9740 train acc 0.9797\n",
            "batch: 94/160 train loss: 0.9740 train acc 0.9796\n",
            "batch: 95/160 train loss: 0.9739 train acc 0.9798\n",
            "batch: 96/160 train loss: 0.9745 train acc 0.9795\n",
            "batch: 97/160 train loss: 0.9745 train acc 0.9794\n",
            "batch: 98/160 train loss: 0.9747 train acc 0.9791\n",
            "batch: 99/160 train loss: 0.9739 train acc 0.9793\n",
            "batch: 100/160 train loss: 0.9731 train acc 0.9794\n",
            "batch: 101/160 train loss: 0.9729 train acc 0.9794\n",
            "batch: 102/160 train loss: 0.9731 train acc 0.9795\n",
            "batch: 103/160 train loss: 0.9736 train acc 0.9795\n",
            "batch: 104/160 train loss: 0.9737 train acc 0.9793\n",
            "batch: 105/160 train loss: 0.9735 train acc 0.9792\n",
            "batch: 106/160 train loss: 0.9737 train acc 0.9791\n",
            "batch: 107/160 train loss: 0.9728 train acc 0.9790\n",
            "batch: 108/160 train loss: 0.9730 train acc 0.9790\n",
            "batch: 109/160 train loss: 0.9726 train acc 0.9791\n",
            "batch: 110/160 train loss: 0.9727 train acc 0.9788\n",
            "batch: 111/160 train loss: 0.9717 train acc 0.9790\n",
            "batch: 112/160 train loss: 0.9720 train acc 0.9791\n",
            "batch: 113/160 train loss: 0.9722 train acc 0.9787\n",
            "batch: 114/160 train loss: 0.9727 train acc 0.9786\n",
            "batch: 115/160 train loss: 0.9735 train acc 0.9785\n",
            "batch: 116/160 train loss: 0.9735 train acc 0.9784\n",
            "batch: 117/160 train loss: 0.9733 train acc 0.9786\n",
            "batch: 118/160 train loss: 0.9736 train acc 0.9787\n",
            "batch: 119/160 train loss: 0.9736 train acc 0.9786\n",
            "batch: 120/160 train loss: 0.9739 train acc 0.9786\n",
            "batch: 121/160 train loss: 0.9737 train acc 0.9787\n",
            "batch: 122/160 train loss: 0.9744 train acc 0.9785\n",
            "batch: 123/160 train loss: 0.9749 train acc 0.9787\n",
            "batch: 124/160 train loss: 0.9746 train acc 0.9788\n",
            "batch: 125/160 train loss: 0.9750 train acc 0.9788\n",
            "batch: 126/160 train loss: 0.9751 train acc 0.9788\n",
            "batch: 127/160 train loss: 0.9747 train acc 0.9788\n",
            "batch: 128/160 train loss: 0.9753 train acc 0.9788\n",
            "batch: 129/160 train loss: 0.9753 train acc 0.9787\n",
            "batch: 130/160 train loss: 0.9757 train acc 0.9785\n",
            "batch: 131/160 train loss: 0.9760 train acc 0.9784\n",
            "batch: 132/160 train loss: 0.9759 train acc 0.9786\n",
            "batch: 133/160 train loss: 0.9758 train acc 0.9784\n",
            "batch: 134/160 train loss: 0.9756 train acc 0.9784\n",
            "batch: 135/160 train loss: 0.9766 train acc 0.9784\n",
            "batch: 136/160 train loss: 0.9758 train acc 0.9785\n",
            "batch: 137/160 train loss: 0.9756 train acc 0.9786\n",
            "batch: 138/160 train loss: 0.9747 train acc 0.9787\n",
            "batch: 139/160 train loss: 0.9742 train acc 0.9789\n",
            "batch: 140/160 train loss: 0.9736 train acc 0.9789\n",
            "batch: 141/160 train loss: 0.9741 train acc 0.9787\n",
            "batch: 142/160 train loss: 0.9744 train acc 0.9787\n",
            "batch: 143/160 train loss: 0.9746 train acc 0.9785\n",
            "batch: 144/160 train loss: 0.9744 train acc 0.9786\n",
            "batch: 145/160 train loss: 0.9740 train acc 0.9786\n",
            "batch: 146/160 train loss: 0.9742 train acc 0.9787\n",
            "batch: 147/160 train loss: 0.9737 train acc 0.9788\n",
            "batch: 148/160 train loss: 0.9735 train acc 0.9789\n",
            "batch: 149/160 train loss: 0.9739 train acc 0.9788\n",
            "batch: 150/160 train loss: 0.9741 train acc 0.9790\n",
            "batch: 151/160 train loss: 0.9743 train acc 0.9790\n",
            "batch: 152/160 train loss: 0.9742 train acc 0.9790\n",
            "batch: 153/160 train loss: 0.9737 train acc 0.9791\n",
            "batch: 154/160 train loss: 0.9738 train acc 0.9790\n",
            "batch: 155/160 train loss: 0.9743 train acc 0.9787\n",
            "batch: 156/160 train loss: 0.9746 train acc 0.9788\n",
            "batch: 157/160 train loss: 0.9743 train acc 0.9788\n",
            "batch: 158/160 train loss: 0.9750 train acc 0.9786\n",
            "batch: 159/160 train loss: 0.9747 train acc 0.9786\n",
            "batch: 160/160 train loss: 0.9761 train acc 0.9783\n",
            "\n",
            "Epoch 17 train loss: 0.9761 test loss 2.3900 train acc 0.9783 test acc 0.6075\n",
            "Epoch 18/25\n",
            "batch: 1/160 train loss: 0.9716 train acc 0.9688\n",
            "batch: 2/160 train loss: 0.9298 train acc 0.9688\n",
            "batch: 3/160 train loss: 0.9184 train acc 0.9792\n",
            "batch: 4/160 train loss: 0.9264 train acc 0.9844\n",
            "batch: 5/160 train loss: 0.9480 train acc 0.9812\n",
            "batch: 6/160 train loss: 0.9496 train acc 0.9792\n",
            "batch: 7/160 train loss: 0.9515 train acc 0.9821\n",
            "batch: 8/160 train loss: 0.9460 train acc 0.9824\n",
            "batch: 9/160 train loss: 0.9460 train acc 0.9826\n",
            "batch: 10/160 train loss: 0.9429 train acc 0.9812\n",
            "batch: 11/160 train loss: 0.9533 train acc 0.9815\n",
            "batch: 12/160 train loss: 0.9563 train acc 0.9792\n",
            "batch: 13/160 train loss: 0.9519 train acc 0.9796\n",
            "batch: 14/160 train loss: 0.9542 train acc 0.9799\n",
            "batch: 15/160 train loss: 0.9601 train acc 0.9781\n",
            "batch: 16/160 train loss: 0.9567 train acc 0.9795\n",
            "batch: 17/160 train loss: 0.9544 train acc 0.9807\n",
            "batch: 18/160 train loss: 0.9525 train acc 0.9818\n",
            "batch: 19/160 train loss: 0.9480 train acc 0.9819\n",
            "batch: 20/160 train loss: 0.9453 train acc 0.9820\n",
            "batch: 21/160 train loss: 0.9438 train acc 0.9829\n",
            "batch: 22/160 train loss: 0.9458 train acc 0.9822\n",
            "batch: 23/160 train loss: 0.9466 train acc 0.9823\n",
            "batch: 24/160 train loss: 0.9464 train acc 0.9824\n",
            "batch: 25/160 train loss: 0.9464 train acc 0.9825\n",
            "batch: 26/160 train loss: 0.9437 train acc 0.9832\n",
            "batch: 27/160 train loss: 0.9434 train acc 0.9832\n",
            "batch: 28/160 train loss: 0.9445 train acc 0.9821\n",
            "batch: 29/160 train loss: 0.9461 train acc 0.9817\n",
            "batch: 30/160 train loss: 0.9445 train acc 0.9818\n",
            "batch: 31/160 train loss: 0.9447 train acc 0.9819\n",
            "batch: 32/160 train loss: 0.9451 train acc 0.9814\n",
            "batch: 33/160 train loss: 0.9457 train acc 0.9806\n",
            "batch: 34/160 train loss: 0.9441 train acc 0.9812\n",
            "batch: 35/160 train loss: 0.9471 train acc 0.9804\n",
            "batch: 36/160 train loss: 0.9458 train acc 0.9809\n",
            "batch: 37/160 train loss: 0.9481 train acc 0.9806\n",
            "batch: 38/160 train loss: 0.9503 train acc 0.9811\n",
            "batch: 39/160 train loss: 0.9502 train acc 0.9812\n",
            "batch: 40/160 train loss: 0.9520 train acc 0.9809\n",
            "batch: 41/160 train loss: 0.9530 train acc 0.9806\n",
            "batch: 42/160 train loss: 0.9492 train acc 0.9810\n",
            "batch: 43/160 train loss: 0.9495 train acc 0.9800\n",
            "batch: 44/160 train loss: 0.9486 train acc 0.9805\n",
            "batch: 45/160 train loss: 0.9489 train acc 0.9809\n",
            "batch: 46/160 train loss: 0.9492 train acc 0.9806\n",
            "batch: 47/160 train loss: 0.9515 train acc 0.9801\n",
            "batch: 48/160 train loss: 0.9521 train acc 0.9798\n",
            "batch: 49/160 train loss: 0.9520 train acc 0.9796\n",
            "batch: 50/160 train loss: 0.9519 train acc 0.9800\n",
            "batch: 51/160 train loss: 0.9524 train acc 0.9798\n",
            "batch: 52/160 train loss: 0.9514 train acc 0.9793\n",
            "batch: 53/160 train loss: 0.9516 train acc 0.9797\n",
            "batch: 54/160 train loss: 0.9514 train acc 0.9792\n",
            "batch: 55/160 train loss: 0.9513 train acc 0.9795\n",
            "batch: 56/160 train loss: 0.9504 train acc 0.9796\n",
            "batch: 57/160 train loss: 0.9504 train acc 0.9800\n",
            "batch: 58/160 train loss: 0.9493 train acc 0.9803\n",
            "batch: 59/160 train loss: 0.9484 train acc 0.9801\n",
            "batch: 60/160 train loss: 0.9493 train acc 0.9802\n",
            "batch: 61/160 train loss: 0.9491 train acc 0.9803\n",
            "batch: 62/160 train loss: 0.9494 train acc 0.9803\n",
            "batch: 63/160 train loss: 0.9499 train acc 0.9807\n",
            "batch: 64/160 train loss: 0.9500 train acc 0.9810\n",
            "batch: 65/160 train loss: 0.9519 train acc 0.9810\n",
            "batch: 66/160 train loss: 0.9530 train acc 0.9813\n",
            "batch: 67/160 train loss: 0.9544 train acc 0.9809\n",
            "batch: 68/160 train loss: 0.9553 train acc 0.9807\n",
            "batch: 69/160 train loss: 0.9563 train acc 0.9805\n",
            "batch: 70/160 train loss: 0.9547 train acc 0.9804\n",
            "batch: 71/160 train loss: 0.9547 train acc 0.9802\n",
            "batch: 72/160 train loss: 0.9537 train acc 0.9803\n",
            "batch: 73/160 train loss: 0.9541 train acc 0.9803\n",
            "batch: 74/160 train loss: 0.9557 train acc 0.9806\n",
            "batch: 75/160 train loss: 0.9560 train acc 0.9804\n",
            "batch: 76/160 train loss: 0.9554 train acc 0.9805\n",
            "batch: 77/160 train loss: 0.9545 train acc 0.9805\n",
            "batch: 78/160 train loss: 0.9538 train acc 0.9808\n",
            "batch: 79/160 train loss: 0.9536 train acc 0.9810\n",
            "batch: 80/160 train loss: 0.9543 train acc 0.9809\n",
            "batch: 81/160 train loss: 0.9537 train acc 0.9811\n",
            "batch: 82/160 train loss: 0.9558 train acc 0.9806\n",
            "batch: 83/160 train loss: 0.9565 train acc 0.9808\n",
            "batch: 84/160 train loss: 0.9562 train acc 0.9808\n",
            "batch: 85/160 train loss: 0.9556 train acc 0.9807\n",
            "batch: 86/160 train loss: 0.9556 train acc 0.9806\n",
            "batch: 87/160 train loss: 0.9557 train acc 0.9804\n",
            "batch: 88/160 train loss: 0.9554 train acc 0.9805\n",
            "batch: 89/160 train loss: 0.9558 train acc 0.9805\n",
            "batch: 90/160 train loss: 0.9567 train acc 0.9802\n",
            "batch: 91/160 train loss: 0.9574 train acc 0.9804\n",
            "batch: 92/160 train loss: 0.9581 train acc 0.9800\n",
            "batch: 93/160 train loss: 0.9572 train acc 0.9802\n",
            "batch: 94/160 train loss: 0.9582 train acc 0.9802\n",
            "batch: 95/160 train loss: 0.9577 train acc 0.9803\n",
            "batch: 96/160 train loss: 0.9571 train acc 0.9805\n",
            "batch: 97/160 train loss: 0.9578 train acc 0.9805\n",
            "batch: 98/160 train loss: 0.9580 train acc 0.9807\n",
            "batch: 99/160 train loss: 0.9582 train acc 0.9804\n",
            "batch: 100/160 train loss: 0.9583 train acc 0.9803\n",
            "batch: 101/160 train loss: 0.9576 train acc 0.9804\n",
            "batch: 102/160 train loss: 0.9575 train acc 0.9804\n",
            "batch: 103/160 train loss: 0.9576 train acc 0.9803\n",
            "batch: 104/160 train loss: 0.9575 train acc 0.9802\n",
            "batch: 105/160 train loss: 0.9577 train acc 0.9802\n",
            "batch: 106/160 train loss: 0.9586 train acc 0.9802\n",
            "batch: 107/160 train loss: 0.9596 train acc 0.9800\n",
            "batch: 108/160 train loss: 0.9595 train acc 0.9800\n",
            "batch: 109/160 train loss: 0.9597 train acc 0.9802\n",
            "batch: 110/160 train loss: 0.9606 train acc 0.9800\n",
            "batch: 111/160 train loss: 0.9607 train acc 0.9802\n",
            "batch: 112/160 train loss: 0.9600 train acc 0.9803\n",
            "batch: 113/160 train loss: 0.9596 train acc 0.9805\n",
            "batch: 114/160 train loss: 0.9602 train acc 0.9803\n",
            "batch: 115/160 train loss: 0.9601 train acc 0.9800\n",
            "batch: 116/160 train loss: 0.9601 train acc 0.9799\n",
            "batch: 117/160 train loss: 0.9598 train acc 0.9801\n",
            "batch: 118/160 train loss: 0.9601 train acc 0.9800\n",
            "batch: 119/160 train loss: 0.9599 train acc 0.9802\n",
            "batch: 120/160 train loss: 0.9594 train acc 0.9799\n",
            "batch: 121/160 train loss: 0.9595 train acc 0.9797\n",
            "batch: 122/160 train loss: 0.9596 train acc 0.9798\n",
            "batch: 123/160 train loss: 0.9593 train acc 0.9799\n",
            "batch: 124/160 train loss: 0.9592 train acc 0.9800\n",
            "batch: 125/160 train loss: 0.9591 train acc 0.9801\n",
            "batch: 126/160 train loss: 0.9594 train acc 0.9800\n",
            "batch: 127/160 train loss: 0.9603 train acc 0.9797\n",
            "batch: 128/160 train loss: 0.9612 train acc 0.9795\n",
            "batch: 129/160 train loss: 0.9608 train acc 0.9797\n",
            "batch: 130/160 train loss: 0.9616 train acc 0.9793\n",
            "batch: 131/160 train loss: 0.9614 train acc 0.9792\n",
            "batch: 132/160 train loss: 0.9615 train acc 0.9794\n",
            "batch: 133/160 train loss: 0.9618 train acc 0.9793\n",
            "batch: 134/160 train loss: 0.9615 train acc 0.9795\n",
            "batch: 135/160 train loss: 0.9619 train acc 0.9794\n",
            "batch: 136/160 train loss: 0.9619 train acc 0.9794\n",
            "batch: 137/160 train loss: 0.9613 train acc 0.9796\n",
            "batch: 138/160 train loss: 0.9608 train acc 0.9796\n",
            "batch: 139/160 train loss: 0.9611 train acc 0.9795\n",
            "batch: 140/160 train loss: 0.9610 train acc 0.9797\n",
            "batch: 141/160 train loss: 0.9609 train acc 0.9797\n",
            "batch: 142/160 train loss: 0.9607 train acc 0.9798\n",
            "batch: 143/160 train loss: 0.9602 train acc 0.9798\n",
            "batch: 144/160 train loss: 0.9603 train acc 0.9798\n",
            "batch: 145/160 train loss: 0.9609 train acc 0.9797\n",
            "batch: 146/160 train loss: 0.9618 train acc 0.9795\n",
            "batch: 147/160 train loss: 0.9619 train acc 0.9795\n",
            "batch: 148/160 train loss: 0.9618 train acc 0.9795\n",
            "batch: 149/160 train loss: 0.9626 train acc 0.9793\n",
            "batch: 150/160 train loss: 0.9631 train acc 0.9793\n",
            "batch: 151/160 train loss: 0.9631 train acc 0.9793\n",
            "batch: 152/160 train loss: 0.9634 train acc 0.9792\n",
            "batch: 153/160 train loss: 0.9631 train acc 0.9794\n",
            "batch: 154/160 train loss: 0.9626 train acc 0.9795\n",
            "batch: 155/160 train loss: 0.9626 train acc 0.9794\n",
            "batch: 156/160 train loss: 0.9618 train acc 0.9796\n",
            "batch: 157/160 train loss: 0.9613 train acc 0.9796\n",
            "batch: 158/160 train loss: 0.9612 train acc 0.9796\n",
            "batch: 159/160 train loss: 0.9616 train acc 0.9797\n",
            "batch: 160/160 train loss: 0.9626 train acc 0.9796\n",
            "\n",
            "Epoch 18 train loss: 0.9626 test loss 2.3869 train acc 0.9796 test acc 0.6092\n",
            "Epoch 19/25\n",
            "batch: 1/160 train loss: 0.9667 train acc 0.9844\n",
            "batch: 2/160 train loss: 0.9357 train acc 0.9766\n",
            "batch: 3/160 train loss: 0.9385 train acc 0.9844\n",
            "batch: 4/160 train loss: 0.9197 train acc 0.9883\n",
            "batch: 5/160 train loss: 0.9254 train acc 0.9875\n",
            "batch: 6/160 train loss: 0.9592 train acc 0.9818\n",
            "batch: 7/160 train loss: 0.9714 train acc 0.9754\n",
            "batch: 8/160 train loss: 0.9828 train acc 0.9746\n",
            "batch: 9/160 train loss: 0.9729 train acc 0.9757\n",
            "batch: 10/160 train loss: 0.9697 train acc 0.9766\n",
            "batch: 11/160 train loss: 0.9754 train acc 0.9730\n",
            "batch: 12/160 train loss: 0.9789 train acc 0.9740\n",
            "batch: 13/160 train loss: 0.9750 train acc 0.9748\n",
            "batch: 14/160 train loss: 0.9822 train acc 0.9721\n",
            "batch: 15/160 train loss: 0.9791 train acc 0.9740\n",
            "batch: 16/160 train loss: 0.9772 train acc 0.9746\n",
            "batch: 17/160 train loss: 0.9714 train acc 0.9761\n",
            "batch: 18/160 train loss: 0.9768 train acc 0.9757\n",
            "batch: 19/160 train loss: 0.9739 train acc 0.9745\n",
            "batch: 20/160 train loss: 0.9757 train acc 0.9742\n",
            "batch: 21/160 train loss: 0.9714 train acc 0.9754\n",
            "batch: 22/160 train loss: 0.9705 train acc 0.9744\n",
            "batch: 23/160 train loss: 0.9716 train acc 0.9742\n",
            "batch: 24/160 train loss: 0.9732 train acc 0.9733\n",
            "batch: 25/160 train loss: 0.9699 train acc 0.9738\n",
            "batch: 26/160 train loss: 0.9692 train acc 0.9730\n",
            "batch: 27/160 train loss: 0.9685 train acc 0.9728\n",
            "batch: 28/160 train loss: 0.9697 train acc 0.9727\n",
            "batch: 29/160 train loss: 0.9705 train acc 0.9725\n",
            "batch: 30/160 train loss: 0.9682 train acc 0.9729\n",
            "batch: 31/160 train loss: 0.9671 train acc 0.9738\n",
            "batch: 32/160 train loss: 0.9694 train acc 0.9731\n",
            "batch: 33/160 train loss: 0.9664 train acc 0.9740\n",
            "batch: 34/160 train loss: 0.9648 train acc 0.9743\n",
            "batch: 35/160 train loss: 0.9621 train acc 0.9750\n",
            "batch: 36/160 train loss: 0.9612 train acc 0.9748\n",
            "batch: 37/160 train loss: 0.9620 train acc 0.9755\n",
            "batch: 38/160 train loss: 0.9606 train acc 0.9757\n",
            "batch: 39/160 train loss: 0.9598 train acc 0.9760\n",
            "batch: 40/160 train loss: 0.9580 train acc 0.9766\n",
            "batch: 41/160 train loss: 0.9572 train acc 0.9768\n",
            "batch: 42/160 train loss: 0.9576 train acc 0.9773\n",
            "batch: 43/160 train loss: 0.9574 train acc 0.9778\n",
            "batch: 44/160 train loss: 0.9583 train acc 0.9776\n",
            "batch: 45/160 train loss: 0.9598 train acc 0.9774\n",
            "batch: 46/160 train loss: 0.9600 train acc 0.9776\n",
            "batch: 47/160 train loss: 0.9591 train acc 0.9777\n",
            "batch: 48/160 train loss: 0.9591 train acc 0.9779\n",
            "batch: 49/160 train loss: 0.9603 train acc 0.9777\n",
            "batch: 50/160 train loss: 0.9595 train acc 0.9778\n",
            "batch: 51/160 train loss: 0.9615 train acc 0.9773\n",
            "batch: 52/160 train loss: 0.9620 train acc 0.9775\n",
            "batch: 53/160 train loss: 0.9632 train acc 0.9773\n",
            "batch: 54/160 train loss: 0.9626 train acc 0.9777\n",
            "batch: 55/160 train loss: 0.9614 train acc 0.9778\n",
            "batch: 56/160 train loss: 0.9605 train acc 0.9780\n",
            "batch: 57/160 train loss: 0.9597 train acc 0.9783\n",
            "batch: 58/160 train loss: 0.9599 train acc 0.9782\n",
            "batch: 59/160 train loss: 0.9584 train acc 0.9783\n",
            "batch: 60/160 train loss: 0.9579 train acc 0.9786\n",
            "batch: 61/160 train loss: 0.9596 train acc 0.9785\n",
            "batch: 62/160 train loss: 0.9588 train acc 0.9788\n",
            "batch: 63/160 train loss: 0.9593 train acc 0.9789\n",
            "batch: 64/160 train loss: 0.9594 train acc 0.9788\n",
            "batch: 65/160 train loss: 0.9594 train acc 0.9791\n",
            "batch: 66/160 train loss: 0.9590 train acc 0.9792\n",
            "batch: 67/160 train loss: 0.9583 train acc 0.9795\n",
            "batch: 68/160 train loss: 0.9582 train acc 0.9793\n",
            "batch: 69/160 train loss: 0.9569 train acc 0.9794\n",
            "batch: 70/160 train loss: 0.9575 train acc 0.9795\n",
            "batch: 71/160 train loss: 0.9568 train acc 0.9798\n",
            "batch: 72/160 train loss: 0.9560 train acc 0.9800\n",
            "batch: 73/160 train loss: 0.9564 train acc 0.9801\n",
            "batch: 74/160 train loss: 0.9557 train acc 0.9802\n",
            "batch: 75/160 train loss: 0.9565 train acc 0.9798\n",
            "batch: 76/160 train loss: 0.9566 train acc 0.9801\n",
            "batch: 77/160 train loss: 0.9555 train acc 0.9801\n",
            "batch: 78/160 train loss: 0.9550 train acc 0.9804\n",
            "batch: 79/160 train loss: 0.9551 train acc 0.9804\n",
            "batch: 80/160 train loss: 0.9558 train acc 0.9807\n",
            "batch: 81/160 train loss: 0.9560 train acc 0.9803\n",
            "batch: 82/160 train loss: 0.9569 train acc 0.9804\n",
            "batch: 83/160 train loss: 0.9558 train acc 0.9806\n",
            "batch: 84/160 train loss: 0.9543 train acc 0.9808\n",
            "batch: 85/160 train loss: 0.9542 train acc 0.9809\n",
            "batch: 86/160 train loss: 0.9545 train acc 0.9806\n",
            "batch: 87/160 train loss: 0.9540 train acc 0.9808\n",
            "batch: 88/160 train loss: 0.9537 train acc 0.9810\n",
            "batch: 89/160 train loss: 0.9542 train acc 0.9810\n",
            "batch: 90/160 train loss: 0.9545 train acc 0.9807\n",
            "batch: 91/160 train loss: 0.9548 train acc 0.9806\n",
            "batch: 92/160 train loss: 0.9551 train acc 0.9806\n",
            "batch: 93/160 train loss: 0.9551 train acc 0.9807\n",
            "batch: 94/160 train loss: 0.9547 train acc 0.9807\n",
            "batch: 95/160 train loss: 0.9555 train acc 0.9806\n",
            "batch: 96/160 train loss: 0.9567 train acc 0.9806\n",
            "batch: 97/160 train loss: 0.9558 train acc 0.9807\n",
            "batch: 98/160 train loss: 0.9553 train acc 0.9807\n",
            "batch: 99/160 train loss: 0.9559 train acc 0.9806\n",
            "batch: 100/160 train loss: 0.9568 train acc 0.9802\n",
            "batch: 101/160 train loss: 0.9559 train acc 0.9802\n",
            "batch: 102/160 train loss: 0.9563 train acc 0.9802\n",
            "batch: 103/160 train loss: 0.9573 train acc 0.9801\n",
            "batch: 104/160 train loss: 0.9568 train acc 0.9800\n",
            "batch: 105/160 train loss: 0.9563 train acc 0.9802\n",
            "batch: 106/160 train loss: 0.9571 train acc 0.9804\n",
            "batch: 107/160 train loss: 0.9570 train acc 0.9803\n",
            "batch: 108/160 train loss: 0.9568 train acc 0.9803\n",
            "batch: 109/160 train loss: 0.9565 train acc 0.9802\n",
            "batch: 110/160 train loss: 0.9559 train acc 0.9804\n",
            "batch: 111/160 train loss: 0.9553 train acc 0.9806\n",
            "batch: 112/160 train loss: 0.9556 train acc 0.9807\n",
            "batch: 113/160 train loss: 0.9552 train acc 0.9808\n",
            "batch: 114/160 train loss: 0.9554 train acc 0.9808\n",
            "batch: 115/160 train loss: 0.9563 train acc 0.9808\n",
            "batch: 116/160 train loss: 0.9566 train acc 0.9805\n",
            "batch: 117/160 train loss: 0.9572 train acc 0.9802\n",
            "batch: 118/160 train loss: 0.9564 train acc 0.9804\n",
            "batch: 119/160 train loss: 0.9567 train acc 0.9804\n",
            "batch: 120/160 train loss: 0.9567 train acc 0.9805\n",
            "batch: 121/160 train loss: 0.9572 train acc 0.9804\n",
            "batch: 122/160 train loss: 0.9559 train acc 0.9805\n",
            "batch: 123/160 train loss: 0.9554 train acc 0.9806\n",
            "batch: 124/160 train loss: 0.9559 train acc 0.9802\n",
            "batch: 125/160 train loss: 0.9552 train acc 0.9802\n",
            "batch: 126/160 train loss: 0.9548 train acc 0.9803\n",
            "batch: 127/160 train loss: 0.9547 train acc 0.9803\n",
            "batch: 128/160 train loss: 0.9548 train acc 0.9802\n",
            "batch: 129/160 train loss: 0.9543 train acc 0.9803\n",
            "batch: 130/160 train loss: 0.9544 train acc 0.9803\n",
            "batch: 131/160 train loss: 0.9537 train acc 0.9803\n",
            "batch: 132/160 train loss: 0.9536 train acc 0.9802\n",
            "batch: 133/160 train loss: 0.9530 train acc 0.9804\n",
            "batch: 134/160 train loss: 0.9527 train acc 0.9805\n",
            "batch: 135/160 train loss: 0.9525 train acc 0.9807\n",
            "batch: 136/160 train loss: 0.9530 train acc 0.9806\n",
            "batch: 137/160 train loss: 0.9528 train acc 0.9807\n",
            "batch: 138/160 train loss: 0.9527 train acc 0.9808\n",
            "batch: 139/160 train loss: 0.9522 train acc 0.9808\n",
            "batch: 140/160 train loss: 0.9522 train acc 0.9809\n",
            "batch: 141/160 train loss: 0.9518 train acc 0.9811\n",
            "batch: 142/160 train loss: 0.9516 train acc 0.9812\n",
            "batch: 143/160 train loss: 0.9520 train acc 0.9810\n",
            "batch: 144/160 train loss: 0.9519 train acc 0.9811\n",
            "batch: 145/160 train loss: 0.9526 train acc 0.9810\n",
            "batch: 146/160 train loss: 0.9528 train acc 0.9811\n",
            "batch: 147/160 train loss: 0.9531 train acc 0.9807\n",
            "batch: 148/160 train loss: 0.9531 train acc 0.9806\n",
            "batch: 149/160 train loss: 0.9530 train acc 0.9805\n",
            "batch: 150/160 train loss: 0.9527 train acc 0.9804\n",
            "batch: 151/160 train loss: 0.9525 train acc 0.9803\n",
            "batch: 152/160 train loss: 0.9531 train acc 0.9803\n",
            "batch: 153/160 train loss: 0.9536 train acc 0.9803\n",
            "batch: 154/160 train loss: 0.9539 train acc 0.9804\n",
            "batch: 155/160 train loss: 0.9537 train acc 0.9803\n",
            "batch: 156/160 train loss: 0.9533 train acc 0.9804\n",
            "batch: 157/160 train loss: 0.9539 train acc 0.9800\n",
            "batch: 158/160 train loss: 0.9533 train acc 0.9800\n",
            "batch: 159/160 train loss: 0.9538 train acc 0.9801\n",
            "batch: 160/160 train loss: 0.9549 train acc 0.9799\n",
            "\n",
            "Epoch 19 train loss: 0.9549 test loss 2.3826 train acc 0.9799 test acc 0.6133\n",
            "Epoch 20/25\n",
            "batch: 1/160 train loss: 0.8821 train acc 0.9844\n",
            "batch: 2/160 train loss: 0.8890 train acc 0.9922\n",
            "batch: 3/160 train loss: 0.9587 train acc 0.9844\n",
            "batch: 4/160 train loss: 0.9593 train acc 0.9844\n",
            "batch: 5/160 train loss: 0.9704 train acc 0.9844\n",
            "batch: 6/160 train loss: 0.9626 train acc 0.9818\n",
            "batch: 7/160 train loss: 0.9614 train acc 0.9821\n",
            "batch: 8/160 train loss: 0.9611 train acc 0.9805\n",
            "batch: 9/160 train loss: 0.9565 train acc 0.9809\n",
            "batch: 10/160 train loss: 0.9554 train acc 0.9812\n",
            "batch: 11/160 train loss: 0.9540 train acc 0.9815\n",
            "batch: 12/160 train loss: 0.9503 train acc 0.9818\n",
            "batch: 13/160 train loss: 0.9457 train acc 0.9808\n",
            "batch: 14/160 train loss: 0.9427 train acc 0.9821\n",
            "batch: 15/160 train loss: 0.9413 train acc 0.9833\n",
            "batch: 16/160 train loss: 0.9379 train acc 0.9834\n",
            "batch: 17/160 train loss: 0.9335 train acc 0.9844\n",
            "batch: 18/160 train loss: 0.9387 train acc 0.9844\n",
            "batch: 19/160 train loss: 0.9403 train acc 0.9852\n",
            "batch: 20/160 train loss: 0.9432 train acc 0.9852\n",
            "batch: 21/160 train loss: 0.9460 train acc 0.9836\n",
            "batch: 22/160 train loss: 0.9478 train acc 0.9830\n",
            "batch: 23/160 train loss: 0.9478 train acc 0.9823\n",
            "batch: 24/160 train loss: 0.9468 train acc 0.9818\n",
            "batch: 25/160 train loss: 0.9479 train acc 0.9819\n",
            "batch: 26/160 train loss: 0.9484 train acc 0.9814\n",
            "batch: 27/160 train loss: 0.9510 train acc 0.9809\n",
            "batch: 28/160 train loss: 0.9508 train acc 0.9810\n",
            "batch: 29/160 train loss: 0.9519 train acc 0.9811\n",
            "batch: 30/160 train loss: 0.9532 train acc 0.9812\n",
            "batch: 31/160 train loss: 0.9521 train acc 0.9808\n",
            "batch: 32/160 train loss: 0.9559 train acc 0.9814\n",
            "batch: 33/160 train loss: 0.9563 train acc 0.9820\n",
            "batch: 34/160 train loss: 0.9554 train acc 0.9821\n",
            "batch: 35/160 train loss: 0.9588 train acc 0.9812\n",
            "batch: 36/160 train loss: 0.9579 train acc 0.9818\n",
            "batch: 37/160 train loss: 0.9588 train acc 0.9818\n",
            "batch: 38/160 train loss: 0.9576 train acc 0.9819\n",
            "batch: 39/160 train loss: 0.9568 train acc 0.9820\n",
            "batch: 40/160 train loss: 0.9589 train acc 0.9820\n",
            "batch: 41/160 train loss: 0.9561 train acc 0.9821\n",
            "batch: 42/160 train loss: 0.9534 train acc 0.9825\n",
            "batch: 43/160 train loss: 0.9526 train acc 0.9826\n",
            "batch: 44/160 train loss: 0.9539 train acc 0.9822\n",
            "batch: 45/160 train loss: 0.9539 train acc 0.9823\n",
            "batch: 46/160 train loss: 0.9534 train acc 0.9827\n",
            "batch: 47/160 train loss: 0.9530 train acc 0.9827\n",
            "batch: 48/160 train loss: 0.9532 train acc 0.9824\n",
            "batch: 49/160 train loss: 0.9519 train acc 0.9828\n",
            "batch: 50/160 train loss: 0.9517 train acc 0.9828\n",
            "batch: 51/160 train loss: 0.9505 train acc 0.9828\n",
            "batch: 52/160 train loss: 0.9499 train acc 0.9829\n",
            "batch: 53/160 train loss: 0.9500 train acc 0.9829\n",
            "batch: 54/160 train loss: 0.9500 train acc 0.9829\n",
            "batch: 55/160 train loss: 0.9493 train acc 0.9832\n",
            "batch: 56/160 train loss: 0.9495 train acc 0.9833\n",
            "batch: 57/160 train loss: 0.9485 train acc 0.9833\n",
            "batch: 58/160 train loss: 0.9496 train acc 0.9833\n",
            "batch: 59/160 train loss: 0.9498 train acc 0.9833\n",
            "batch: 60/160 train loss: 0.9499 train acc 0.9833\n",
            "batch: 61/160 train loss: 0.9480 train acc 0.9834\n",
            "batch: 62/160 train loss: 0.9471 train acc 0.9836\n",
            "batch: 63/160 train loss: 0.9472 train acc 0.9839\n",
            "batch: 64/160 train loss: 0.9466 train acc 0.9841\n",
            "batch: 65/160 train loss: 0.9467 train acc 0.9839\n",
            "batch: 66/160 train loss: 0.9462 train acc 0.9841\n",
            "batch: 67/160 train loss: 0.9468 train acc 0.9841\n",
            "batch: 68/160 train loss: 0.9446 train acc 0.9844\n",
            "batch: 69/160 train loss: 0.9454 train acc 0.9839\n",
            "batch: 70/160 train loss: 0.9460 train acc 0.9842\n",
            "batch: 71/160 train loss: 0.9459 train acc 0.9839\n",
            "batch: 72/160 train loss: 0.9463 train acc 0.9839\n",
            "batch: 73/160 train loss: 0.9463 train acc 0.9839\n",
            "batch: 74/160 train loss: 0.9462 train acc 0.9840\n",
            "batch: 75/160 train loss: 0.9453 train acc 0.9842\n",
            "batch: 76/160 train loss: 0.9454 train acc 0.9840\n",
            "batch: 77/160 train loss: 0.9452 train acc 0.9840\n",
            "batch: 78/160 train loss: 0.9441 train acc 0.9842\n",
            "batch: 79/160 train loss: 0.9444 train acc 0.9838\n",
            "batch: 80/160 train loss: 0.9438 train acc 0.9840\n",
            "batch: 81/160 train loss: 0.9448 train acc 0.9836\n",
            "batch: 82/160 train loss: 0.9455 train acc 0.9834\n",
            "batch: 83/160 train loss: 0.9454 train acc 0.9836\n",
            "batch: 84/160 train loss: 0.9460 train acc 0.9838\n",
            "batch: 85/160 train loss: 0.9456 train acc 0.9836\n",
            "batch: 86/160 train loss: 0.9452 train acc 0.9835\n",
            "batch: 87/160 train loss: 0.9446 train acc 0.9835\n",
            "batch: 88/160 train loss: 0.9439 train acc 0.9835\n",
            "batch: 89/160 train loss: 0.9436 train acc 0.9837\n",
            "batch: 90/160 train loss: 0.9437 train acc 0.9837\n",
            "batch: 91/160 train loss: 0.9438 train acc 0.9835\n",
            "batch: 92/160 train loss: 0.9438 train acc 0.9837\n",
            "batch: 93/160 train loss: 0.9434 train acc 0.9839\n",
            "batch: 94/160 train loss: 0.9434 train acc 0.9839\n",
            "batch: 95/160 train loss: 0.9440 train acc 0.9837\n",
            "batch: 96/160 train loss: 0.9437 train acc 0.9837\n",
            "batch: 97/160 train loss: 0.9437 train acc 0.9837\n",
            "batch: 98/160 train loss: 0.9433 train acc 0.9839\n",
            "batch: 99/160 train loss: 0.9432 train acc 0.9839\n",
            "batch: 100/160 train loss: 0.9434 train acc 0.9841\n",
            "batch: 101/160 train loss: 0.9443 train acc 0.9841\n",
            "batch: 102/160 train loss: 0.9442 train acc 0.9842\n",
            "batch: 103/160 train loss: 0.9452 train acc 0.9841\n",
            "batch: 104/160 train loss: 0.9444 train acc 0.9841\n",
            "batch: 105/160 train loss: 0.9443 train acc 0.9839\n",
            "batch: 106/160 train loss: 0.9438 train acc 0.9839\n",
            "batch: 107/160 train loss: 0.9449 train acc 0.9835\n",
            "batch: 108/160 train loss: 0.9441 train acc 0.9835\n",
            "batch: 109/160 train loss: 0.9432 train acc 0.9837\n",
            "batch: 110/160 train loss: 0.9431 train acc 0.9838\n",
            "batch: 111/160 train loss: 0.9432 train acc 0.9838\n",
            "batch: 112/160 train loss: 0.9428 train acc 0.9840\n",
            "batch: 113/160 train loss: 0.9427 train acc 0.9838\n",
            "batch: 114/160 train loss: 0.9435 train acc 0.9836\n",
            "batch: 115/160 train loss: 0.9439 train acc 0.9836\n",
            "batch: 116/160 train loss: 0.9432 train acc 0.9837\n",
            "batch: 117/160 train loss: 0.9432 train acc 0.9838\n",
            "batch: 118/160 train loss: 0.9431 train acc 0.9838\n",
            "batch: 119/160 train loss: 0.9421 train acc 0.9838\n",
            "batch: 120/160 train loss: 0.9426 train acc 0.9839\n",
            "batch: 121/160 train loss: 0.9429 train acc 0.9837\n",
            "batch: 122/160 train loss: 0.9423 train acc 0.9836\n",
            "batch: 123/160 train loss: 0.9431 train acc 0.9832\n",
            "batch: 124/160 train loss: 0.9426 train acc 0.9831\n",
            "batch: 125/160 train loss: 0.9429 train acc 0.9832\n",
            "batch: 126/160 train loss: 0.9430 train acc 0.9834\n",
            "batch: 127/160 train loss: 0.9428 train acc 0.9834\n",
            "batch: 128/160 train loss: 0.9427 train acc 0.9834\n",
            "batch: 129/160 train loss: 0.9428 train acc 0.9833\n",
            "batch: 130/160 train loss: 0.9423 train acc 0.9834\n",
            "batch: 131/160 train loss: 0.9422 train acc 0.9834\n",
            "batch: 132/160 train loss: 0.9418 train acc 0.9835\n",
            "batch: 133/160 train loss: 0.9415 train acc 0.9836\n",
            "batch: 134/160 train loss: 0.9417 train acc 0.9834\n",
            "batch: 135/160 train loss: 0.9426 train acc 0.9833\n",
            "batch: 136/160 train loss: 0.9425 train acc 0.9833\n",
            "batch: 137/160 train loss: 0.9422 train acc 0.9835\n",
            "batch: 138/160 train loss: 0.9422 train acc 0.9835\n",
            "batch: 139/160 train loss: 0.9416 train acc 0.9835\n",
            "batch: 140/160 train loss: 0.9420 train acc 0.9836\n",
            "batch: 141/160 train loss: 0.9418 train acc 0.9837\n",
            "batch: 142/160 train loss: 0.9418 train acc 0.9838\n",
            "batch: 143/160 train loss: 0.9418 train acc 0.9839\n",
            "batch: 144/160 train loss: 0.9418 train acc 0.9838\n",
            "batch: 145/160 train loss: 0.9423 train acc 0.9838\n",
            "batch: 146/160 train loss: 0.9425 train acc 0.9837\n",
            "batch: 147/160 train loss: 0.9420 train acc 0.9838\n",
            "batch: 148/160 train loss: 0.9420 train acc 0.9838\n",
            "batch: 149/160 train loss: 0.9419 train acc 0.9840\n",
            "batch: 150/160 train loss: 0.9417 train acc 0.9839\n",
            "batch: 151/160 train loss: 0.9416 train acc 0.9838\n",
            "batch: 152/160 train loss: 0.9416 train acc 0.9837\n",
            "batch: 153/160 train loss: 0.9419 train acc 0.9836\n",
            "batch: 154/160 train loss: 0.9422 train acc 0.9834\n",
            "batch: 155/160 train loss: 0.9418 train acc 0.9835\n",
            "batch: 156/160 train loss: 0.9421 train acc 0.9833\n",
            "batch: 157/160 train loss: 0.9420 train acc 0.9833\n",
            "batch: 158/160 train loss: 0.9421 train acc 0.9832\n",
            "batch: 159/160 train loss: 0.9422 train acc 0.9831\n",
            "batch: 160/160 train loss: 0.9424 train acc 0.9831\n",
            "\n",
            "Epoch 20 train loss: 0.9424 test loss 2.3758 train acc 0.9831 test acc 0.6158\n",
            "Epoch 21/25\n",
            "batch: 1/160 train loss: 0.8932 train acc 1.0000\n",
            "batch: 2/160 train loss: 0.9103 train acc 0.9922\n",
            "batch: 3/160 train loss: 0.9120 train acc 0.9896\n",
            "batch: 4/160 train loss: 0.9066 train acc 0.9922\n",
            "batch: 5/160 train loss: 0.9104 train acc 0.9906\n",
            "batch: 6/160 train loss: 0.9204 train acc 0.9870\n",
            "batch: 7/160 train loss: 0.9162 train acc 0.9821\n",
            "batch: 8/160 train loss: 0.9069 train acc 0.9844\n",
            "batch: 9/160 train loss: 0.9037 train acc 0.9861\n",
            "batch: 10/160 train loss: 0.9079 train acc 0.9875\n",
            "batch: 11/160 train loss: 0.9148 train acc 0.9886\n",
            "batch: 12/160 train loss: 0.9131 train acc 0.9883\n",
            "batch: 13/160 train loss: 0.9264 train acc 0.9868\n",
            "batch: 14/160 train loss: 0.9287 train acc 0.9866\n",
            "batch: 15/160 train loss: 0.9240 train acc 0.9875\n",
            "batch: 16/160 train loss: 0.9248 train acc 0.9883\n",
            "batch: 17/160 train loss: 0.9219 train acc 0.9890\n",
            "batch: 18/160 train loss: 0.9243 train acc 0.9887\n",
            "batch: 19/160 train loss: 0.9267 train acc 0.9885\n",
            "batch: 20/160 train loss: 0.9252 train acc 0.9891\n",
            "batch: 21/160 train loss: 0.9249 train acc 0.9888\n",
            "batch: 22/160 train loss: 0.9279 train acc 0.9886\n",
            "batch: 23/160 train loss: 0.9295 train acc 0.9885\n",
            "batch: 24/160 train loss: 0.9271 train acc 0.9889\n",
            "batch: 25/160 train loss: 0.9287 train acc 0.9888\n",
            "batch: 26/160 train loss: 0.9283 train acc 0.9886\n",
            "batch: 27/160 train loss: 0.9320 train acc 0.9878\n",
            "batch: 28/160 train loss: 0.9311 train acc 0.9883\n",
            "batch: 29/160 train loss: 0.9314 train acc 0.9881\n",
            "batch: 30/160 train loss: 0.9323 train acc 0.9880\n",
            "batch: 31/160 train loss: 0.9330 train acc 0.9879\n",
            "batch: 32/160 train loss: 0.9341 train acc 0.9873\n",
            "batch: 33/160 train loss: 0.9323 train acc 0.9877\n",
            "batch: 34/160 train loss: 0.9312 train acc 0.9876\n",
            "batch: 35/160 train loss: 0.9296 train acc 0.9879\n",
            "batch: 36/160 train loss: 0.9292 train acc 0.9878\n",
            "batch: 37/160 train loss: 0.9276 train acc 0.9878\n",
            "batch: 38/160 train loss: 0.9276 train acc 0.9873\n",
            "batch: 39/160 train loss: 0.9280 train acc 0.9872\n",
            "batch: 40/160 train loss: 0.9268 train acc 0.9871\n",
            "batch: 41/160 train loss: 0.9280 train acc 0.9867\n",
            "batch: 42/160 train loss: 0.9296 train acc 0.9866\n",
            "batch: 43/160 train loss: 0.9303 train acc 0.9858\n",
            "batch: 44/160 train loss: 0.9310 train acc 0.9858\n",
            "batch: 45/160 train loss: 0.9324 train acc 0.9854\n",
            "batch: 46/160 train loss: 0.9334 train acc 0.9854\n",
            "batch: 47/160 train loss: 0.9332 train acc 0.9854\n",
            "batch: 48/160 train loss: 0.9345 train acc 0.9850\n",
            "batch: 49/160 train loss: 0.9348 train acc 0.9853\n",
            "batch: 50/160 train loss: 0.9341 train acc 0.9850\n",
            "batch: 51/160 train loss: 0.9330 train acc 0.9853\n",
            "batch: 52/160 train loss: 0.9321 train acc 0.9853\n",
            "batch: 53/160 train loss: 0.9308 train acc 0.9856\n",
            "batch: 54/160 train loss: 0.9299 train acc 0.9852\n",
            "batch: 55/160 train loss: 0.9302 train acc 0.9855\n",
            "batch: 56/160 train loss: 0.9298 train acc 0.9855\n",
            "batch: 57/160 train loss: 0.9302 train acc 0.9852\n",
            "batch: 58/160 train loss: 0.9303 train acc 0.9855\n",
            "batch: 59/160 train loss: 0.9304 train acc 0.9857\n",
            "batch: 60/160 train loss: 0.9301 train acc 0.9854\n",
            "batch: 61/160 train loss: 0.9291 train acc 0.9857\n",
            "batch: 62/160 train loss: 0.9289 train acc 0.9856\n",
            "batch: 63/160 train loss: 0.9275 train acc 0.9859\n",
            "batch: 64/160 train loss: 0.9277 train acc 0.9861\n",
            "batch: 65/160 train loss: 0.9276 train acc 0.9863\n",
            "batch: 66/160 train loss: 0.9277 train acc 0.9863\n",
            "batch: 67/160 train loss: 0.9277 train acc 0.9862\n",
            "batch: 68/160 train loss: 0.9282 train acc 0.9862\n",
            "batch: 69/160 train loss: 0.9285 train acc 0.9860\n",
            "batch: 70/160 train loss: 0.9287 train acc 0.9862\n",
            "batch: 71/160 train loss: 0.9284 train acc 0.9861\n",
            "batch: 72/160 train loss: 0.9281 train acc 0.9859\n",
            "batch: 73/160 train loss: 0.9274 train acc 0.9861\n",
            "batch: 74/160 train loss: 0.9279 train acc 0.9859\n",
            "batch: 75/160 train loss: 0.9280 train acc 0.9858\n",
            "batch: 76/160 train loss: 0.9277 train acc 0.9860\n",
            "batch: 77/160 train loss: 0.9277 train acc 0.9860\n",
            "batch: 78/160 train loss: 0.9282 train acc 0.9856\n",
            "batch: 79/160 train loss: 0.9274 train acc 0.9858\n",
            "batch: 80/160 train loss: 0.9280 train acc 0.9854\n",
            "batch: 81/160 train loss: 0.9287 train acc 0.9853\n",
            "batch: 82/160 train loss: 0.9287 train acc 0.9853\n",
            "batch: 83/160 train loss: 0.9283 train acc 0.9853\n",
            "batch: 84/160 train loss: 0.9293 train acc 0.9851\n",
            "batch: 85/160 train loss: 0.9296 train acc 0.9853\n",
            "batch: 86/160 train loss: 0.9290 train acc 0.9851\n",
            "batch: 87/160 train loss: 0.9292 train acc 0.9853\n",
            "batch: 88/160 train loss: 0.9290 train acc 0.9853\n",
            "batch: 89/160 train loss: 0.9294 train acc 0.9853\n",
            "batch: 90/160 train loss: 0.9299 train acc 0.9849\n",
            "batch: 91/160 train loss: 0.9298 train acc 0.9851\n",
            "batch: 92/160 train loss: 0.9297 train acc 0.9849\n",
            "batch: 93/160 train loss: 0.9299 train acc 0.9850\n",
            "batch: 94/160 train loss: 0.9294 train acc 0.9849\n",
            "batch: 95/160 train loss: 0.9299 train acc 0.9849\n",
            "batch: 96/160 train loss: 0.9302 train acc 0.9849\n",
            "batch: 97/160 train loss: 0.9302 train acc 0.9847\n",
            "batch: 98/160 train loss: 0.9301 train acc 0.9847\n",
            "batch: 99/160 train loss: 0.9297 train acc 0.9845\n",
            "batch: 100/160 train loss: 0.9311 train acc 0.9844\n",
            "batch: 101/160 train loss: 0.9311 train acc 0.9842\n",
            "batch: 102/160 train loss: 0.9320 train acc 0.9844\n",
            "batch: 103/160 train loss: 0.9330 train acc 0.9844\n",
            "batch: 104/160 train loss: 0.9323 train acc 0.9845\n",
            "batch: 105/160 train loss: 0.9318 train acc 0.9845\n",
            "batch: 106/160 train loss: 0.9311 train acc 0.9847\n",
            "batch: 107/160 train loss: 0.9308 train acc 0.9847\n",
            "batch: 108/160 train loss: 0.9313 train acc 0.9845\n",
            "batch: 109/160 train loss: 0.9317 train acc 0.9844\n",
            "batch: 110/160 train loss: 0.9315 train acc 0.9844\n",
            "batch: 111/160 train loss: 0.9322 train acc 0.9844\n",
            "batch: 112/160 train loss: 0.9323 train acc 0.9845\n",
            "batch: 113/160 train loss: 0.9318 train acc 0.9847\n",
            "batch: 114/160 train loss: 0.9311 train acc 0.9846\n",
            "batch: 115/160 train loss: 0.9318 train acc 0.9845\n",
            "batch: 116/160 train loss: 0.9321 train acc 0.9845\n",
            "batch: 117/160 train loss: 0.9321 train acc 0.9845\n",
            "batch: 118/160 train loss: 0.9313 train acc 0.9846\n",
            "batch: 119/160 train loss: 0.9302 train acc 0.9848\n",
            "batch: 120/160 train loss: 0.9304 train acc 0.9846\n",
            "batch: 121/160 train loss: 0.9302 train acc 0.9846\n",
            "batch: 122/160 train loss: 0.9295 train acc 0.9848\n",
            "batch: 123/160 train loss: 0.9300 train acc 0.9848\n",
            "batch: 124/160 train loss: 0.9305 train acc 0.9845\n",
            "batch: 125/160 train loss: 0.9303 train acc 0.9844\n",
            "batch: 126/160 train loss: 0.9306 train acc 0.9844\n",
            "batch: 127/160 train loss: 0.9301 train acc 0.9845\n",
            "batch: 128/160 train loss: 0.9303 train acc 0.9846\n",
            "batch: 129/160 train loss: 0.9304 train acc 0.9845\n",
            "batch: 130/160 train loss: 0.9302 train acc 0.9845\n",
            "batch: 131/160 train loss: 0.9298 train acc 0.9845\n",
            "batch: 132/160 train loss: 0.9296 train acc 0.9846\n",
            "batch: 133/160 train loss: 0.9294 train acc 0.9847\n",
            "batch: 134/160 train loss: 0.9291 train acc 0.9848\n",
            "batch: 135/160 train loss: 0.9293 train acc 0.9847\n",
            "batch: 136/160 train loss: 0.9288 train acc 0.9847\n",
            "batch: 137/160 train loss: 0.9285 train acc 0.9848\n",
            "batch: 138/160 train loss: 0.9294 train acc 0.9847\n",
            "batch: 139/160 train loss: 0.9292 train acc 0.9848\n",
            "batch: 140/160 train loss: 0.9295 train acc 0.9847\n",
            "batch: 141/160 train loss: 0.9304 train acc 0.9846\n",
            "batch: 142/160 train loss: 0.9305 train acc 0.9846\n",
            "batch: 143/160 train loss: 0.9309 train acc 0.9846\n",
            "batch: 144/160 train loss: 0.9315 train acc 0.9844\n",
            "batch: 145/160 train loss: 0.9314 train acc 0.9844\n",
            "batch: 146/160 train loss: 0.9316 train acc 0.9844\n",
            "batch: 147/160 train loss: 0.9310 train acc 0.9845\n",
            "batch: 148/160 train loss: 0.9306 train acc 0.9845\n",
            "batch: 149/160 train loss: 0.9308 train acc 0.9846\n",
            "batch: 150/160 train loss: 0.9305 train acc 0.9846\n",
            "batch: 151/160 train loss: 0.9304 train acc 0.9847\n",
            "batch: 152/160 train loss: 0.9308 train acc 0.9846\n",
            "batch: 153/160 train loss: 0.9310 train acc 0.9845\n",
            "batch: 154/160 train loss: 0.9306 train acc 0.9844\n",
            "batch: 155/160 train loss: 0.9311 train acc 0.9842\n",
            "batch: 156/160 train loss: 0.9307 train acc 0.9842\n",
            "batch: 157/160 train loss: 0.9315 train acc 0.9843\n",
            "batch: 158/160 train loss: 0.9316 train acc 0.9842\n",
            "batch: 159/160 train loss: 0.9317 train acc 0.9842\n",
            "batch: 160/160 train loss: 0.9318 train acc 0.9842\n",
            "\n",
            "Epoch 21 train loss: 0.9318 test loss 2.3733 train acc 0.9842 test acc 0.6108\n",
            "Epoch 22/25\n",
            "batch: 1/160 train loss: 0.8896 train acc 0.9688\n",
            "batch: 2/160 train loss: 0.9099 train acc 0.9844\n",
            "batch: 3/160 train loss: 0.9416 train acc 0.9688\n",
            "batch: 4/160 train loss: 0.9264 train acc 0.9766\n",
            "batch: 5/160 train loss: 0.9190 train acc 0.9781\n",
            "batch: 6/160 train loss: 0.9195 train acc 0.9766\n",
            "batch: 7/160 train loss: 0.9200 train acc 0.9799\n",
            "batch: 8/160 train loss: 0.9420 train acc 0.9746\n",
            "batch: 9/160 train loss: 0.9324 train acc 0.9774\n",
            "batch: 10/160 train loss: 0.9331 train acc 0.9797\n",
            "batch: 11/160 train loss: 0.9324 train acc 0.9801\n",
            "batch: 12/160 train loss: 0.9373 train acc 0.9805\n",
            "batch: 13/160 train loss: 0.9386 train acc 0.9820\n",
            "batch: 14/160 train loss: 0.9388 train acc 0.9833\n",
            "batch: 15/160 train loss: 0.9391 train acc 0.9844\n",
            "batch: 16/160 train loss: 0.9415 train acc 0.9844\n",
            "batch: 17/160 train loss: 0.9436 train acc 0.9825\n",
            "batch: 18/160 train loss: 0.9449 train acc 0.9826\n",
            "batch: 19/160 train loss: 0.9407 train acc 0.9836\n",
            "batch: 20/160 train loss: 0.9392 train acc 0.9844\n",
            "batch: 21/160 train loss: 0.9370 train acc 0.9851\n",
            "batch: 22/160 train loss: 0.9396 train acc 0.9851\n",
            "batch: 23/160 train loss: 0.9374 train acc 0.9857\n",
            "batch: 24/160 train loss: 0.9364 train acc 0.9857\n",
            "batch: 25/160 train loss: 0.9343 train acc 0.9862\n",
            "batch: 26/160 train loss: 0.9332 train acc 0.9868\n",
            "batch: 27/160 train loss: 0.9342 train acc 0.9867\n",
            "batch: 28/160 train loss: 0.9310 train acc 0.9872\n",
            "batch: 29/160 train loss: 0.9312 train acc 0.9865\n",
            "batch: 30/160 train loss: 0.9330 train acc 0.9870\n",
            "batch: 31/160 train loss: 0.9347 train acc 0.9869\n",
            "batch: 32/160 train loss: 0.9332 train acc 0.9873\n",
            "batch: 33/160 train loss: 0.9341 train acc 0.9867\n",
            "batch: 34/160 train loss: 0.9322 train acc 0.9867\n",
            "batch: 35/160 train loss: 0.9342 train acc 0.9862\n",
            "batch: 36/160 train loss: 0.9322 train acc 0.9861\n",
            "batch: 37/160 train loss: 0.9337 train acc 0.9856\n",
            "batch: 38/160 train loss: 0.9354 train acc 0.9860\n",
            "batch: 39/160 train loss: 0.9362 train acc 0.9856\n",
            "batch: 40/160 train loss: 0.9350 train acc 0.9859\n",
            "batch: 41/160 train loss: 0.9348 train acc 0.9863\n",
            "batch: 42/160 train loss: 0.9340 train acc 0.9866\n",
            "batch: 43/160 train loss: 0.9354 train acc 0.9862\n",
            "batch: 44/160 train loss: 0.9334 train acc 0.9858\n",
            "batch: 45/160 train loss: 0.9325 train acc 0.9858\n",
            "batch: 46/160 train loss: 0.9323 train acc 0.9861\n",
            "batch: 47/160 train loss: 0.9329 train acc 0.9854\n",
            "batch: 48/160 train loss: 0.9330 train acc 0.9854\n",
            "batch: 49/160 train loss: 0.9330 train acc 0.9853\n",
            "batch: 50/160 train loss: 0.9326 train acc 0.9856\n",
            "batch: 51/160 train loss: 0.9310 train acc 0.9853\n",
            "batch: 52/160 train loss: 0.9327 train acc 0.9844\n",
            "batch: 53/160 train loss: 0.9329 train acc 0.9838\n",
            "batch: 54/160 train loss: 0.9342 train acc 0.9841\n",
            "batch: 55/160 train loss: 0.9326 train acc 0.9844\n",
            "batch: 56/160 train loss: 0.9315 train acc 0.9844\n",
            "batch: 57/160 train loss: 0.9319 train acc 0.9844\n",
            "batch: 58/160 train loss: 0.9324 train acc 0.9841\n",
            "batch: 59/160 train loss: 0.9329 train acc 0.9844\n",
            "batch: 60/160 train loss: 0.9317 train acc 0.9846\n",
            "batch: 61/160 train loss: 0.9317 train acc 0.9846\n",
            "batch: 62/160 train loss: 0.9310 train acc 0.9846\n",
            "batch: 63/160 train loss: 0.9300 train acc 0.9846\n",
            "batch: 64/160 train loss: 0.9314 train acc 0.9841\n",
            "batch: 65/160 train loss: 0.9313 train acc 0.9839\n",
            "batch: 66/160 train loss: 0.9314 train acc 0.9837\n",
            "batch: 67/160 train loss: 0.9316 train acc 0.9834\n",
            "batch: 68/160 train loss: 0.9314 train acc 0.9835\n",
            "batch: 69/160 train loss: 0.9321 train acc 0.9832\n",
            "batch: 70/160 train loss: 0.9320 train acc 0.9835\n",
            "batch: 71/160 train loss: 0.9317 train acc 0.9835\n",
            "batch: 72/160 train loss: 0.9324 train acc 0.9833\n",
            "batch: 73/160 train loss: 0.9336 train acc 0.9829\n",
            "batch: 74/160 train loss: 0.9337 train acc 0.9831\n",
            "batch: 75/160 train loss: 0.9341 train acc 0.9833\n",
            "batch: 76/160 train loss: 0.9357 train acc 0.9831\n",
            "batch: 77/160 train loss: 0.9367 train acc 0.9834\n",
            "batch: 78/160 train loss: 0.9360 train acc 0.9834\n",
            "batch: 79/160 train loss: 0.9369 train acc 0.9834\n",
            "batch: 80/160 train loss: 0.9373 train acc 0.9836\n",
            "batch: 81/160 train loss: 0.9372 train acc 0.9834\n",
            "batch: 82/160 train loss: 0.9370 train acc 0.9834\n",
            "batch: 83/160 train loss: 0.9368 train acc 0.9836\n",
            "batch: 84/160 train loss: 0.9364 train acc 0.9836\n",
            "batch: 85/160 train loss: 0.9368 train acc 0.9836\n",
            "batch: 86/160 train loss: 0.9364 train acc 0.9836\n",
            "batch: 87/160 train loss: 0.9354 train acc 0.9837\n",
            "batch: 88/160 train loss: 0.9346 train acc 0.9837\n",
            "batch: 89/160 train loss: 0.9331 train acc 0.9837\n",
            "batch: 90/160 train loss: 0.9332 train acc 0.9839\n",
            "batch: 91/160 train loss: 0.9334 train acc 0.9840\n",
            "batch: 92/160 train loss: 0.9327 train acc 0.9840\n",
            "batch: 93/160 train loss: 0.9321 train acc 0.9839\n",
            "batch: 94/160 train loss: 0.9321 train acc 0.9839\n",
            "batch: 95/160 train loss: 0.9322 train acc 0.9839\n",
            "batch: 96/160 train loss: 0.9319 train acc 0.9837\n",
            "batch: 97/160 train loss: 0.9309 train acc 0.9839\n",
            "batch: 98/160 train loss: 0.9309 train acc 0.9841\n",
            "batch: 99/160 train loss: 0.9305 train acc 0.9842\n",
            "batch: 100/160 train loss: 0.9305 train acc 0.9844\n",
            "batch: 101/160 train loss: 0.9301 train acc 0.9844\n",
            "batch: 102/160 train loss: 0.9308 train acc 0.9844\n",
            "batch: 103/160 train loss: 0.9300 train acc 0.9845\n",
            "batch: 104/160 train loss: 0.9305 train acc 0.9842\n",
            "batch: 105/160 train loss: 0.9302 train acc 0.9844\n",
            "batch: 106/160 train loss: 0.9297 train acc 0.9845\n",
            "batch: 107/160 train loss: 0.9294 train acc 0.9845\n",
            "batch: 108/160 train loss: 0.9289 train acc 0.9845\n",
            "batch: 109/160 train loss: 0.9299 train acc 0.9844\n",
            "batch: 110/160 train loss: 0.9298 train acc 0.9845\n",
            "batch: 111/160 train loss: 0.9291 train acc 0.9847\n",
            "batch: 112/160 train loss: 0.9286 train acc 0.9847\n",
            "batch: 113/160 train loss: 0.9288 train acc 0.9847\n",
            "batch: 114/160 train loss: 0.9277 train acc 0.9848\n",
            "batch: 115/160 train loss: 0.9269 train acc 0.9849\n",
            "batch: 116/160 train loss: 0.9273 train acc 0.9850\n",
            "batch: 117/160 train loss: 0.9267 train acc 0.9852\n",
            "batch: 118/160 train loss: 0.9280 train acc 0.9846\n",
            "batch: 119/160 train loss: 0.9289 train acc 0.9845\n",
            "batch: 120/160 train loss: 0.9285 train acc 0.9846\n",
            "batch: 121/160 train loss: 0.9284 train acc 0.9848\n",
            "batch: 122/160 train loss: 0.9276 train acc 0.9849\n",
            "batch: 123/160 train loss: 0.9286 train acc 0.9850\n",
            "batch: 124/160 train loss: 0.9287 train acc 0.9848\n",
            "batch: 125/160 train loss: 0.9289 train acc 0.9848\n",
            "batch: 126/160 train loss: 0.9294 train acc 0.9847\n",
            "batch: 127/160 train loss: 0.9289 train acc 0.9849\n",
            "batch: 128/160 train loss: 0.9286 train acc 0.9849\n",
            "batch: 129/160 train loss: 0.9289 train acc 0.9846\n",
            "batch: 130/160 train loss: 0.9292 train acc 0.9845\n",
            "batch: 131/160 train loss: 0.9294 train acc 0.9846\n",
            "batch: 132/160 train loss: 0.9297 train acc 0.9844\n",
            "batch: 133/160 train loss: 0.9294 train acc 0.9844\n",
            "batch: 134/160 train loss: 0.9287 train acc 0.9844\n",
            "batch: 135/160 train loss: 0.9287 train acc 0.9844\n",
            "batch: 136/160 train loss: 0.9283 train acc 0.9844\n",
            "batch: 137/160 train loss: 0.9281 train acc 0.9844\n",
            "batch: 138/160 train loss: 0.9283 train acc 0.9845\n",
            "batch: 139/160 train loss: 0.9282 train acc 0.9845\n",
            "batch: 140/160 train loss: 0.9278 train acc 0.9845\n",
            "batch: 141/160 train loss: 0.9278 train acc 0.9843\n",
            "batch: 142/160 train loss: 0.9281 train acc 0.9842\n",
            "batch: 143/160 train loss: 0.9287 train acc 0.9840\n",
            "batch: 144/160 train loss: 0.9284 train acc 0.9842\n",
            "batch: 145/160 train loss: 0.9280 train acc 0.9843\n",
            "batch: 146/160 train loss: 0.9290 train acc 0.9841\n",
            "batch: 147/160 train loss: 0.9290 train acc 0.9841\n",
            "batch: 148/160 train loss: 0.9293 train acc 0.9840\n",
            "batch: 149/160 train loss: 0.9291 train acc 0.9841\n",
            "batch: 150/160 train loss: 0.9291 train acc 0.9842\n",
            "batch: 151/160 train loss: 0.9291 train acc 0.9842\n",
            "batch: 152/160 train loss: 0.9293 train acc 0.9843\n",
            "batch: 153/160 train loss: 0.9287 train acc 0.9843\n",
            "batch: 154/160 train loss: 0.9289 train acc 0.9844\n",
            "batch: 155/160 train loss: 0.9286 train acc 0.9844\n",
            "batch: 156/160 train loss: 0.9291 train acc 0.9842\n",
            "batch: 157/160 train loss: 0.9295 train acc 0.9841\n",
            "batch: 158/160 train loss: 0.9293 train acc 0.9839\n",
            "batch: 159/160 train loss: 0.9292 train acc 0.9839\n",
            "batch: 160/160 train loss: 0.9301 train acc 0.9838\n",
            "\n",
            "Epoch 22 train loss: 0.9301 test loss 2.3721 train acc 0.9838 test acc 0.6092\n",
            "Epoch 23/25\n",
            "batch: 1/160 train loss: 0.9818 train acc 0.9531\n",
            "batch: 2/160 train loss: 0.9546 train acc 0.9609\n",
            "batch: 3/160 train loss: 0.9465 train acc 0.9740\n",
            "batch: 4/160 train loss: 0.9476 train acc 0.9766\n",
            "batch: 5/160 train loss: 0.9251 train acc 0.9812\n",
            "batch: 6/160 train loss: 0.9305 train acc 0.9818\n",
            "batch: 7/160 train loss: 0.9265 train acc 0.9821\n",
            "batch: 8/160 train loss: 0.9208 train acc 0.9805\n",
            "batch: 9/160 train loss: 0.9187 train acc 0.9809\n",
            "batch: 10/160 train loss: 0.9125 train acc 0.9828\n",
            "batch: 11/160 train loss: 0.9279 train acc 0.9815\n",
            "batch: 12/160 train loss: 0.9294 train acc 0.9805\n",
            "batch: 13/160 train loss: 0.9279 train acc 0.9820\n",
            "batch: 14/160 train loss: 0.9292 train acc 0.9833\n",
            "batch: 15/160 train loss: 0.9177 train acc 0.9844\n",
            "batch: 16/160 train loss: 0.9160 train acc 0.9844\n",
            "batch: 17/160 train loss: 0.9218 train acc 0.9853\n",
            "batch: 18/160 train loss: 0.9233 train acc 0.9852\n",
            "batch: 19/160 train loss: 0.9232 train acc 0.9852\n",
            "batch: 20/160 train loss: 0.9232 train acc 0.9852\n",
            "batch: 21/160 train loss: 0.9258 train acc 0.9851\n",
            "batch: 22/160 train loss: 0.9250 train acc 0.9858\n",
            "batch: 23/160 train loss: 0.9257 train acc 0.9851\n",
            "batch: 24/160 train loss: 0.9221 train acc 0.9857\n",
            "batch: 25/160 train loss: 0.9228 train acc 0.9862\n",
            "batch: 26/160 train loss: 0.9189 train acc 0.9868\n",
            "batch: 27/160 train loss: 0.9200 train acc 0.9867\n",
            "batch: 28/160 train loss: 0.9184 train acc 0.9866\n",
            "batch: 29/160 train loss: 0.9181 train acc 0.9865\n",
            "batch: 30/160 train loss: 0.9166 train acc 0.9870\n",
            "batch: 31/160 train loss: 0.9164 train acc 0.9874\n",
            "batch: 32/160 train loss: 0.9184 train acc 0.9873\n",
            "batch: 33/160 train loss: 0.9177 train acc 0.9877\n",
            "batch: 34/160 train loss: 0.9192 train acc 0.9876\n",
            "batch: 35/160 train loss: 0.9193 train acc 0.9871\n",
            "batch: 36/160 train loss: 0.9197 train acc 0.9870\n",
            "batch: 37/160 train loss: 0.9194 train acc 0.9869\n",
            "batch: 38/160 train loss: 0.9195 train acc 0.9868\n",
            "batch: 39/160 train loss: 0.9196 train acc 0.9868\n",
            "batch: 40/160 train loss: 0.9198 train acc 0.9863\n",
            "batch: 41/160 train loss: 0.9216 train acc 0.9863\n",
            "batch: 42/160 train loss: 0.9216 train acc 0.9866\n",
            "batch: 43/160 train loss: 0.9219 train acc 0.9866\n",
            "batch: 44/160 train loss: 0.9215 train acc 0.9865\n",
            "batch: 45/160 train loss: 0.9205 train acc 0.9868\n",
            "batch: 46/160 train loss: 0.9204 train acc 0.9871\n",
            "batch: 47/160 train loss: 0.9207 train acc 0.9870\n",
            "batch: 48/160 train loss: 0.9204 train acc 0.9870\n",
            "batch: 49/160 train loss: 0.9213 train acc 0.9869\n",
            "batch: 50/160 train loss: 0.9197 train acc 0.9872\n",
            "batch: 51/160 train loss: 0.9201 train acc 0.9871\n",
            "batch: 52/160 train loss: 0.9213 train acc 0.9868\n",
            "batch: 53/160 train loss: 0.9200 train acc 0.9870\n",
            "batch: 54/160 train loss: 0.9209 train acc 0.9870\n",
            "batch: 55/160 train loss: 0.9195 train acc 0.9872\n",
            "batch: 56/160 train loss: 0.9186 train acc 0.9866\n",
            "batch: 57/160 train loss: 0.9175 train acc 0.9868\n",
            "batch: 58/160 train loss: 0.9171 train acc 0.9868\n",
            "batch: 59/160 train loss: 0.9188 train acc 0.9868\n",
            "batch: 60/160 train loss: 0.9184 train acc 0.9859\n",
            "batch: 61/160 train loss: 0.9170 train acc 0.9862\n",
            "batch: 62/160 train loss: 0.9168 train acc 0.9861\n",
            "batch: 63/160 train loss: 0.9167 train acc 0.9864\n",
            "batch: 64/160 train loss: 0.9174 train acc 0.9861\n",
            "batch: 65/160 train loss: 0.9180 train acc 0.9863\n",
            "batch: 66/160 train loss: 0.9176 train acc 0.9865\n",
            "batch: 67/160 train loss: 0.9183 train acc 0.9867\n",
            "batch: 68/160 train loss: 0.9181 train acc 0.9867\n",
            "batch: 69/160 train loss: 0.9185 train acc 0.9869\n",
            "batch: 70/160 train loss: 0.9182 train acc 0.9871\n",
            "batch: 71/160 train loss: 0.9199 train acc 0.9866\n",
            "batch: 72/160 train loss: 0.9197 train acc 0.9861\n",
            "batch: 73/160 train loss: 0.9194 train acc 0.9861\n",
            "batch: 74/160 train loss: 0.9197 train acc 0.9859\n",
            "batch: 75/160 train loss: 0.9231 train acc 0.9852\n",
            "batch: 76/160 train loss: 0.9244 train acc 0.9850\n",
            "batch: 77/160 train loss: 0.9247 train acc 0.9852\n",
            "batch: 78/160 train loss: 0.9239 train acc 0.9854\n",
            "batch: 79/160 train loss: 0.9231 train acc 0.9856\n",
            "batch: 80/160 train loss: 0.9230 train acc 0.9857\n",
            "batch: 81/160 train loss: 0.9229 train acc 0.9859\n",
            "batch: 82/160 train loss: 0.9238 train acc 0.9857\n",
            "batch: 83/160 train loss: 0.9239 train acc 0.9857\n",
            "batch: 84/160 train loss: 0.9229 train acc 0.9859\n",
            "batch: 85/160 train loss: 0.9234 train acc 0.9857\n",
            "batch: 86/160 train loss: 0.9227 train acc 0.9856\n",
            "batch: 87/160 train loss: 0.9224 train acc 0.9853\n",
            "batch: 88/160 train loss: 0.9232 train acc 0.9854\n",
            "batch: 89/160 train loss: 0.9226 train acc 0.9854\n",
            "batch: 90/160 train loss: 0.9233 train acc 0.9851\n",
            "batch: 91/160 train loss: 0.9227 train acc 0.9852\n",
            "batch: 92/160 train loss: 0.9224 train acc 0.9852\n",
            "batch: 93/160 train loss: 0.9214 train acc 0.9854\n",
            "batch: 94/160 train loss: 0.9224 train acc 0.9855\n",
            "batch: 95/160 train loss: 0.9239 train acc 0.9852\n",
            "batch: 96/160 train loss: 0.9231 train acc 0.9854\n",
            "batch: 97/160 train loss: 0.9229 train acc 0.9855\n",
            "batch: 98/160 train loss: 0.9222 train acc 0.9857\n",
            "batch: 99/160 train loss: 0.9223 train acc 0.9858\n",
            "batch: 100/160 train loss: 0.9213 train acc 0.9859\n",
            "batch: 101/160 train loss: 0.9207 train acc 0.9861\n",
            "batch: 102/160 train loss: 0.9205 train acc 0.9862\n",
            "batch: 103/160 train loss: 0.9214 train acc 0.9859\n",
            "batch: 104/160 train loss: 0.9216 train acc 0.9857\n",
            "batch: 105/160 train loss: 0.9217 train acc 0.9857\n",
            "batch: 106/160 train loss: 0.9222 train acc 0.9856\n",
            "batch: 107/160 train loss: 0.9215 train acc 0.9857\n",
            "batch: 108/160 train loss: 0.9208 train acc 0.9855\n",
            "batch: 109/160 train loss: 0.9207 train acc 0.9855\n",
            "batch: 110/160 train loss: 0.9210 train acc 0.9854\n",
            "batch: 111/160 train loss: 0.9207 train acc 0.9855\n",
            "batch: 112/160 train loss: 0.9202 train acc 0.9855\n",
            "batch: 113/160 train loss: 0.9201 train acc 0.9855\n",
            "batch: 114/160 train loss: 0.9208 train acc 0.9853\n",
            "batch: 115/160 train loss: 0.9210 train acc 0.9855\n",
            "batch: 116/160 train loss: 0.9212 train acc 0.9855\n",
            "batch: 117/160 train loss: 0.9217 train acc 0.9853\n",
            "batch: 118/160 train loss: 0.9215 train acc 0.9854\n",
            "batch: 119/160 train loss: 0.9215 train acc 0.9856\n",
            "batch: 120/160 train loss: 0.9217 train acc 0.9854\n",
            "batch: 121/160 train loss: 0.9229 train acc 0.9851\n",
            "batch: 122/160 train loss: 0.9231 train acc 0.9851\n",
            "batch: 123/160 train loss: 0.9228 train acc 0.9851\n",
            "batch: 124/160 train loss: 0.9231 train acc 0.9853\n",
            "batch: 125/160 train loss: 0.9229 train acc 0.9852\n",
            "batch: 126/160 train loss: 0.9244 train acc 0.9846\n",
            "batch: 127/160 train loss: 0.9237 train acc 0.9846\n",
            "batch: 128/160 train loss: 0.9242 train acc 0.9845\n",
            "batch: 129/160 train loss: 0.9241 train acc 0.9845\n",
            "batch: 130/160 train loss: 0.9235 train acc 0.9846\n",
            "batch: 131/160 train loss: 0.9243 train acc 0.9845\n",
            "batch: 132/160 train loss: 0.9240 train acc 0.9845\n",
            "batch: 133/160 train loss: 0.9236 train acc 0.9846\n",
            "batch: 134/160 train loss: 0.9233 train acc 0.9847\n",
            "batch: 135/160 train loss: 0.9238 train acc 0.9847\n",
            "batch: 136/160 train loss: 0.9245 train acc 0.9845\n",
            "batch: 137/160 train loss: 0.9246 train acc 0.9844\n",
            "batch: 138/160 train loss: 0.9247 train acc 0.9844\n",
            "batch: 139/160 train loss: 0.9257 train acc 0.9842\n",
            "batch: 140/160 train loss: 0.9255 train acc 0.9839\n",
            "batch: 141/160 train loss: 0.9257 train acc 0.9839\n",
            "batch: 142/160 train loss: 0.9253 train acc 0.9840\n",
            "batch: 143/160 train loss: 0.9252 train acc 0.9842\n",
            "batch: 144/160 train loss: 0.9250 train acc 0.9840\n",
            "batch: 145/160 train loss: 0.9252 train acc 0.9841\n",
            "batch: 146/160 train loss: 0.9248 train acc 0.9841\n",
            "batch: 147/160 train loss: 0.9248 train acc 0.9842\n",
            "batch: 148/160 train loss: 0.9249 train acc 0.9842\n",
            "batch: 149/160 train loss: 0.9250 train acc 0.9843\n",
            "batch: 150/160 train loss: 0.9245 train acc 0.9844\n",
            "batch: 151/160 train loss: 0.9241 train acc 0.9845\n",
            "batch: 152/160 train loss: 0.9243 train acc 0.9844\n",
            "batch: 153/160 train loss: 0.9242 train acc 0.9844\n",
            "batch: 154/160 train loss: 0.9243 train acc 0.9844\n",
            "batch: 155/160 train loss: 0.9245 train acc 0.9843\n",
            "batch: 156/160 train loss: 0.9241 train acc 0.9843\n",
            "batch: 157/160 train loss: 0.9240 train acc 0.9844\n",
            "batch: 158/160 train loss: 0.9239 train acc 0.9845\n",
            "batch: 159/160 train loss: 0.9241 train acc 0.9846\n",
            "batch: 160/160 train loss: 0.9246 train acc 0.9846\n",
            "\n",
            "Epoch 23 train loss: 0.9246 test loss 2.3728 train acc 0.9846 test acc 0.6100\n",
            "Epoch 24/25\n",
            "batch: 1/160 train loss: 0.9235 train acc 0.9688\n",
            "batch: 2/160 train loss: 0.8862 train acc 0.9844\n",
            "batch: 3/160 train loss: 0.9165 train acc 0.9896\n",
            "batch: 4/160 train loss: 0.9259 train acc 0.9883\n",
            "batch: 5/160 train loss: 0.9413 train acc 0.9781\n",
            "batch: 6/160 train loss: 0.9484 train acc 0.9766\n",
            "batch: 7/160 train loss: 0.9486 train acc 0.9799\n",
            "batch: 8/160 train loss: 0.9383 train acc 0.9805\n",
            "batch: 9/160 train loss: 0.9448 train acc 0.9826\n",
            "batch: 10/160 train loss: 0.9391 train acc 0.9844\n",
            "batch: 11/160 train loss: 0.9350 train acc 0.9858\n",
            "batch: 12/160 train loss: 0.9321 train acc 0.9857\n",
            "batch: 13/160 train loss: 0.9239 train acc 0.9868\n",
            "batch: 14/160 train loss: 0.9195 train acc 0.9877\n",
            "batch: 15/160 train loss: 0.9200 train acc 0.9875\n",
            "batch: 16/160 train loss: 0.9160 train acc 0.9873\n",
            "batch: 17/160 train loss: 0.9131 train acc 0.9881\n",
            "batch: 18/160 train loss: 0.9169 train acc 0.9878\n",
            "batch: 19/160 train loss: 0.9149 train acc 0.9885\n",
            "batch: 20/160 train loss: 0.9127 train acc 0.9891\n",
            "batch: 21/160 train loss: 0.9081 train acc 0.9881\n",
            "batch: 22/160 train loss: 0.9083 train acc 0.9886\n",
            "batch: 23/160 train loss: 0.9100 train acc 0.9871\n",
            "batch: 24/160 train loss: 0.9077 train acc 0.9870\n",
            "batch: 25/160 train loss: 0.9083 train acc 0.9862\n",
            "batch: 26/160 train loss: 0.9089 train acc 0.9850\n",
            "batch: 27/160 train loss: 0.9112 train acc 0.9844\n",
            "batch: 28/160 train loss: 0.9123 train acc 0.9849\n",
            "batch: 29/160 train loss: 0.9145 train acc 0.9844\n",
            "batch: 30/160 train loss: 0.9155 train acc 0.9844\n",
            "batch: 31/160 train loss: 0.9140 train acc 0.9844\n",
            "batch: 32/160 train loss: 0.9126 train acc 0.9849\n",
            "batch: 33/160 train loss: 0.9128 train acc 0.9844\n",
            "batch: 34/160 train loss: 0.9127 train acc 0.9844\n",
            "batch: 35/160 train loss: 0.9127 train acc 0.9848\n",
            "batch: 36/160 train loss: 0.9134 train acc 0.9852\n",
            "batch: 37/160 train loss: 0.9111 train acc 0.9856\n",
            "batch: 38/160 train loss: 0.9106 train acc 0.9856\n",
            "batch: 39/160 train loss: 0.9104 train acc 0.9860\n",
            "batch: 40/160 train loss: 0.9109 train acc 0.9855\n",
            "batch: 41/160 train loss: 0.9125 train acc 0.9859\n",
            "batch: 42/160 train loss: 0.9129 train acc 0.9862\n",
            "batch: 43/160 train loss: 0.9125 train acc 0.9866\n",
            "batch: 44/160 train loss: 0.9110 train acc 0.9869\n",
            "batch: 45/160 train loss: 0.9122 train acc 0.9865\n",
            "batch: 46/160 train loss: 0.9134 train acc 0.9861\n",
            "batch: 47/160 train loss: 0.9123 train acc 0.9864\n",
            "batch: 48/160 train loss: 0.9136 train acc 0.9867\n",
            "batch: 49/160 train loss: 0.9131 train acc 0.9869\n",
            "batch: 50/160 train loss: 0.9120 train acc 0.9869\n",
            "batch: 51/160 train loss: 0.9109 train acc 0.9868\n",
            "batch: 52/160 train loss: 0.9089 train acc 0.9871\n",
            "batch: 53/160 train loss: 0.9099 train acc 0.9870\n",
            "batch: 54/160 train loss: 0.9100 train acc 0.9870\n",
            "batch: 55/160 train loss: 0.9114 train acc 0.9866\n",
            "batch: 56/160 train loss: 0.9136 train acc 0.9858\n",
            "batch: 57/160 train loss: 0.9147 train acc 0.9860\n",
            "batch: 58/160 train loss: 0.9148 train acc 0.9863\n",
            "batch: 59/160 train loss: 0.9142 train acc 0.9865\n",
            "batch: 60/160 train loss: 0.9132 train acc 0.9865\n",
            "batch: 61/160 train loss: 0.9124 train acc 0.9867\n",
            "batch: 62/160 train loss: 0.9127 train acc 0.9866\n",
            "batch: 63/160 train loss: 0.9137 train acc 0.9861\n",
            "batch: 64/160 train loss: 0.9121 train acc 0.9863\n",
            "batch: 65/160 train loss: 0.9120 train acc 0.9865\n",
            "batch: 66/160 train loss: 0.9110 train acc 0.9867\n",
            "batch: 67/160 train loss: 0.9102 train acc 0.9867\n",
            "batch: 68/160 train loss: 0.9103 train acc 0.9869\n",
            "batch: 69/160 train loss: 0.9123 train acc 0.9864\n",
            "batch: 70/160 train loss: 0.9128 train acc 0.9864\n",
            "batch: 71/160 train loss: 0.9128 train acc 0.9864\n",
            "batch: 72/160 train loss: 0.9127 train acc 0.9865\n",
            "batch: 73/160 train loss: 0.9128 train acc 0.9865\n",
            "batch: 74/160 train loss: 0.9141 train acc 0.9865\n",
            "batch: 75/160 train loss: 0.9134 train acc 0.9867\n",
            "batch: 76/160 train loss: 0.9134 train acc 0.9866\n",
            "batch: 77/160 train loss: 0.9139 train acc 0.9866\n",
            "batch: 78/160 train loss: 0.9135 train acc 0.9868\n",
            "batch: 79/160 train loss: 0.9143 train acc 0.9869\n",
            "batch: 80/160 train loss: 0.9143 train acc 0.9869\n",
            "batch: 81/160 train loss: 0.9142 train acc 0.9865\n",
            "batch: 82/160 train loss: 0.9136 train acc 0.9867\n",
            "batch: 83/160 train loss: 0.9142 train acc 0.9868\n",
            "batch: 84/160 train loss: 0.9147 train acc 0.9868\n",
            "batch: 85/160 train loss: 0.9148 train acc 0.9866\n",
            "batch: 86/160 train loss: 0.9153 train acc 0.9864\n",
            "batch: 87/160 train loss: 0.9156 train acc 0.9865\n",
            "batch: 88/160 train loss: 0.9157 train acc 0.9865\n",
            "batch: 89/160 train loss: 0.9153 train acc 0.9865\n",
            "batch: 90/160 train loss: 0.9143 train acc 0.9866\n",
            "batch: 91/160 train loss: 0.9143 train acc 0.9868\n",
            "batch: 92/160 train loss: 0.9147 train acc 0.9868\n",
            "batch: 93/160 train loss: 0.9148 train acc 0.9866\n",
            "batch: 94/160 train loss: 0.9156 train acc 0.9867\n",
            "batch: 95/160 train loss: 0.9162 train acc 0.9868\n",
            "batch: 96/160 train loss: 0.9160 train acc 0.9870\n",
            "batch: 97/160 train loss: 0.9160 train acc 0.9871\n",
            "batch: 98/160 train loss: 0.9160 train acc 0.9871\n",
            "batch: 99/160 train loss: 0.9154 train acc 0.9872\n",
            "batch: 100/160 train loss: 0.9154 train acc 0.9872\n",
            "batch: 101/160 train loss: 0.9156 train acc 0.9872\n",
            "batch: 102/160 train loss: 0.9164 train acc 0.9871\n",
            "batch: 103/160 train loss: 0.9172 train acc 0.9868\n",
            "batch: 104/160 train loss: 0.9167 train acc 0.9869\n",
            "batch: 105/160 train loss: 0.9166 train acc 0.9868\n",
            "batch: 106/160 train loss: 0.9169 train acc 0.9867\n",
            "batch: 107/160 train loss: 0.9182 train acc 0.9866\n",
            "batch: 108/160 train loss: 0.9191 train acc 0.9863\n",
            "batch: 109/160 train loss: 0.9192 train acc 0.9862\n",
            "batch: 110/160 train loss: 0.9193 train acc 0.9862\n",
            "batch: 111/160 train loss: 0.9188 train acc 0.9862\n",
            "batch: 112/160 train loss: 0.9187 train acc 0.9862\n",
            "batch: 113/160 train loss: 0.9187 train acc 0.9862\n",
            "batch: 114/160 train loss: 0.9195 train acc 0.9859\n",
            "batch: 115/160 train loss: 0.9197 train acc 0.9857\n",
            "batch: 116/160 train loss: 0.9203 train acc 0.9855\n",
            "batch: 117/160 train loss: 0.9198 train acc 0.9854\n",
            "batch: 118/160 train loss: 0.9201 train acc 0.9853\n",
            "batch: 119/160 train loss: 0.9201 train acc 0.9853\n",
            "batch: 120/160 train loss: 0.9202 train acc 0.9852\n",
            "batch: 121/160 train loss: 0.9208 train acc 0.9851\n",
            "batch: 122/160 train loss: 0.9208 train acc 0.9851\n",
            "batch: 123/160 train loss: 0.9215 train acc 0.9851\n",
            "batch: 124/160 train loss: 0.9206 train acc 0.9853\n",
            "batch: 125/160 train loss: 0.9209 train acc 0.9852\n",
            "batch: 126/160 train loss: 0.9211 train acc 0.9852\n",
            "batch: 127/160 train loss: 0.9212 train acc 0.9854\n",
            "batch: 128/160 train loss: 0.9212 train acc 0.9854\n",
            "batch: 129/160 train loss: 0.9204 train acc 0.9852\n",
            "batch: 130/160 train loss: 0.9208 train acc 0.9852\n",
            "batch: 131/160 train loss: 0.9215 train acc 0.9852\n",
            "batch: 132/160 train loss: 0.9223 train acc 0.9851\n",
            "batch: 133/160 train loss: 0.9225 train acc 0.9850\n",
            "batch: 134/160 train loss: 0.9221 train acc 0.9850\n",
            "batch: 135/160 train loss: 0.9227 train acc 0.9847\n",
            "batch: 136/160 train loss: 0.9225 train acc 0.9847\n",
            "batch: 137/160 train loss: 0.9221 train acc 0.9848\n",
            "batch: 138/160 train loss: 0.9228 train acc 0.9847\n",
            "batch: 139/160 train loss: 0.9231 train acc 0.9844\n",
            "batch: 140/160 train loss: 0.9229 train acc 0.9845\n",
            "batch: 141/160 train loss: 0.9229 train acc 0.9844\n",
            "batch: 142/160 train loss: 0.9229 train acc 0.9844\n",
            "batch: 143/160 train loss: 0.9230 train acc 0.9843\n",
            "batch: 144/160 train loss: 0.9232 train acc 0.9842\n",
            "batch: 145/160 train loss: 0.9228 train acc 0.9841\n",
            "batch: 146/160 train loss: 0.9228 train acc 0.9841\n",
            "batch: 147/160 train loss: 0.9225 train acc 0.9842\n",
            "batch: 148/160 train loss: 0.9226 train acc 0.9842\n",
            "batch: 149/160 train loss: 0.9224 train acc 0.9843\n",
            "batch: 150/160 train loss: 0.9226 train acc 0.9842\n",
            "batch: 151/160 train loss: 0.9231 train acc 0.9842\n",
            "batch: 152/160 train loss: 0.9226 train acc 0.9842\n",
            "batch: 153/160 train loss: 0.9230 train acc 0.9843\n",
            "batch: 154/160 train loss: 0.9235 train acc 0.9842\n",
            "batch: 155/160 train loss: 0.9235 train acc 0.9843\n",
            "batch: 156/160 train loss: 0.9238 train acc 0.9842\n",
            "batch: 157/160 train loss: 0.9231 train acc 0.9843\n",
            "batch: 158/160 train loss: 0.9228 train acc 0.9843\n",
            "batch: 159/160 train loss: 0.9223 train acc 0.9844\n",
            "batch: 160/160 train loss: 0.9232 train acc 0.9843\n",
            "\n",
            "Epoch 24 train loss: 0.9232 test loss 2.3753 train acc 0.9843 test acc 0.6167\n",
            "Epoch 25/25\n",
            "batch: 1/160 train loss: 0.9506 train acc 0.9844\n",
            "batch: 2/160 train loss: 0.9466 train acc 0.9922\n",
            "batch: 3/160 train loss: 0.9110 train acc 0.9896\n",
            "batch: 4/160 train loss: 0.8963 train acc 0.9922\n",
            "batch: 5/160 train loss: 0.9001 train acc 0.9906\n",
            "batch: 6/160 train loss: 0.8985 train acc 0.9870\n",
            "batch: 7/160 train loss: 0.9109 train acc 0.9866\n",
            "batch: 8/160 train loss: 0.9143 train acc 0.9883\n",
            "batch: 9/160 train loss: 0.9232 train acc 0.9861\n",
            "batch: 10/160 train loss: 0.9146 train acc 0.9875\n",
            "batch: 11/160 train loss: 0.9175 train acc 0.9872\n",
            "batch: 12/160 train loss: 0.9125 train acc 0.9883\n",
            "batch: 13/160 train loss: 0.9119 train acc 0.9880\n",
            "batch: 14/160 train loss: 0.9187 train acc 0.9866\n",
            "batch: 15/160 train loss: 0.9138 train acc 0.9875\n",
            "batch: 16/160 train loss: 0.9107 train acc 0.9873\n",
            "batch: 17/160 train loss: 0.9034 train acc 0.9881\n",
            "batch: 18/160 train loss: 0.9076 train acc 0.9878\n",
            "batch: 19/160 train loss: 0.9093 train acc 0.9885\n",
            "batch: 20/160 train loss: 0.9114 train acc 0.9875\n",
            "batch: 21/160 train loss: 0.9159 train acc 0.9866\n",
            "batch: 22/160 train loss: 0.9181 train acc 0.9865\n",
            "batch: 23/160 train loss: 0.9171 train acc 0.9871\n",
            "batch: 24/160 train loss: 0.9193 train acc 0.9870\n",
            "batch: 25/160 train loss: 0.9184 train acc 0.9869\n",
            "batch: 26/160 train loss: 0.9200 train acc 0.9868\n",
            "batch: 27/160 train loss: 0.9155 train acc 0.9873\n",
            "batch: 28/160 train loss: 0.9152 train acc 0.9872\n",
            "batch: 29/160 train loss: 0.9136 train acc 0.9876\n",
            "batch: 30/160 train loss: 0.9115 train acc 0.9875\n",
            "batch: 31/160 train loss: 0.9124 train acc 0.9874\n",
            "batch: 32/160 train loss: 0.9122 train acc 0.9868\n",
            "batch: 33/160 train loss: 0.9132 train acc 0.9872\n",
            "batch: 34/160 train loss: 0.9118 train acc 0.9871\n",
            "batch: 35/160 train loss: 0.9121 train acc 0.9871\n",
            "batch: 36/160 train loss: 0.9096 train acc 0.9874\n",
            "batch: 37/160 train loss: 0.9097 train acc 0.9878\n",
            "batch: 38/160 train loss: 0.9108 train acc 0.9881\n",
            "batch: 39/160 train loss: 0.9103 train acc 0.9884\n",
            "batch: 40/160 train loss: 0.9110 train acc 0.9883\n",
            "batch: 41/160 train loss: 0.9102 train acc 0.9882\n",
            "batch: 42/160 train loss: 0.9118 train acc 0.9881\n",
            "batch: 43/160 train loss: 0.9114 train acc 0.9880\n",
            "batch: 44/160 train loss: 0.9108 train acc 0.9879\n",
            "batch: 45/160 train loss: 0.9133 train acc 0.9878\n",
            "batch: 46/160 train loss: 0.9127 train acc 0.9881\n",
            "batch: 47/160 train loss: 0.9130 train acc 0.9874\n",
            "batch: 48/160 train loss: 0.9150 train acc 0.9870\n",
            "batch: 49/160 train loss: 0.9168 train acc 0.9866\n",
            "batch: 50/160 train loss: 0.9187 train acc 0.9859\n",
            "batch: 51/160 train loss: 0.9197 train acc 0.9859\n",
            "batch: 52/160 train loss: 0.9187 train acc 0.9862\n",
            "batch: 53/160 train loss: 0.9192 train acc 0.9861\n",
            "batch: 54/160 train loss: 0.9199 train acc 0.9858\n",
            "batch: 55/160 train loss: 0.9203 train acc 0.9861\n",
            "batch: 56/160 train loss: 0.9193 train acc 0.9860\n",
            "batch: 57/160 train loss: 0.9184 train acc 0.9863\n",
            "batch: 58/160 train loss: 0.9176 train acc 0.9865\n",
            "batch: 59/160 train loss: 0.9176 train acc 0.9868\n",
            "batch: 60/160 train loss: 0.9184 train acc 0.9870\n",
            "batch: 61/160 train loss: 0.9174 train acc 0.9869\n",
            "batch: 62/160 train loss: 0.9170 train acc 0.9869\n",
            "batch: 63/160 train loss: 0.9161 train acc 0.9869\n",
            "batch: 64/160 train loss: 0.9169 train acc 0.9871\n",
            "batch: 65/160 train loss: 0.9179 train acc 0.9870\n",
            "batch: 66/160 train loss: 0.9196 train acc 0.9867\n",
            "batch: 67/160 train loss: 0.9206 train acc 0.9867\n",
            "batch: 68/160 train loss: 0.9209 train acc 0.9862\n",
            "batch: 69/160 train loss: 0.9231 train acc 0.9857\n",
            "batch: 70/160 train loss: 0.9228 train acc 0.9859\n",
            "batch: 71/160 train loss: 0.9237 train acc 0.9859\n",
            "batch: 72/160 train loss: 0.9238 train acc 0.9859\n",
            "batch: 73/160 train loss: 0.9240 train acc 0.9861\n",
            "batch: 74/160 train loss: 0.9244 train acc 0.9856\n",
            "batch: 75/160 train loss: 0.9240 train acc 0.9854\n",
            "batch: 76/160 train loss: 0.9245 train acc 0.9852\n",
            "batch: 77/160 train loss: 0.9241 train acc 0.9854\n",
            "batch: 78/160 train loss: 0.9251 train acc 0.9856\n",
            "batch: 79/160 train loss: 0.9255 train acc 0.9858\n",
            "batch: 80/160 train loss: 0.9255 train acc 0.9857\n",
            "batch: 81/160 train loss: 0.9253 train acc 0.9859\n",
            "batch: 82/160 train loss: 0.9248 train acc 0.9859\n",
            "batch: 83/160 train loss: 0.9248 train acc 0.9857\n",
            "batch: 84/160 train loss: 0.9246 train acc 0.9857\n",
            "batch: 85/160 train loss: 0.9244 train acc 0.9857\n",
            "batch: 86/160 train loss: 0.9238 train acc 0.9856\n",
            "batch: 87/160 train loss: 0.9240 train acc 0.9858\n",
            "batch: 88/160 train loss: 0.9230 train acc 0.9860\n",
            "batch: 89/160 train loss: 0.9231 train acc 0.9861\n",
            "batch: 90/160 train loss: 0.9221 train acc 0.9863\n",
            "batch: 91/160 train loss: 0.9224 train acc 0.9863\n",
            "batch: 92/160 train loss: 0.9219 train acc 0.9864\n",
            "batch: 93/160 train loss: 0.9211 train acc 0.9866\n",
            "batch: 94/160 train loss: 0.9213 train acc 0.9865\n",
            "batch: 95/160 train loss: 0.9218 train acc 0.9863\n",
            "batch: 96/160 train loss: 0.9214 train acc 0.9862\n",
            "batch: 97/160 train loss: 0.9211 train acc 0.9863\n",
            "batch: 98/160 train loss: 0.9207 train acc 0.9863\n",
            "batch: 99/160 train loss: 0.9207 train acc 0.9861\n",
            "batch: 100/160 train loss: 0.9220 train acc 0.9859\n",
            "batch: 101/160 train loss: 0.9219 train acc 0.9859\n",
            "batch: 102/160 train loss: 0.9230 train acc 0.9861\n",
            "batch: 103/160 train loss: 0.9221 train acc 0.9862\n",
            "batch: 104/160 train loss: 0.9226 train acc 0.9860\n",
            "batch: 105/160 train loss: 0.9228 train acc 0.9860\n",
            "batch: 106/160 train loss: 0.9219 train acc 0.9860\n",
            "batch: 107/160 train loss: 0.9213 train acc 0.9861\n",
            "batch: 108/160 train loss: 0.9212 train acc 0.9863\n",
            "batch: 109/160 train loss: 0.9211 train acc 0.9864\n",
            "batch: 110/160 train loss: 0.9211 train acc 0.9861\n",
            "batch: 111/160 train loss: 0.9213 train acc 0.9859\n",
            "batch: 112/160 train loss: 0.9210 train acc 0.9859\n",
            "batch: 113/160 train loss: 0.9206 train acc 0.9859\n",
            "batch: 114/160 train loss: 0.9201 train acc 0.9860\n",
            "batch: 115/160 train loss: 0.9195 train acc 0.9861\n",
            "batch: 116/160 train loss: 0.9199 train acc 0.9863\n",
            "batch: 117/160 train loss: 0.9200 train acc 0.9862\n",
            "batch: 118/160 train loss: 0.9200 train acc 0.9864\n",
            "batch: 119/160 train loss: 0.9197 train acc 0.9865\n",
            "batch: 120/160 train loss: 0.9194 train acc 0.9866\n",
            "batch: 121/160 train loss: 0.9190 train acc 0.9866\n",
            "batch: 122/160 train loss: 0.9190 train acc 0.9864\n",
            "batch: 123/160 train loss: 0.9187 train acc 0.9864\n",
            "batch: 124/160 train loss: 0.9186 train acc 0.9864\n",
            "batch: 125/160 train loss: 0.9189 train acc 0.9864\n",
            "batch: 126/160 train loss: 0.9194 train acc 0.9862\n",
            "batch: 127/160 train loss: 0.9192 train acc 0.9863\n",
            "batch: 128/160 train loss: 0.9190 train acc 0.9863\n",
            "batch: 129/160 train loss: 0.9188 train acc 0.9863\n",
            "batch: 130/160 train loss: 0.9185 train acc 0.9864\n",
            "batch: 131/160 train loss: 0.9185 train acc 0.9865\n",
            "batch: 132/160 train loss: 0.9185 train acc 0.9865\n",
            "batch: 133/160 train loss: 0.9185 train acc 0.9864\n",
            "batch: 134/160 train loss: 0.9193 train acc 0.9864\n",
            "batch: 135/160 train loss: 0.9190 train acc 0.9862\n",
            "batch: 136/160 train loss: 0.9195 train acc 0.9860\n",
            "batch: 137/160 train loss: 0.9196 train acc 0.9859\n",
            "batch: 138/160 train loss: 0.9196 train acc 0.9860\n",
            "batch: 139/160 train loss: 0.9190 train acc 0.9861\n",
            "batch: 140/160 train loss: 0.9186 train acc 0.9862\n",
            "batch: 141/160 train loss: 0.9190 train acc 0.9861\n",
            "batch: 142/160 train loss: 0.9188 train acc 0.9862\n",
            "batch: 143/160 train loss: 0.9196 train acc 0.9862\n",
            "batch: 144/160 train loss: 0.9201 train acc 0.9860\n",
            "batch: 145/160 train loss: 0.9195 train acc 0.9861\n",
            "batch: 146/160 train loss: 0.9192 train acc 0.9860\n",
            "batch: 147/160 train loss: 0.9194 train acc 0.9861\n",
            "batch: 148/160 train loss: 0.9192 train acc 0.9861\n",
            "batch: 149/160 train loss: 0.9188 train acc 0.9861\n",
            "batch: 150/160 train loss: 0.9187 train acc 0.9859\n",
            "batch: 151/160 train loss: 0.9188 train acc 0.9859\n",
            "batch: 152/160 train loss: 0.9188 train acc 0.9859\n",
            "batch: 153/160 train loss: 0.9187 train acc 0.9859\n",
            "batch: 154/160 train loss: 0.9188 train acc 0.9859\n",
            "batch: 155/160 train loss: 0.9186 train acc 0.9860\n",
            "batch: 156/160 train loss: 0.9181 train acc 0.9861\n",
            "batch: 157/160 train loss: 0.9180 train acc 0.9860\n",
            "batch: 158/160 train loss: 0.9183 train acc 0.9859\n",
            "batch: 159/160 train loss: 0.9181 train acc 0.9859\n",
            "batch: 160/160 train loss: 0.9187 train acc 0.9859\n",
            "\n",
            "Epoch 25 train loss: 0.9187 test loss 2.3685 train acc 0.9859 test acc 0.6133\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Сохраняем веса модели\n",
        "path_to_save_model_state = '/content/drive/MyDrive/DLS Face recognition/models weights/arcmobilenet_DLS_FR_weightes_v2.pth'\n",
        "torch.save(arc_model.state_dict(), path_to_save_model_state)"
      ],
      "metadata": {
        "id": "KVysC2_yXHOZ"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Функция подготавливающая изображение перед подачей его в сеть\n",
        "def make_transforms(image):\n",
        "    transformations = transforms.Compose([transforms.Resize(224, antialias=True),\n",
        "                                          transforms.ToTensor(),\n",
        "                                          transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])\n",
        "\n",
        "    image = transformations(image)\n",
        "    image = image[None,:,:,:]\n",
        "    return image"
      ],
      "metadata": {
        "id": "lngXmlMeXltw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics.pairwise import cosine_distances\n",
        "\n",
        "class IdentificationRateMetric:\n",
        "    def __init__(self, query, distraction, transform, device, fpr=[0.5, 0.2, 0.1, 0.05]):\n",
        "        self.query = query\n",
        "        self.distraction = distraction\n",
        "        self.transform = transform\n",
        "        self.fpr = fpr\n",
        "        self.saved = False\n",
        "        self.query_emb = None\n",
        "        self.distraction_emb = None\n",
        "\n",
        "        self.device = device\n",
        "        self.path_query = '/content/drive/MyDrive/DLS Face recognition/celebA_ir/celebA_query'\n",
        "        self.path_distraction = '/content/drive/MyDrive/DLS Face recognition/celebA_ir/celebA_distractors'\n",
        "\n",
        "    def get_embendings(self, model, df, root):\n",
        "        model.eval()\n",
        "        model.to(self.device)\n",
        "        emb = []\n",
        "        for i in range(df.shape[0]):\n",
        "            path = os.path.join(root, df['File_name'].iloc[i])\n",
        "            img = self.transform(Image.open(path)).to(self.device)\n",
        "            embending = model(img).cpu().detach().numpy().reshape(-1, 1)\n",
        "            emb.append((embending, df['Class'].iloc[i]))\n",
        "        return emb\n",
        "\n",
        "    def get_metric(self, model, fpr=None, save_embendings=False, load_embendings=False):\n",
        "        if load_embendings and self.saved:\n",
        "            query = self.query_emb\n",
        "            distraction = self.distraction_emb\n",
        "        else:\n",
        "            query = self.get_embendings(model, self.query, self.path_query)\n",
        "            distraction = self.get_embendings(model, self.distraction, self.path_distraction)\n",
        "\n",
        "        if save_embendings:\n",
        "            self.query_emb = query\n",
        "            self.distraction_emb = distraction\n",
        "            self.saved = True\n",
        "\n",
        "        diff_face = []\n",
        "        sim_face = []\n",
        "        dist = []\n",
        "\n",
        "        # compute cosine distance for query part\n",
        "        for i in range(len(query)):\n",
        "            emb1, label1 = query[i]\n",
        "            for j in range(i+1, len(query)):\n",
        "                emb2, label2 = query[j]\n",
        "                if label1 == label2:\n",
        "                    sim_face.append(cosine_distances(emb1, emb2)[0][0])\n",
        "                else:\n",
        "                    diff_face.append(cosine_distances(emb1, emb2)[0][0])\n",
        "\n",
        "        # compute cosine distance for distraction part\n",
        "        for emb1, _ in distraction:\n",
        "            for emb2, _ in query:\n",
        "                dist.append(cosine_distances(emb1, emb2)[0][0])\n",
        "\n",
        "        FPR = self.fpr if fpr is None else fpr\n",
        "\n",
        "        rate = []\n",
        "        for fpr_ in FPR:\n",
        "            N = int((len(dist) + len(diff_face)) * fpr_)\n",
        "            threashold = sorted(dist + diff_face)[N]\n",
        "            positive_pair = np.array(sim_face)\n",
        "            rate.append(positive_pair[positive_pair < threashold].size/positive_pair.size)\n",
        "        return rate"
      ],
      "metadata": {
        "id": "sshc6bE3wMuV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query_df = pd.read_csv('/content/drive/MyDrive/DLS Face recognition/celebA_ir/celebA_anno_query.csv')\n",
        "query_df.columns = ['File_name', 'Class']\n",
        "\n",
        "path = os.listdir('/content/drive/MyDrive/DLS Face recognition/celebA_ir/celebA_distractors')\n",
        "t = {'File_name': path, 'Class': -1}\n",
        "\n",
        "distractors_df = pd.DataFrame(t)"
      ],
      "metadata": {
        "id": "2z00r6q8Xm4_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "id_rate = IdentificationRateMetric(query_df, distractors_df, make_transforms, DEVICE)\n",
        "\n",
        "rate = id_rate.get_metric(arc_model, save_embendings=True)"
      ],
      "metadata": {
        "id": "vPoawszhYX95"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rate = [id_rate.get_metric(arc_model, load_embendings=id_rate.saved, fpr=fpr) for fpr in [0.5, 0.2, 0.1, 0.05]]\n",
        "rate"
      ],
      "metadata": {
        "id": "-O4owFZ_o9TH"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}